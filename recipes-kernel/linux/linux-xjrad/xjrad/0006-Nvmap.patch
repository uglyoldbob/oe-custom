From f09f503edd4ac40896fb2a071bbe64ea0db9385e Mon Sep 17 00:00:00 2001
From: Thomas Epperson <thomas.epperson@snapon.com>
Date: Fri, 13 Sep 2024 18:15:00 -0500
Subject: [PATCH] Nvmap
Upstream-Status: Pending

---
 drivers/video/Kconfig                         |    3 +
 drivers/video/Makefile                        |    1 +
 drivers/video/tegra/Makefile                  |    3 +
 drivers/video/tegra/nvmap/Kconfig             |   92 ++
 drivers/video/tegra/nvmap/Makefile            |   51 +
 drivers/video/tegra/nvmap/nv2/Kconfig         |   86 +
 drivers/video/tegra/nvmap/nv2/Makefile        |   50 +
 drivers/video/tegra/nvmap/nv2/nvmap_cache.c   |  383 +++++
 drivers/video/tegra/nvmap/nv2/nvmap_cache.h   |   89 ++
 .../tegra/nvmap/nv2/nvmap_cache_nvmap_t18x.c  |   66 +
 .../video/tegra/nvmap/nv2/nvmap_cache_t19x.c  |   67 +
 .../video/tegra/nvmap/nv2/nvmap_carveout.c    |  503 ++++++
 .../video/tegra/nvmap/nv2/nvmap_carveout.h    |   42 +
 drivers/video/tegra/nvmap/nv2/nvmap_client.c  |  580 +++++++
 drivers/video/tegra/nvmap/nv2/nvmap_client.h  |   65 +
 drivers/video/tegra/nvmap/nv2/nvmap_dev.c     |  915 +++++++++++
 drivers/video/tegra/nvmap/nv2/nvmap_dev.h     |   63 +
 drivers/video/tegra/nvmap/nv2/nvmap_dmabuf.c  |   88 ++
 drivers/video/tegra/nvmap/nv2/nvmap_dmabuf.h  |   33 +
 .../video/tegra/nvmap/nv2/nvmap_dmabuf_ops.c  |  634 ++++++++
 .../video/tegra/nvmap/nv2/nvmap_dmabuf_t19x.c |  102 ++
 drivers/video/tegra/nvmap/nv2/nvmap_fault.c   |  189 +++
 drivers/video/tegra/nvmap/nv2/nvmap_handle.c  | 1015 ++++++++++++
 drivers/video/tegra/nvmap/nv2/nvmap_handle.h  |  175 +++
 .../video/tegra/nvmap/nv2/nvmap_handle_map.c  |  156 ++
 .../tegra/nvmap/nv2/nvmap_handle_map_ops.c    |  293 ++++
 .../video/tegra/nvmap/nv2/nvmap_handle_mm.c   |  219 +++
 .../tegra/nvmap/nv2/nvmap_handle_print.c      |  353 +++++
 .../video/tegra/nvmap/nv2/nvmap_handle_priv.h |   62 +
 .../video/tegra/nvmap/nv2/nvmap_handle_ref.c  |   85 +
 .../video/tegra/nvmap/nv2/nvmap_handle_ref.h  |   36 +
 .../video/tegra/nvmap/nv2/nvmap_handle_vma.c  |  260 ++++
 drivers/video/tegra/nvmap/nv2/nvmap_heap.c    |  355 +++++
 drivers/video/tegra/nvmap/nv2/nvmap_heap.h    |   59 +
 .../video/tegra/nvmap/nv2/nvmap_heap_alloc.c  |  361 +++++
 .../video/tegra/nvmap/nv2/nvmap_heap_alloc.h  |   37 +
 drivers/video/tegra/nvmap/nv2/nvmap_helper.c  |  111 ++
 drivers/video/tegra/nvmap/nv2/nvmap_init.c    |  530 +++++++
 drivers/video/tegra/nvmap/nv2/nvmap_init.h    |   19 +
 .../video/tegra/nvmap/nv2/nvmap_init_t19x.c   |  316 ++++
 drivers/video/tegra/nvmap/nv2/nvmap_ioctl.c   |  920 +++++++++++
 drivers/video/tegra/nvmap/nv2/nvmap_ioctl.h   |   67 +
 .../video/tegra/nvmap/nv2/nvmap_ioctl_cache.c |  264 ++++
 drivers/video/tegra/nvmap/nv2/nvmap_misc.h    |  108 ++
 .../video/tegra/nvmap/nv2/nvmap_page_color.c  |  417 +++++
 .../video/tegra/nvmap/nv2/nvmap_page_color.h  |   46 +
 drivers/video/tegra/nvmap/nv2/nvmap_pp.c      |  750 +++++++++
 drivers/video/tegra/nvmap/nv2/nvmap_pp.h      |   64 +
 drivers/video/tegra/nvmap/nv2/nvmap_stats.c   |  107 ++
 drivers/video/tegra/nvmap/nv2/nvmap_stats.h   |   45 +
 drivers/video/tegra/nvmap/nv2/nvmap_structs.h |   26 +
 drivers/video/tegra/nvmap/nv2/nvmap_tag.c     |  116 ++
 drivers/video/tegra/nvmap/nv2/nvmap_tag.h     |   82 +
 drivers/video/tegra/nvmap/nv2/nvmap_vma.c     |  162 ++
 drivers/video/tegra/nvmap/nv2/nvmap_vma.h     |   43 +
 drivers/video/tegra/nvmap/nvmap.c             |  315 ++++
 drivers/video/tegra/nvmap/nvmap_alloc.c       | 1047 +++++++++++++
 drivers/video/tegra/nvmap/nvmap_cache.c       |  667 ++++++++
 .../tegra/nvmap/nvmap_cache_nvmap_t18x.c      |   73 +
 drivers/video/tegra/nvmap/nvmap_cache_t19x.c  |   63 +
 drivers/video/tegra/nvmap/nvmap_carveout.c    |  152 ++
 drivers/video/tegra/nvmap/nvmap_dev.c         | 1378 +++++++++++++++++
 drivers/video/tegra/nvmap/nvmap_dmabuf.c      |  794 ++++++++++
 drivers/video/tegra/nvmap/nvmap_dmabuf_t19x.c |   88 ++
 drivers/video/tegra/nvmap/nvmap_fault.c       |  292 ++++
 drivers/video/tegra/nvmap/nvmap_handle.c      |  408 +++++
 drivers/video/tegra/nvmap/nvmap_heap.c        |  582 +++++++
 drivers/video/tegra/nvmap/nvmap_heap.h        |   57 +
 drivers/video/tegra/nvmap/nvmap_init.c        |  622 ++++++++
 drivers/video/tegra/nvmap/nvmap_init_t19x.c   |  308 ++++
 drivers/video/tegra/nvmap/nvmap_ioctl.c       | 1057 +++++++++++++
 drivers/video/tegra/nvmap/nvmap_ioctl.h       |   72 +
 drivers/video/tegra/nvmap/nvmap_mm.c          |  266 ++++
 drivers/video/tegra/nvmap/nvmap_pp.c          |  745 +++++++++
 drivers/video/tegra/nvmap/nvmap_priv.h        |  770 +++++++++
 drivers/video/tegra/nvmap/nvmap_stats.c       |  106 ++
 drivers/video/tegra/nvmap/nvmap_stats.h       |   45 +
 drivers/video/tegra/nvmap/nvmap_tag.c         |  113 ++
 include/linux/nvmap.h                         |  110 ++
 include/linux/platform/tegra/tegra_fd.h       |   23 +
 include/linux/tegra-mce.h                     |  128 ++
 include/trace/events/nvmap.h                  |  798 ++++++++++
 include/uapi/linux/nvmap.h                    |  418 +++++
 83 files changed, 23261 insertions(+)
 create mode 100644 drivers/video/tegra/Makefile
 create mode 100644 drivers/video/tegra/nvmap/Kconfig
 create mode 100644 drivers/video/tegra/nvmap/Makefile
 create mode 100644 drivers/video/tegra/nvmap/nv2/Kconfig
 create mode 100644 drivers/video/tegra/nvmap/nv2/Makefile
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_cache.c
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_cache.h
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_cache_nvmap_t18x.c
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_cache_t19x.c
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_carveout.c
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_carveout.h
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_client.c
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_client.h
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_dev.c
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_dev.h
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_dmabuf.c
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_dmabuf.h
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_dmabuf_ops.c
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_dmabuf_t19x.c
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_fault.c
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_handle.c
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_handle.h
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_handle_map.c
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_handle_map_ops.c
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_handle_mm.c
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_handle_print.c
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_handle_priv.h
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_handle_ref.c
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_handle_ref.h
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_handle_vma.c
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_heap.c
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_heap.h
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_heap_alloc.c
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_heap_alloc.h
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_helper.c
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_init.c
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_init.h
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_init_t19x.c
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_ioctl.c
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_ioctl.h
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_ioctl_cache.c
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_misc.h
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_page_color.c
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_page_color.h
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_pp.c
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_pp.h
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_stats.c
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_stats.h
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_structs.h
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_tag.c
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_tag.h
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_vma.c
 create mode 100644 drivers/video/tegra/nvmap/nv2/nvmap_vma.h
 create mode 100644 drivers/video/tegra/nvmap/nvmap.c
 create mode 100644 drivers/video/tegra/nvmap/nvmap_alloc.c
 create mode 100644 drivers/video/tegra/nvmap/nvmap_cache.c
 create mode 100644 drivers/video/tegra/nvmap/nvmap_cache_nvmap_t18x.c
 create mode 100644 drivers/video/tegra/nvmap/nvmap_cache_t19x.c
 create mode 100644 drivers/video/tegra/nvmap/nvmap_carveout.c
 create mode 100644 drivers/video/tegra/nvmap/nvmap_dev.c
 create mode 100644 drivers/video/tegra/nvmap/nvmap_dmabuf.c
 create mode 100644 drivers/video/tegra/nvmap/nvmap_dmabuf_t19x.c
 create mode 100644 drivers/video/tegra/nvmap/nvmap_fault.c
 create mode 100644 drivers/video/tegra/nvmap/nvmap_handle.c
 create mode 100644 drivers/video/tegra/nvmap/nvmap_heap.c
 create mode 100644 drivers/video/tegra/nvmap/nvmap_heap.h
 create mode 100644 drivers/video/tegra/nvmap/nvmap_init.c
 create mode 100644 drivers/video/tegra/nvmap/nvmap_init_t19x.c
 create mode 100644 drivers/video/tegra/nvmap/nvmap_ioctl.c
 create mode 100644 drivers/video/tegra/nvmap/nvmap_ioctl.h
 create mode 100644 drivers/video/tegra/nvmap/nvmap_mm.c
 create mode 100644 drivers/video/tegra/nvmap/nvmap_pp.c
 create mode 100644 drivers/video/tegra/nvmap/nvmap_priv.h
 create mode 100644 drivers/video/tegra/nvmap/nvmap_stats.c
 create mode 100644 drivers/video/tegra/nvmap/nvmap_stats.h
 create mode 100644 drivers/video/tegra/nvmap/nvmap_tag.c
 create mode 100644 include/linux/nvmap.h
 create mode 100644 include/linux/platform/tegra/tegra_fd.h
 create mode 100644 include/linux/tegra-mce.h
 create mode 100644 include/trace/events/nvmap.h
 create mode 100644 include/uapi/linux/nvmap.h

diff --git a/drivers/video/Kconfig b/drivers/video/Kconfig
index 1eb755a94940..66e55f4ef9e3 100644
--- a/drivers/video/Kconfig
+++ b/drivers/video/Kconfig
@@ -72,5 +72,8 @@ if FB_CORE || SGI_NEWPORT_CONSOLE
 
 endif
 
+if ARCH_TEGRA
+	source "drivers/video/tegra/nvmap/Kconfig"
+endif
 
 endmenu
diff --git a/drivers/video/Makefile b/drivers/video/Makefile
index 6bbf87c1b579..5ece1298ac80 100644
--- a/drivers/video/Makefile
+++ b/drivers/video/Makefile
@@ -7,6 +7,7 @@ obj-$(CONFIG_VGASTATE)            += vgastate.o
 obj-$(CONFIG_VIDEO_CMDLINE)       += cmdline.o
 obj-$(CONFIG_VIDEO_NOMODESET)     += nomodeset.o
 obj-$(CONFIG_HDMI)                += hdmi.o
+obj-$(CONFIG_ARCH_TEGRA) += tegra/
 
 screen_info-y			  := screen_info_generic.o
 screen_info-$(CONFIG_PCI)         += screen_info_pci.o
diff --git a/drivers/video/tegra/Makefile b/drivers/video/tegra/Makefile
new file mode 100644
index 000000000000..8d029ce6dbf6
--- /dev/null
+++ b/drivers/video/tegra/Makefile
@@ -0,0 +1,3 @@
+obj-$(CONFIG_TEGRA_NVMAP) += nvmap/
+
+GCOV_PROFILE := y
diff --git a/drivers/video/tegra/nvmap/Kconfig b/drivers/video/tegra/nvmap/Kconfig
new file mode 100644
index 000000000000..e94da6406a93
--- /dev/null
+++ b/drivers/video/tegra/nvmap/Kconfig
@@ -0,0 +1,92 @@
+menuconfig TEGRA_NVMAP
+	bool "Tegra GPU memory management driver (nvmap)"
+	select ARM_DMA_USE_IOMMU if IOMMU_API
+	select DMA_SHARED_BUFFER
+	select CRYPTO_LZO
+	default y
+	help
+	  Say Y here to include the memory management driver for the Tegra
+	  GPU, multimedia and display subsystems
+
+if TEGRA_NVMAP
+
+config TEGRA_NVMAP_V2
+	bool "Use NVMAP version 2"
+	default n
+	help
+	  say Y here to use nvmap version 2 which has been rewritten for
+	  code clarify and safety.
+
+config NVMAP_PAGE_POOLS
+	bool "Use page pools to reduce allocation overhead"
+	default y
+	help
+	  say Y here to reduce the alloction overhead, which is significant
+	  for uncached, writecombine and inner cacheable memories as it
+	  involves changing page attributes during every allocation per page
+	  and flushing cache. Alloc time is reduced by allcoating the pages
+	  ahead and keeping them aside. The reserved pages would be released
+	  when system is low on memory and acquired back during release of
+	  memory.
+
+config NVMAP_PAGE_POOL_DEBUG
+	bool "Debugging for page pools"
+	depends on NVMAP_PAGE_POOLS
+	help
+	  Say Y here to include some debugging info in the page pools. This
+	  adds a bit of unnecessary overhead so only enable this is you
+	  suspect there is an issue with the nvmap page pools.
+
+config NVMAP_PAGE_POOL_SIZE
+	depends on NVMAP_PAGE_POOLS
+	hex "Page pool size in pages"
+	default 0x0
+
+config NVMAP_COLOR_PAGES
+	bool "Color pages allocated"
+	help
+	  Say Y here to enable page coloring.
+	  Page coloring rearranges the pages allocated based on the color
+	  of the page. It can improve memory access performance.
+	  The coloring option enable can optionally overallocate a portion of
+	  reqeusted allcoation size to improve the probabilty of better
+	  page coloring. If unsure, say Y.
+
+config NVMAP_CACHE_MAINT_BY_SET_WAYS
+	bool "Enable cache maintenance by set/ways"
+	help
+	 Say Y here to reduce cache maintenance overhead by MVA.
+	 This helps in reducing cache maintenance overhead in the systems,
+	 where inner cache includes only L1. For the systems, where inner cache
+	 includes L1 and L2, keep this option disabled.
+
+config NVMAP_FD_START
+	hex "FD number to start allocation from"
+	default 0x400
+	help
+	  NvMap handles are represented with FD's in the user processes.
+	  To avoid Linux FD usage limitations, NvMap allocates FD starting
+	  from this number.
+
+config NVMAP_DEFER_FD_RECYCLE
+	bool "Defer FD recycle"
+	help
+	  Say Y here to enable deferred FD recycle.
+	  A released nvmap handle would release memory and FD. This FD
+	  can be reused immediately for subsequent nvmap allocation req in
+	  the same process. Any buggy code in client process that continues to
+	  use FD of released allocation would continue to use new allocation
+	  and can lead to undesired consequences, which can be hard to debug.
+	  Enabling this option would defer recycling FD for longer time and
+	  allows debugging incorrect FD references by clients by returning errors
+	  for the accesses that occur after handle/FD release.
+
+config NVMAP_DEFER_FD_RECYCLE_MAX_FD
+	hex "FD number to start free FD recycle"
+	depends on NVMAP_DEFER_FD_RECYCLE
+	default 0x8000
+	help
+	  Once last allocated FD reaches this number, allocation of subsequent
+	  FD's start from NVMAP_START_FD.
+
+endif
diff --git a/drivers/video/tegra/nvmap/Makefile b/drivers/video/tegra/nvmap/Makefile
new file mode 100644
index 000000000000..a6b2db19af81
--- /dev/null
+++ b/drivers/video/tegra/nvmap/Makefile
@@ -0,0 +1,51 @@
+GCOV_PROFILE := y
+
+
+subdir-ccflags-y := -Werror
+
+USE_NV2 := n
+
+ifeq ($(CONFIG_TEGRA_NVMAP_V2),y)
+
+obj-y += nv2/
+
+else
+
+ifeq ($(CONFIG_ARM64),y)
+ccflags-y += -Iarch/arm/mach-tegra/include
+endif
+
+ccflags-y += -Iarch/arm/mach-tegra
+ccflags-y += -I$(srctree.nvidia)/drivers/video/tegra/nvmap/
+ccflags-y += -I$(srctree.nvidia)/include/
+
+obj-y += nvmap.o
+obj-y += nvmap_alloc.o
+obj-y += nvmap_cache.o
+obj-y += nvmap_dev.o
+obj-y += nvmap_dmabuf.o
+obj-y += nvmap_fault.o
+obj-y += nvmap_handle.o
+obj-y += nvmap_heap.o
+obj-y += nvmap_ioctl.o
+obj-y += nvmap_init.o
+obj-y += nvmap_tag.o
+obj-y += nvmap_mm.o
+obj-y += nvmap_stats.o
+obj-y += nvmap_carveout.o
+
+obj-$(CONFIG_NVMAP_PAGE_POOLS) += nvmap_pp.o
+
+ifeq ($(CONFIG_ARCH_TEGRA_18x_SOC),y)
+obj-y += nvmap_cache_nvmap_t18x.o
+endif
+
+ifdef CONFIG_ARCH_TEGRA_19x_SOC
+
+obj-y += nvmap_init_t19x.o
+obj-y += nvmap_dmabuf_t19x.o
+obj-y += nvmap_cache_t19x.o
+
+endif
+
+endif
diff --git a/drivers/video/tegra/nvmap/nv2/Kconfig b/drivers/video/tegra/nvmap/nv2/Kconfig
new file mode 100644
index 000000000000..8bac18bf62a8
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/Kconfig
@@ -0,0 +1,86 @@
+menuconfig TEGRA_NVMAP
+	bool "Tegra GPU memory management driver (nvmap)"
+	select ARM_DMA_USE_IOMMU if IOMMU_API
+	select DMA_SHARED_BUFFER
+	select CRYPTO_LZO
+	default y
+	help
+	  Say Y here to include the memory management driver for the Tegra
+	  GPU, multimedia and display subsystems
+
+if TEGRA_NVMAP
+
+config NVMAP_PAGE_POOLS
+	bool "Use page pools to reduce allocation overhead"
+	default y
+	help
+	  say Y here to reduce the alloction overhead, which is significant
+	  for uncached, writecombine and inner cacheable memories as it
+	  involves changing page attributes during every allocation per page
+	  and flushing cache. Alloc time is reduced by allcoating the pages
+	  ahead and keeping them aside. The reserved pages would be released
+	  when system is low on memory and acquired back during release of
+	  memory.
+
+config NVMAP_PAGE_POOL_DEBUG
+	bool "Debugging for page pools"
+	depends on NVMAP_PAGE_POOLS
+	help
+	  Say Y here to include some debugging info in the page pools. This
+	  adds a bit of unnecessary overhead so only enable this is you
+	  suspect there is an issue with the nvmap page pools.
+
+config NVMAP_PAGE_POOL_SIZE
+	depends on NVMAP_PAGE_POOLS
+	hex "Page pool size in pages"
+	default 0x0
+
+config NVMAP_COLOR_PAGES
+	bool "Color pages allocated"
+	default y
+	help
+	  Say Y here to enable page coloring.
+	  Page coloring rearranges the pages allocated based on the color
+	  of the page. It can improve memory access performance.
+	  The coloring option enable can optionally overallocate a portion of
+	  reqeusted allcoation size to improve the probabilty of better
+	  page coloring. If unsure, say Y.
+
+config NVMAP_CACHE_MAINT_BY_SET_WAYS
+	bool "Enable cache maintenance by set/ways"
+	help
+	 Say Y here to reduce cache maintenance overhead by MVA.
+	 This helps in reducing cache maintenance overhead in the systems,
+	 where inner cache includes only L1. For the systems, where inner cache
+	 includes L1 and L2, keep this option disabled.
+
+config NVMAP_FD_START
+	hex "FD number to start allocation from"
+	default 0x400
+	help
+	  NvMap handles are represented with FD's in the user processes.
+	  To avoid Linux FD usage limitations, NvMap allocates FD starting
+	  from this number.
+
+config NVMAP_DEFER_FD_RECYCLE
+	bool "Defer FD recycle"
+	help
+	  Say Y here to enable deferred FD recycle.
+	  A released nvmap handle would release memory and FD. This FD
+	  can be reused immediately for subsequent nvmap allocation req in
+	  the same process. Any buggy code in client process that continues to
+	  use FD of released allocation would continue to use new allocation
+	  and can lead to undesired consequences, which can be hard to debug.
+	  Enabling this option would defer recycling FD for longer time and
+	  allows debugging incorrect FD references by clients by returning errors
+	  for the accesses that occur after handle/FD release.
+
+config NVMAP_DEFER_FD_RECYCLE_MAX_FD
+	hex "FD number to start free FD recycle"
+	depends on NVMAP_DEFER_FD_RECYCLE
+	default 0x8000
+	help
+	  Once last allocated FD reaches this number, allocation of subsequent
+	  FD's start from NVMAP_START_FD.
+
+endif
diff --git a/drivers/video/tegra/nvmap/nv2/Makefile b/drivers/video/tegra/nvmap/nv2/Makefile
new file mode 100644
index 000000000000..9b0b0194f239
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/Makefile
@@ -0,0 +1,50 @@
+GCOV_PROFILE := y
+
+subdir-ccflags-y := -Werror
+
+ifeq ($(CONFIG_ARM64),y)
+ccflags-y += -Iarch/arm/mach-tegra/include
+endif
+
+ccflags-y += -Iarch/arm/mach-tegra
+ccflags-y += -I$(srctree.nvidia)/drivers/video/tegra/nvmap/
+ccflags-y += -I$(srctree.nvidia)/include/
+
+obj-y += nvmap_dev.o
+obj-y += nvmap_fault.o
+obj-y += nvmap_heap.o
+obj-y += nvmap_init.o
+obj-y += nvmap_tag.o
+obj-y += nvmap_stats.o
+obj-y += nvmap_client.o
+obj-y += nvmap_dmabuf.o
+obj-y += nvmap_dmabuf_ops.o
+obj-y += nvmap_handle.o
+obj-y += nvmap_handle_mm.o
+obj-y += nvmap_handle_map_ops.o
+obj-y += nvmap_handle_vma.o
+obj-y += nvmap_handle_print.o
+obj-y += nvmap_handle_ref.o
+obj-y += nvmap_heap_alloc.o
+obj-y += nvmap_ioctl.o
+obj-y += nvmap_ioctl_cache.o
+obj-y += nvmap_vma.o
+obj-y += nvmap_cache.o
+obj-y += nvmap_carveout.o
+obj-y += nvmap_handle_map.o
+obj-y += nvmap_helper.o
+obj-y += nvmap_page_color.o
+
+obj-$(CONFIG_NVMAP_PAGE_POOLS) += nvmap_pp.o
+
+ifeq ($(CONFIG_ARCH_TEGRA_18x_SOC),y)
+obj-y += nvmap_cache_nvmap_t18x.o
+endif
+
+ifdef CONFIG_ARCH_TEGRA_19x_SOC
+
+obj-y += nvmap_init_t19x.o
+obj-y += nvmap_dmabuf_t19x.o
+obj-y += nvmap_cache_t19x.o
+
+endif
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_cache.c b/drivers/video/tegra/nvmap/nv2/nvmap_cache.c
new file mode 100644
index 000000000000..90a1bb0bf881
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_cache.c
@@ -0,0 +1,383 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_cache.c
+ *
+ * Copyright (c) 2011-2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"nvmap: %s() " fmt, __func__
+
+#include <linux/highmem.h>
+#include <linux/io.h>
+#include <linux/debugfs.h>
+#include <linux/of.h>
+#include <linux/version.h>
+#include <linux/slab.h>
+#include <linux/tegra-mce.h>
+
+#include <soc/tegra/chip-id.h>
+
+#include <trace/events/nvmap.h>
+
+#include "nvmap_tag.h"
+#include "nvmap_misc.h"
+#include "nvmap_cache.h"
+#include "nvmap_handle.h"
+#include "nvmap_stats.h"
+
+typedef void (*nvmap_setup_chip_cache_fn)(struct nvmap_chip_cache_op *);
+
+extern struct of_device_id __nvmapcache_of_table;
+
+#define NVMAP_CACHE_OF_DECLARE(compat, fn) \
+	_OF_DECLARE(nvmapcache, nvmapcache_of, compat, fn, \
+			nvmap_setup_chip_cache_fn)
+
+#ifndef CONFIG_NVMAP_CACHE_MAINT_BY_SET_WAYS
+/* This is basically the L2 cache size but may be tuned as per requirement */
+size_t cache_maint_inner_threshold = SIZE_MAX;
+int nvmap_cache_maint_by_set_ways;
+#else
+int nvmap_cache_maint_by_set_ways = 1;
+size_t cache_maint_inner_threshold = 8 * SZ_2M;
+#endif
+
+struct static_key nvmap_disable_vaddr_for_cache_maint;
+
+static void nvmap_flush_dcache_all(void *dummy)
+{
+#if defined(CONFIG_DENVER_CPU)
+	u64 id_afr0;
+	u64 midr;
+
+	asm volatile ("mrs %0, MIDR_EL1" : "=r"(midr));
+	/* check if current core is a Denver processor */
+	if ((midr & 0xFF8FFFF0) == 0x4e0f0000) {
+		asm volatile ("mrs %0, ID_AFR0_EL1" : "=r"(id_afr0));
+		/* check if complete cache flush through msr is supported */
+		if (likely((id_afr0 & 0xf00) == 0x100)) {
+			asm volatile ("msr s3_0_c15_c13_0, %0" : : "r" (0));
+			asm volatile ("dsb sy");
+			return;
+		}
+	}
+#endif
+	tegra_flush_dcache_all(NULL);
+}
+
+void nvmap_cache_inner_flush_all(void) {
+	nvmap_flush_dcache_all(NULL);
+}
+
+// TODO: Clean up these global function pointers
+void (*inner_flush_cache_all)(void) = nvmap_cache_inner_flush_all;
+
+extern void __clean_dcache_louis(void *);
+static void nvmap_inner_clean_cache_all(void)
+{
+#ifdef CONFIG_ARCH_TEGRA_210_SOC
+	on_each_cpu(__clean_dcache_louis, NULL, 1);
+#endif
+	tegra_clean_dcache_all(NULL);
+}
+void (*inner_clean_cache_all)(void) = nvmap_inner_clean_cache_all;
+
+static void nvmap_cache_of_setup(struct nvmap_chip_cache_op *op)
+{
+	op->inner_clean_cache_all = nvmap_inner_clean_cache_all;
+	op->inner_flush_cache_all = nvmap_cache_inner_flush_all;
+	op->name = kstrdup("set/ways", GFP_KERNEL);
+	BUG_ON(!op->name);
+}
+NVMAP_CACHE_OF_DECLARE("nvidia,carveouts", nvmap_cache_of_setup);
+
+void nvmap_select_cache_ops(struct device *dev)
+{
+	struct nvmap_chip_cache_op op;
+	bool match_found = false;
+	const struct of_device_id *matches = &__nvmapcache_of_table;
+
+	memset(&op, 0, sizeof(op));
+
+	for (; matches; matches++) {
+		if (of_device_is_compatible(dev->of_node,
+					    matches->compatible)) {
+			const nvmap_setup_chip_cache_fn init_fn = matches->data;
+			init_fn(&op);
+			match_found = true;
+			break;
+		}
+	}
+
+	if (WARN_ON(match_found == false)) {
+		pr_err("%s: no cache ops found\n",__func__);
+		return;
+	}
+	inner_flush_cache_all = op.inner_flush_cache_all;
+	inner_clean_cache_all = op.inner_clean_cache_all;
+	pr_info("nvmap cache ops set to %s\n", op.name);
+	kfree(op.name);
+
+	if (inner_clean_cache_all && (op.flags & CALL_CLEAN_CACHE_ON_INIT)) {
+		pr_info("calling cache operation %pF\n",
+					inner_clean_cache_all);
+		inner_clean_cache_all();
+	}
+
+	if (inner_flush_cache_all && (op.flags & CALL_FLUSH_CACHE_ON_INIT)) {
+		pr_info("calling cache operation %pF\n",
+					inner_flush_cache_all);
+		inner_flush_cache_all();
+	}
+}
+
+__weak void nvmap_override_cache_ops(void)
+{
+	nvmap_select_cache_ops(nvmap_dev->dev_user.parent);
+}
+
+__weak void nvmap_handle_get_cacheability(struct nvmap_handle *h,
+		bool *inner, bool *outer)
+{
+	u32 flags = nvmap_handle_flags(h);
+
+	*inner = flags == NVMAP_HANDLE_CACHEABLE ||
+		 flags == NVMAP_HANDLE_INNER_CACHEABLE;
+	*outer = flags == NVMAP_HANDLE_CACHEABLE;
+}
+
+/*
+ * FIXME:
+ *
+ *   __clean_dcache_page() is only available on ARM64 (well, we haven't
+ *   implemented it on ARMv7).
+ */
+static void cache_clean_page(struct page *page)
+{
+	__clean_dcache_page(page);
+}
+
+void nvmap_cache_clean_pages(struct page **pages, int numpages)
+{
+	int i;
+
+	/* Not technically a flush but that's what nvmap knows about. */
+	nvmap_stats_inc(NS_CFLUSH_DONE, numpages << PAGE_SHIFT);
+	trace_nvmap_cache_flush(numpages << PAGE_SHIFT,
+		nvmap_stats_read(NS_ALLOC),
+		nvmap_stats_read(NS_CFLUSH_RQ),
+		nvmap_stats_read(NS_CFLUSH_DONE));
+
+	for (i = 0; i < numpages; i++)
+		cache_clean_page(pages[i]);
+}
+
+void nvmap_cache_inner_clean_all(void)
+{
+#ifdef CONFIG_ARCH_TEGRA_210_SOC
+	on_each_cpu(__clean_dcache_louis, NULL, 1);
+#endif
+	tegra_clean_dcache_all(NULL);
+
+}
+
+void nvmap_cache_inner_maint(unsigned int op, void *vaddr, size_t size)
+{
+	if (op == NVMAP_CACHE_OP_WB_INV)
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 9, 0)
+		__dma_flush_range(vaddr, vaddr + size);
+#else
+		__dma_flush_area(vaddr, size);
+#endif
+	else if (op == NVMAP_CACHE_OP_INV)
+		__dma_map_area(vaddr, size, DMA_FROM_DEVICE);
+	else
+		__dma_map_area(vaddr, size, DMA_TO_DEVICE);
+}
+
+bool nvmap_cache_can_fast_maint(unsigned long start,
+			unsigned long end, unsigned int op)
+{
+	if (!nvmap_cache_maint_by_set_ways)
+		return false;
+
+	if ((op == NVMAP_CACHE_OP_INV) ||
+		((end - start) < cache_maint_inner_threshold))
+		return false;
+	return true;
+}
+
+void nvmap_cache_fast_maint(unsigned int op)
+{
+
+	if (op == NVMAP_CACHE_OP_WB_INV)
+		nvmap_cache_inner_flush_all();
+	else if (op == NVMAP_CACHE_OP_WB)
+		nvmap_cache_inner_clean_all();
+}
+
+/*
+ * If you want to call this function with inner = false,
+ * then don't call this function at all
+ */
+int nvmap_cache_maint_phys_range(unsigned int op, phys_addr_t pstart,
+		phys_addr_t pend)
+{
+	unsigned long kaddr;
+	struct vm_struct *area = NULL;
+	phys_addr_t cur_addr;
+
+	/* TODO: Move this outside of everywhere this is called */
+	if (nvmap_cache_can_fast_maint((unsigned long)pstart,
+				 (unsigned long)pend, op)) {
+		nvmap_cache_fast_maint(op);
+		return 0;
+	}
+
+	area = alloc_vm_area(PAGE_SIZE, NULL);
+	if (!area)
+		return -ENOMEM;
+	kaddr = (ulong)area->addr;
+
+	cur_addr = pstart;
+	while (cur_addr < pend) {
+		phys_addr_t next = (cur_addr + PAGE_SIZE) & PAGE_MASK;
+		void *base = (void *)kaddr + (cur_addr & ~PAGE_MASK);
+
+		next = min(next, pend);
+		ioremap_page_range(kaddr, kaddr + PAGE_SIZE,
+			cur_addr, PG_PROT_KERNEL);
+		nvmap_cache_inner_maint(op, base, next - cur_addr);
+		cur_addr = next;
+		unmap_kernel_range(kaddr, PAGE_SIZE);
+	}
+
+	free_vm_area(area);
+	return 0;
+}
+
+void nvmap_cache_maint_heap_page_outer(struct page **pages,
+				unsigned int op,
+				unsigned long start, unsigned long end)
+{
+	while (start < end) {
+		struct page *page;
+		phys_addr_t paddr;
+		unsigned long next;
+		unsigned long off;
+		size_t size;
+		int ret;
+
+		page = nvmap_to_page(pages[start >> PAGE_SHIFT]);
+		next = min(((start + PAGE_SIZE) & PAGE_MASK), end);
+		off = start & ~PAGE_MASK;
+		size = next - start;
+		paddr = page_to_phys(page) + off;
+
+		ret = nvmap_cache_maint_phys_range(op, paddr, paddr + size);
+		BUG_ON(ret != 0);
+		start = next;
+	}
+}
+
+void nvmap_cache_maint_inner(unsigned int op, void *vaddr, size_t size)
+{
+	if (op == NVMAP_CACHE_OP_WB_INV)
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 9, 0)
+		__dma_flush_range(vaddr, vaddr + size);
+#else
+		__dma_flush_area(vaddr, size);
+#endif
+	else if (op == NVMAP_CACHE_OP_INV)
+		__dma_map_area(vaddr, size, DMA_FROM_DEVICE);
+	else
+		__dma_map_area(vaddr, size, DMA_TO_DEVICE);
+}
+
+static int cache_inner_threshold_show(struct seq_file *m, void *v)
+{
+	if (nvmap_cache_maint_by_set_ways)
+		seq_printf(m, "%zuB\n", cache_maint_inner_threshold);
+	else
+		seq_printf(m, "%zuB\n", SIZE_MAX);
+	return 0;
+}
+
+static int cache_inner_threshold_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, cache_inner_threshold_show, inode->i_private);
+}
+
+static ssize_t cache_inner_threshold_write(struct file *file,
+					const char __user *buffer,
+					size_t count, loff_t *pos)
+{
+	int ret;
+	struct seq_file *p = file->private_data;
+	char str[] = "0123456789abcdef";
+
+	count = min_t(size_t, strlen(str), count);
+	if (copy_from_user(str, buffer, count))
+		return -EINVAL;
+
+	if (!nvmap_cache_maint_by_set_ways)
+		return -EINVAL;
+
+	mutex_lock(&p->lock);
+	ret = sscanf(str, "%16zu", &cache_maint_inner_threshold);
+	mutex_unlock(&p->lock);
+	if (ret != 1)
+		return -EINVAL;
+
+	pr_debug("nvmap:cache_maint_inner_threshold is now :%zuB\n",
+			cache_maint_inner_threshold);
+	return count;
+}
+
+static const struct file_operations cache_inner_threshold_fops = {
+	.open		= cache_inner_threshold_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+	.write		= cache_inner_threshold_write,
+};
+
+int nvmap_cache_debugfs_init(struct dentry *nvmap_root)
+{
+	struct dentry *cache_root;
+
+	if (!nvmap_root)
+		return -ENODEV;
+
+	cache_root = debugfs_create_dir("cache", nvmap_root);
+	if (!cache_root)
+		return -ENODEV;
+
+	if (nvmap_cache_maint_by_set_ways) {
+		debugfs_create_x32("nvmap_cache_maint_by_set_ways",
+				   S_IRUSR | S_IWUSR,
+				   cache_root,
+				   &nvmap_cache_maint_by_set_ways);
+
+	debugfs_create_file("cache_maint_inner_threshold",
+			    S_IRUSR | S_IWUSR,
+			    cache_root,
+			    NULL,
+			    &cache_inner_threshold_fops);
+	}
+
+	debugfs_create_atomic_t("nvmap_disable_vaddr_for_cache_maint",
+				S_IRUSR | S_IWUSR,
+				cache_root,
+				&nvmap_disable_vaddr_for_cache_maint.enabled);
+
+	return 0;
+}
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_cache.h b/drivers/video/tegra/nvmap/nv2/nvmap_cache.h
new file mode 100644
index 000000000000..236b0fea8019
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_cache.h
@@ -0,0 +1,89 @@
+/*
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __NVMAP_CACHE_H
+#define __NVMAP_CACHE_H
+
+#include "nvmap_structs.h"
+
+#define CALL_CLEAN_CACHE_ON_INIT 1
+#define CALL_FLUSH_CACHE_ON_INIT 2
+
+#ifdef CONFIG_ARM64
+#define PG_PROT_KERNEL PAGE_KERNEL
+#define FLUSH_DCACHE_AREA __flush_dcache_area
+#define outer_flush_range(s, e)
+#define outer_inv_range(s, e)
+#define outer_clean_range(s, e)
+#define outer_flush_all()
+#define outer_clean_all()
+extern void __clean_dcache_page(struct page *);
+#else
+#define PG_PROT_KERNEL pgprot_kernel
+#define FLUSH_DCACHE_AREA __cpuc_flush_dcache_area
+extern void __flush_dcache_page(struct address_space *, struct page *);
+#endif
+
+/*
+ * TODO: put op at beginning of APIS
+ * 	- remove the cache_maint_op
+ *
+ */
+
+struct cache_maint_op;
+
+int nvmap_cache_maint(struct cache_maint_op *cache_work);
+void nvmap_cache_maint_inner(unsigned int op, void *vaddr, size_t size);
+
+bool nvmap_cache_can_fast_maint(unsigned long start,
+			unsigned long end, unsigned int op);
+void nvmap_cache_fast_maint(unsigned int op);
+
+void nvmap_cache_maint_heap_page_outer(struct page **pages,
+				unsigned int op,
+				unsigned long start, unsigned long end);
+
+void nvmap_cache_clean_pages(struct page **pages, int numpages);
+
+int nvmap_cache_maint_phys_range(unsigned int op, phys_addr_t pstart,
+					phys_addr_t pend);
+
+void nvmap_cache_inner_clean_all(void);
+void nvmap_cache_inner_flush_all(void);
+
+struct nvmap_chip_cache_op {
+	void (*inner_clean_cache_all)(void);
+	void (*inner_flush_cache_all)(void);
+	const char *name;
+	int flags;
+};
+
+// TODO: Rename this
+void nvmap_handle_get_cacheability(struct nvmap_handle *h,
+		bool *inner, bool *outer);
+
+typedef void (*nvmap_setup_chip_cache_fn)(struct nvmap_chip_cache_op *);
+
+extern struct of_device_id __nvmapcache_of_table;
+
+#define NVMAP_CACHE_OF_DECLARE(compat, fn) \
+	_OF_DECLARE(nvmapcache, nvmapcache_of, compat, fn, \
+			nvmap_setup_chip_cache_fn)
+
+extern size_t cache_maint_inner_threshold;
+extern int nvmap_cache_maint_by_set_ways;
+
+int nvmap_cache_debugfs_init(struct dentry *nvmap_root);
+void nvmap_override_cache_ops(void);
+
+#endif /* __NVMAP_CACHE_H */
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_cache_nvmap_t18x.c b/drivers/video/tegra/nvmap/nv2/nvmap_cache_nvmap_t18x.c
new file mode 100644
index 000000000000..4a13362418fb
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_cache_nvmap_t18x.c
@@ -0,0 +1,66 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_cache.c
+ *
+ * Copyright (c) 2015-2018, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"nvmap: %s() " fmt, __func__
+
+#include <linux/types.h>
+#include <linux/tegra-mce.h>
+#include <soc/tegra/chip-id.h>
+
+#include "nvmap_cache.h"
+
+static void nvmap_roc_flush_cache(void)
+{
+	int ret;
+
+	if (!tegra_platform_is_silicon() && !tegra_platform_is_fpga()) {
+		pr_info_once("ROC flush supported on only FPGA and silicon\n");
+		pr_info_once("Fall back to flush by VA\n");
+		nvmap_cache_maint_by_set_ways = 0;
+		return;
+	}
+
+	ret = tegra_flush_dcache_all(NULL);
+	if (ret) {
+		pr_info_once("ROC flush failed with %u\n", ret);
+		pr_info_once("Fall back to flush by VA\n");
+		nvmap_cache_maint_by_set_ways = 0;
+	}
+}
+
+static void nvmap_roc_clean_cache(void)
+{
+	int ret;
+
+	if (!tegra_platform_is_silicon() && !tegra_platform_is_fpga()) {
+		pr_info_once("ROC flush supported on only FPGA and silicon\n");
+		return;
+	}
+
+	ret = tegra_clean_dcache_all(NULL);
+	if (ret) {
+		pr_info_once("ROC clean failed with %u\n", ret);
+		pr_info_once("Fall back to clean by VA\n");
+		nvmap_cache_maint_by_set_ways = 0;
+	}
+}
+
+static void nvmap_setup_t18x_cache_ops(struct nvmap_chip_cache_op *op)
+{
+	op->inner_flush_cache_all = nvmap_roc_flush_cache;
+	op->inner_clean_cache_all = nvmap_roc_clean_cache;
+	op->name = kstrdup("roc", GFP_KERNEL);
+}
+NVMAP_CACHE_OF_DECLARE("nvidia,carveouts-t18x", nvmap_setup_t18x_cache_ops);
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_cache_t19x.c b/drivers/video/tegra/nvmap/nv2/nvmap_cache_t19x.c
new file mode 100644
index 000000000000..445771eed0ab
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_cache_t19x.c
@@ -0,0 +1,67 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_cache_t19x.c
+ *
+ * Copyright (c) 2016-2021, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"nvmap: %s() " fmt, __func__
+
+#include <linux/miscdevice.h>
+#include <linux/nvmap_t19x.h>
+#include <linux/tegra-mce.h>
+#include <linux/of.h>
+
+#include "nvmap_handle.h"
+#include "nvmap_cache.h"
+#include "nvmap_dev.h"
+
+struct static_key nvmap_updated_cache_config;
+
+void nvmap_handle_get_cacheability(struct nvmap_handle *h,
+		bool *inner, bool *outer)
+{
+	struct nvmap_handle_t19x *handle_t19x;
+	struct device *dev = nvmap_dev->dev_user.parent;
+	u32 flags = nvmap_handle_flags(h);
+
+	handle_t19x = dma_buf_get_drvdata(nvmap_handle_to_dmabuf(h), dev);
+	if (handle_t19x && atomic_read(&handle_t19x->nc_pin)) {
+		*inner = *outer = false;
+		return;
+	}
+
+	*inner = flags == NVMAP_HANDLE_CACHEABLE ||
+		 flags == NVMAP_HANDLE_INNER_CACHEABLE;
+	*outer = flags == NVMAP_HANDLE_CACHEABLE;
+}
+
+static void nvmap_t19x_flush_cache(void)
+{
+	void *unused = NULL;
+
+	tegra_flush_dcache_all(unused);
+}
+
+static void nvmap_t19x_clean_cache(void)
+{
+	void *unused = NULL;
+
+	tegra_clean_dcache_all(unused);
+}
+
+static void nvmap_setup_t19x_cache_ops(struct nvmap_chip_cache_op *op)
+{
+	op->inner_flush_cache_all = nvmap_t19x_flush_cache;
+	op->inner_clean_cache_all = nvmap_t19x_clean_cache;
+	op->name = kstrdup("scf", GFP_KERNEL);
+}
+NVMAP_CACHE_OF_DECLARE("nvidia,carveouts-t19x", nvmap_setup_t19x_cache_ops);
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_carveout.c b/drivers/video/tegra/nvmap/nv2/nvmap_carveout.c
new file mode 100644
index 000000000000..07a4e361e4b9
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_carveout.c
@@ -0,0 +1,503 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_carveout.c
+ *
+ * Interface with nvmap carveouts
+ *
+ * Copyright (c) 2011-2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#include <linux/device.h>
+#include <linux/debugfs.h>
+#include <linux/slab.h>
+
+#include "nvmap_handle.h"
+#include "nvmap_dev.h"
+#include "nvmap_cache.h"
+#include "nvmap_misc.h"
+
+extern struct nvmap_device *nvmap_dev;
+extern struct kmem_cache *heap_block_cache;
+
+struct list_block {
+	struct nvmap_heap_block block;
+	struct list_head all_list;
+	unsigned int mem_prot;
+	phys_addr_t orig_addr;
+	size_t size;
+	size_t align;
+	struct nvmap_heap *heap;
+	struct list_head free_list;
+};
+
+struct nvmap_carveout_node {
+	unsigned int		heap_bit;
+	struct nvmap_heap	*carveout;
+	int			index;
+	phys_addr_t		base;
+	size_t			size;
+};
+
+struct nvmap_heap {
+	struct list_head all_list;
+	struct mutex lock;
+	const char *name;
+	void *arg;
+	/* heap base */
+	phys_addr_t base;
+	/* heap size */
+	size_t len;
+	struct device *cma_dev;
+	struct device *dma_dev;
+	bool is_ivm;
+	bool can_alloc; /* Used only if is_ivm == true */
+	int peer; /* Used only if is_ivm == true */
+	int vm_id; /* Used only if is_ivm == true */
+	struct nvmap_pm_ops pm_ops;
+};
+
+/* TODO fix these global variables */
+extern struct nvmap_device *nvmap_dev;
+extern const struct file_operations debug_clients_fops;
+extern const struct file_operations debug_allocations_fops;
+extern const struct file_operations debug_all_allocations_fops;
+extern const struct file_operations debug_orphan_handles_fops;
+extern const struct file_operations debug_maps_fops;
+
+int nvmap_carveout_is_ivm(struct nvmap_carveout_node *carveout)
+{
+	return carveout->heap_bit & NVMAP_HEAP_CARVEOUT_IVM;
+}
+
+int nvmap_carveout_query_peer(struct nvmap_carveout_node *carveout)
+{
+	return nvmap_query_heap_peer(carveout->carveout);
+}
+
+int nvmap_carveout_heap_bit(struct nvmap_carveout_node *carveout)
+{
+	return carveout->heap_bit;
+}
+
+int nvmap_carveout_query_heap_size(struct nvmap_carveout_node *carveout)
+{
+	return nvmap_query_heap_size(carveout->carveout);
+}
+
+int nvmap_carveout_create(const struct nvmap_platform_carveout *co)
+{
+	int i, err = 0;
+	struct nvmap_carveout_node *node;
+
+	if (!nvmap_dev->heaps) {
+		nvmap_dev->nr_carveouts = 0;
+		nvmap_dev->nr_heaps = nvmap_dev->plat->nr_carveouts + 1;
+		nvmap_dev->heaps = kzalloc(sizeof(struct nvmap_carveout_node) *
+				     nvmap_dev->nr_heaps, GFP_KERNEL);
+		if (!nvmap_dev->heaps) {
+			err = -ENOMEM;
+			pr_err("couldn't allocate carveout memory\n");
+			goto out;
+		}
+	} else if (nvmap_dev->nr_carveouts >= nvmap_dev->nr_heaps) {
+		node = krealloc(nvmap_dev->heaps,
+				sizeof(*node) * (nvmap_dev->nr_carveouts + 1),
+				GFP_KERNEL);
+		if (!node) {
+			err = -ENOMEM;
+			pr_err("nvmap heap array resize failed\n");
+			goto out;
+		}
+		nvmap_dev->heaps = node;
+		nvmap_dev->nr_heaps = nvmap_dev->nr_carveouts + 1;
+	}
+
+	for (i = 0; i < nvmap_dev->nr_heaps; i++)
+		if ((co->usage_mask != NVMAP_HEAP_CARVEOUT_IVM) &&
+		    (nvmap_dev->heaps[i].heap_bit & co->usage_mask)) {
+			pr_err("carveout %s already exists\n", co->name);
+			return -EEXIST;
+		}
+
+	node = &nvmap_dev->heaps[nvmap_dev->nr_carveouts];
+
+	node->base = round_up(co->base, PAGE_SIZE);
+	node->size = round_down(co->size -
+				(node->base - co->base), PAGE_SIZE);
+	if (!co->size)
+		goto out;
+
+	node->carveout = nvmap_heap_create(
+			nvmap_dev->dev_user.this_device, co,
+			node->base, node->size, node);
+
+	if (!node->carveout) {
+		err = -ENOMEM;
+		pr_err("couldn't create %s\n", co->name);
+		goto out;
+	}
+	node->index = nvmap_dev->nr_carveouts;
+	nvmap_dev->nr_carveouts++;
+	node->heap_bit = co->usage_mask;
+
+	if (!IS_ERR_OR_NULL(nvmap_dev->debug_root)) {
+		struct dentry *heap_root =
+			debugfs_create_dir(co->name, nvmap_dev->debug_root);
+		if (!IS_ERR_OR_NULL(heap_root)) {
+			debugfs_create_file("clients", S_IRUGO,
+				heap_root,
+				(void *)(uintptr_t)node->heap_bit,
+				&debug_clients_fops);
+			debugfs_create_file("allocations", S_IRUGO,
+				heap_root,
+				(void *)(uintptr_t)node->heap_bit,
+				&debug_allocations_fops);
+			debugfs_create_file("all_allocations", S_IRUGO,
+				heap_root,
+				(void *)(uintptr_t)node->heap_bit,
+				&debug_all_allocations_fops);
+			debugfs_create_file("orphan_handles", S_IRUGO,
+				heap_root,
+				(void *)(uintptr_t)node->heap_bit,
+				&debug_orphan_handles_fops);
+			debugfs_create_file("maps", S_IRUGO,
+				heap_root,
+				(void *)(uintptr_t)node->heap_bit,
+				&debug_maps_fops);
+			nvmap_heap_debugfs_init(heap_root,
+						node->carveout);
+		}
+	}
+out:
+	return err;
+}
+
+struct device *nvmap_heap_type_to_dev(unsigned long type)
+{
+	int i;
+	struct nvmap_carveout_node *co_heap;
+
+	for (i = 0; i < nvmap_dev->nr_carveouts; i++) {
+		co_heap = &nvmap_dev->heaps[i];
+
+		if (!(co_heap->heap_bit & type))
+			continue;
+
+		return co_heap->carveout->dma_dev;
+	}
+	return ERR_PTR(-ENODEV);
+}
+
+int heap_alloc_mem_virtualized(struct device *dev, phys_addr_t pa, size_t len)
+{
+	void *ret;
+	DEFINE_DMA_ATTRS(attrs);
+
+	dma_set_attr(DMA_ATTR_ALLOC_EXACT_SIZE, __DMA_ATTR(attrs));
+
+	ret = dma_mark_declared_memory_occupied(dev, pa, len, __DMA_ATTR(attrs));
+	if (IS_ERR(ret)) {
+		dev_err(dev, "Failed to reserve (%pa) len(%zu)\n", &pa, len);
+		return 1;
+	} else {
+		dev_dbg(dev, "reserved (%pa) len(%zu)\n", &pa, len);
+	}
+	return 0;
+}
+
+static void nvmap_free_mem(struct nvmap_heap *h, phys_addr_t base,
+				size_t len)
+{
+	struct device *dev = h->dma_dev;
+	DEFINE_DMA_ATTRS(attrs);
+
+	dma_set_attr(DMA_ATTR_ALLOC_EXACT_SIZE, __DMA_ATTR(attrs));
+	dev_dbg(dev, "Free base (%pa) size (%zu)\n", &base, len);
+#ifdef CONFIG_TEGRA_VIRTUALIZATION
+	if (h->is_ivm && !h->can_alloc) {
+		dma_mark_declared_memory_unoccupied(dev, base, len, __DMA_ATTR(attrs));
+	} else
+#endif
+	{
+		dma_free_attrs(dev, len,
+			        (void *)(uintptr_t)base,
+			        (dma_addr_t)base, __DMA_ATTR(attrs));
+	}
+}
+
+static phys_addr_t nvmap_alloc_mem(struct nvmap_heap *h, size_t len,
+				   phys_addr_t *start)
+{
+	phys_addr_t pa;
+	DEFINE_DMA_ATTRS(attrs);
+	struct device *dev = h->dma_dev;
+
+	dma_set_attr(DMA_ATTR_ALLOC_EXACT_SIZE, __DMA_ATTR(attrs));
+
+#ifdef CONFIG_TEGRA_VIRTUALIZATION
+	if (start && h->is_ivm) {
+		int err;
+
+		pa = h->base + *start;
+		err = heap_alloc_mem_virtualized(dev, pa, len);
+		if (err) {
+			return DMA_ERROR_CODE;
+		}
+		return pa;
+	}
+#endif
+
+	(void)dma_alloc_attrs(dev, len, &pa, GFP_KERNEL, __DMA_ATTR(attrs));
+	if (dma_mapping_error(dev, pa)) {
+		return pa;
+	}
+
+	dev_dbg(dev, "Allocated addr (%pa) len(%zu)\n", &pa, len);
+
+	if (!dma_is_coherent_dev(dev) && h->cma_dev) {
+		int ret;
+		ret = nvmap_cache_maint_phys_range(NVMAP_CACHE_OP_WB,
+							pa, pa + len);
+		if (ret) {
+			dev_err(dev, "cache WB on (%pa, %zu) failed\n",
+								&pa, len);
+		}
+	}
+
+	return pa;
+}
+
+/*
+ * This routine is used to flush the carveout memory from cache.
+ * Why cache flush is needed for carveout? Consider the case, where a piece of
+ * carveout is allocated as cached and released. After this, if the same memory is
+ * allocated for uncached request and the memory is not flushed out from cache.
+ * In this case, the client might pass this to H/W engine and it could start modify
+ * the memory. As this was cached earlier, it might have some portion of it in cache.
+ * During cpu request to read/write other memory, the cached portion of this memory
+ * might get flushed back to main memory and would cause corruptions, if it happens
+ * after H/W writes data to memory.
+ *
+ * But flushing out the memory blindly on each carveout allocation is redundant.
+ *
+ * In order to optimize the carveout buffer cache flushes, the following
+ * strategy is used.
+ *
+ * The whole Carveout is flushed out from cache during its initialization.
+ * During allocation, carveout buffers are not flused from cache.
+ * During deallocation, carveout buffers are flushed, if they were allocated as cached.
+ * if they were allocated as uncached/writecombined, no cache flush is needed.
+ * Just draining store buffers is enough.
+ */
+static int heap_block_flush(struct nvmap_heap_block *block, size_t len,
+							unsigned int prot)
+{
+	phys_addr_t phys = block->base;
+	phys_addr_t end = block->base + len;
+	int ret = 0;
+
+	if (prot == NVMAP_HANDLE_UNCACHEABLE
+				|| prot == NVMAP_HANDLE_WRITE_COMBINE)
+		goto out;
+
+	ret = nvmap_cache_maint_phys_range(NVMAP_CACHE_OP_WB_INV, phys, end);
+	if (ret)
+		goto out;
+out:
+	wmb();
+	return ret;
+}
+
+void nvmap_heap_block_free(struct nvmap_heap_block *b)
+{
+	struct nvmap_heap *h;
+	struct list_block *lb;
+
+	if (!b)
+		return;
+
+	h = nvmap_block_to_heap(b);
+	mutex_lock(&h->lock);
+
+	lb = container_of(b, struct list_block, block);
+	heap_block_flush(b, lb->size, lb->mem_prot);
+
+
+	list_del(&lb->all_list);
+	nvmap_free_mem(h, b->base, lb->size);
+	kmem_cache_free(heap_block_cache, lb);
+
+	/*
+	 * If this HEAP has pm_ops defined and powering off the
+	 * RAM attached with the HEAP returns error, raise warning.
+	 */
+	if (h->pm_ops.idle) {
+		if (h->pm_ops.idle() < 0)
+			WARN_ON(1);
+	}
+
+	mutex_unlock(&h->lock);
+}
+
+static struct nvmap_heap_block *heap_block_alloc(struct nvmap_heap *heap,
+					      size_t len, size_t align,
+					      unsigned int mem_prot,
+					      phys_addr_t *start)
+{
+	struct list_block *heap_block = NULL;
+	dma_addr_t dev_base;
+	struct device *dev = heap->dma_dev;
+
+	align = max_t(size_t, align, L1_CACHE_BYTES);
+
+	/* since pages are only mappable with one cache attribute,
+	 * and most allocations from carveout heaps are DMA coherent
+	 * (i.e., non-cacheable), round cacheable allocations up to
+	 * a page boundary to ensure that the physical pages will
+	 * only be mapped one way. */
+	if (mem_prot == NVMAP_HANDLE_CACHEABLE ||
+	    mem_prot == NVMAP_HANDLE_INNER_CACHEABLE) {
+		align = max_t(size_t, align, PAGE_SIZE);
+		len = PAGE_ALIGN(len);
+	}
+
+	if (heap->is_ivm)
+		align = max_t(size_t, align, NVMAP_IVM_ALIGNMENT);
+
+	heap_block = kmem_cache_zalloc(heap_block_cache, GFP_KERNEL);
+	if (!heap_block) {
+		dev_err(dev, "%s: failed to alloc heap block %s\n",
+			__func__, dev_name(dev));
+		goto fail_heap_block_alloc;
+	}
+
+	dev_base = nvmap_alloc_mem(heap, len, start);
+	if (dma_mapping_error(dev, dev_base)) {
+		dev_err(dev, "failed to alloc mem of size (%zu)\n",
+			len);
+		if (dma_is_coherent_dev(dev)) {
+			struct dma_coherent_stats stats;
+
+			dma_get_coherent_stats(dev, &stats);
+			dev_err(dev, "used:%zu,curr_size:%zu max:%zu\n",
+				stats.used, stats.size, stats.max);
+		}
+		goto fail_dma_alloc;
+	}
+
+	heap_block->block.base = dev_base;
+	heap_block->orig_addr = dev_base;
+	heap_block->size = len;
+
+	list_add_tail(&heap_block->all_list, &heap->all_list);
+	heap_block->heap = heap;
+	heap_block->mem_prot = mem_prot;
+	heap_block->align = align;
+	return &heap_block->block;
+
+fail_dma_alloc:
+	kmem_cache_free(heap_block_cache, heap_block);
+fail_heap_block_alloc:
+	return NULL;
+}
+
+static int heap_can_allocate(struct nvmap_heap *h, int peer, phys_addr_t *start)
+{
+	if (h->is_ivm) { /* Is IVM carveout? */
+		/* Check if this correct IVM heap */
+		if (peer != h->peer) {
+			return 0;
+		}
+		/* If this partition does actual allocation, it
+		 * should not specify start_offset.
+		 */
+		if (h->can_alloc && start) {
+			return 0;
+		}
+
+		/* If this partition does not do actual
+		 * allocation, it should specify start_offset.
+		 */
+		if (!h->can_alloc && !start) {
+			return 0;
+		}
+	}
+
+	/*
+	 * If this HEAP has pm_ops defined and powering on the
+	 * RAM attached with the HEAP returns error, don't
+	 * allocate from the heap and return NULL.
+	 */
+	if (h->pm_ops.busy) {
+		if (h->pm_ops.busy() < 0) {
+			pr_err("Unable to power on the heap device\n");
+			return 0;
+		}
+	}
+	return 1;
+}
+
+/* nvmap_heap_alloc: allocates a block of memory of len bytes, aligned to
+ * align bytes. */
+struct nvmap_heap_block *nvmap_carveout_alloc(struct nvmap_carveout_node *co,
+					phys_addr_t *start,
+					size_t len,
+					size_t align,
+					unsigned int prot,
+					int peer)
+{
+	struct nvmap_heap *h = co->carveout;
+	struct nvmap_heap_block *b;
+
+	mutex_lock(&h->lock);
+
+	if (!heap_can_allocate(h, peer, start)) {
+		mutex_unlock(&h->lock);
+		return NULL;
+	}
+
+	b = heap_block_alloc(h, len, align, prot, start);
+	if (!b) {
+		mutex_unlock(&h->lock);
+		return NULL;
+	}
+
+	mutex_unlock(&h->lock);
+	return b;
+}
+
+u64 nvmap_carveout_ivm(struct nvmap_carveout_node *co,
+				struct nvmap_heap_block *b, size_t len)
+{
+	struct nvmap_heap *h = co->carveout;
+	unsigned int offs;
+
+	/* Generate IVM for partition that can alloc */
+	if (h->is_ivm && h->can_alloc) {
+		offs = (b->base - h->base);
+		return nvmap_calculate_ivm_id(h->vm_id, len, offs);
+	} else {
+		return 0;
+	}
+}
+
+// This is only needed because dev doesn't have a double pointer
+// and carveout.c is the only file that knows the size of the struct
+struct nvmap_carveout_node *nvmap_carveout_index(
+				struct nvmap_carveout_node *node, int i)
+{
+	return node + i;
+}
+
+void nvmap_carveout_destroy(struct nvmap_carveout_node *node)
+{
+	nvmap_heap_destroy(node->carveout);
+}
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_carveout.h b/drivers/video/tegra/nvmap/nv2/nvmap_carveout.h
new file mode 100644
index 000000000000..9c578780c111
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_carveout.h
@@ -0,0 +1,42 @@
+/*
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __NVMAP_CARVEOUT_H
+#define __NVMAP_CARVEOUT_H
+
+#include "nvmap_structs.h"
+
+int nvmap_carveout_create(const struct nvmap_platform_carveout *co);
+void nvmap_carveout_destroy(struct nvmap_carveout_node *node);
+
+struct nvmap_heap_block *nvmap_carveout_alloc(struct nvmap_carveout_node *co,
+					phys_addr_t *start,
+					size_t len,
+					size_t align,
+					unsigned int prot,
+					int peer);
+int nvmap_carveout_is_ivm(struct nvmap_carveout_node *carveout);
+
+int nvmap_carveout_query_peer(struct nvmap_carveout_node *carveout);
+
+int nvmap_carveout_heap_bit(struct nvmap_carveout_node *carveout);
+
+int nvmap_carveout_query_heap_size(struct nvmap_carveout_node *carveout);
+
+struct nvmap_carveout_node *nvmap_carveout_index(
+				struct nvmap_carveout_node *node, int i);
+
+u64 nvmap_carveout_ivm(struct nvmap_carveout_node *co,
+				struct nvmap_heap_block *b, size_t len);
+
+#endif /* __NVMAP_CARVEOUT_H */
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_client.c b/drivers/video/tegra/nvmap/nv2/nvmap_client.c
new file mode 100644
index 000000000000..e0dbc6e37466
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_client.c
@@ -0,0 +1,580 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_handle.c
+ *
+ * Handle allocation and freeing routines for nvmap
+ *
+ * Copyright (c) 2009-2017, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"%s: " fmt, __func__
+
+#include <linux/err.h>
+#include <linux/io.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/mm.h>
+#include <linux/rbtree.h>
+#include <linux/dma-buf.h>
+#include <linux/platform/tegra/tegra_fd.h>
+#include <linux/moduleparam.h>
+#include <linux/nvmap.h>
+#include <linux/slab.h>
+#include <soc/tegra/chip-id.h>
+
+#include <asm/pgtable.h>
+
+#include <trace/events/nvmap.h>
+
+#include "nvmap_handle_ref.h"
+#include "nvmap_handle.h"
+#include "nvmap_dmabuf.h"
+#include "nvmap_client.h"
+#include "nvmap_dev.h"
+#include "nvmap_stats.h"
+
+struct nvmap_client {
+	const char			*name;
+	struct rb_root			handle_refs;
+	struct mutex			ref_lock;
+	bool				kernel_client;
+	atomic_t			count;
+	struct task_struct		*task;
+	struct list_head		list;
+	u32				handle_count;
+	u32				next_fd;
+	int				warned;
+	int				tag_warned;
+};
+
+extern bool dmabuf_is_nvmap(struct dma_buf *dmabuf);
+extern u32 nvmap_max_handle_count;
+
+struct nvmap_client *nvmap_client_create(struct list_head *dev_client_list,
+		const char *name)
+{
+	struct nvmap_client *client;
+	struct task_struct *task;
+
+	client = kzalloc(sizeof(*client), GFP_KERNEL);
+	if (!client)
+		return NULL;
+
+	client->name = name;
+	client->kernel_client = true;
+	client->handle_refs = RB_ROOT;
+
+	get_task_struct(current->group_leader);
+	task_lock(current->group_leader);
+	/* don't bother to store task struct for kernel threads,
+	   they can't be killed anyway */
+	if (current->flags & PF_KTHREAD) {
+		put_task_struct(current->group_leader);
+		task = NULL;
+	} else {
+		task = current->group_leader;
+	}
+	task_unlock(current->group_leader);
+	client->task = task;
+
+	mutex_init(&client->ref_lock);
+	atomic_set(&client->count, 1);
+
+	list_add(&client->list, dev_client_list);
+
+	trace_nvmap_open(client, client->name);
+
+	client->kernel_client = false;
+
+	return client;
+}
+
+void nvmap_client_destroy(struct nvmap_client *client)
+{
+	struct rb_node *n;
+
+	if (!client)
+		return;
+
+	while ((n = rb_first(&client->handle_refs))) {
+		struct nvmap_handle_ref *ref;
+		int count;
+
+		ref = rb_entry(n, struct nvmap_handle_ref, node);
+		smp_rmb();
+
+		count = nvmap_handle_ref_count(ref);
+
+		while (count--)
+			nvmap_client_remove_handle(client, ref->handle);
+	}
+
+	if (client->task)
+		put_task_struct(client->task);
+
+	trace_nvmap_release(client, client->name);
+	kfree(client);
+}
+
+const char *nvmap_client_name(struct nvmap_client *client)
+{
+	return client->name;
+}
+
+static void client_lock(struct nvmap_client *c)
+{
+	mutex_lock(&c->ref_lock);
+}
+
+static void client_unlock(struct nvmap_client *c)
+{
+	mutex_unlock(&c->ref_lock);
+}
+
+pid_t nvmap_client_pid(struct nvmap_client *client)
+{
+	return client->task ? client->task->pid : 0;
+}
+
+void nvmap_client_stats_alloc(struct nvmap_client *client, size_t size)
+{
+	if (client->kernel_client)
+		nvmap_stats_inc(NS_KALLOC, size);
+	else
+		nvmap_stats_inc(NS_UALLOC, size);
+}
+
+int nvmap_client_add_handle(struct nvmap_client *client,
+			   struct nvmap_handle *handle)
+{
+	struct nvmap_handle_ref *ref;
+
+	ref = nvmap_client_to_handle_ref(client, handle);
+	if (ref) {
+		nvmap_handle_ref_get(ref);
+		return 0;
+	}
+
+	ref = nvmap_handle_ref_create(handle);
+	if (!ref) {
+		return -ENOMEM;
+	}
+
+	nvmap_client_add_ref(client, ref);
+	nvmap_handle_add_owner(handle, client);
+
+	return 0;
+}
+
+void nvmap_client_remove_handle(struct nvmap_client *client,
+			   struct nvmap_handle *handle)
+{
+	struct nvmap_handle_ref *ref;
+	int ref_count;
+
+	ref = nvmap_client_to_handle_ref(client, handle);
+	if (!ref)
+		return;
+
+	ref_count = nvmap_handle_ref_put(ref);
+	if (ref_count == 0) {
+		nvmap_client_remove_ref(client, ref);
+		nvmap_handle_ref_free(ref);
+		// TODO set ref->handle->owner to NULL
+	}
+}
+
+int nvmap_client_create_handle(struct nvmap_client *client, size_t size)
+{
+	struct nvmap_handle *handle = NULL;
+	int err;
+	int fd;
+
+	handle = nvmap_handle_create(size);
+	if (IS_ERR_OR_NULL(handle)) {
+		return -1;
+	}
+
+	err = nvmap_client_add_handle(client, handle);
+	if (err) {
+		nvmap_handle_put(handle);
+		return -1;
+	}
+	/* This is the first handle ref we are creating and we want the dmabuf
+	 * to have a ref of 1.
+	 * client_add_handle increases the dmabuf ref so decrease it again
+	 */
+	dma_buf_put(nvmap_handle_to_dmabuf(handle));
+
+	fd = nvmap_client_create_fd(client);
+	if (fd < 0) {
+		nvmap_client_remove_handle(client, handle);
+		nvmap_handle_put(handle);
+		return -1;
+	}
+	nvmap_handle_install_fd(handle, fd);
+
+	return fd;
+}
+
+void nvmap_client_add_ref(struct nvmap_client *client,
+			   struct nvmap_handle_ref *ref)
+{
+	struct rb_node **p, *parent = NULL;
+
+	if(IS_ERR(ref)) {
+		pr_warn("Putting Error Ref into client\n");
+		return;
+	}
+
+	client_lock(client);
+	p = &client->handle_refs.rb_node;
+	while (*p) {
+		struct nvmap_handle_ref *node;
+		parent = *p;
+		node = rb_entry(parent, struct nvmap_handle_ref, node);
+		if (ref->handle > node->handle)
+			p = &parent->rb_right;
+		else
+			p = &parent->rb_left;
+	}
+	rb_link_node(&ref->node, parent, p);
+	rb_insert_color(&ref->node, &client->handle_refs);
+	client->handle_count++;
+	if (client->handle_count > nvmap_max_handle_count)
+		nvmap_max_handle_count = client->handle_count;
+
+	client_unlock(client);
+}
+
+void nvmap_client_remove_ref(struct nvmap_client *client,
+					struct nvmap_handle_ref *ref)
+{
+	client_lock(client);
+
+	smp_rmb();
+	rb_erase(&ref->node, &client->handle_refs);
+	client->handle_count--;
+
+	client_unlock(client);
+}
+
+struct nvmap_handle_ref *nvmap_client_to_handle_ref(struct nvmap_client *client,
+					struct nvmap_handle *handle)
+{
+	struct rb_node *n = client->handle_refs.rb_node;
+	struct nvmap_handle_ref *return_ref = NULL;
+
+	client_lock(client);
+
+	n = client->handle_refs.rb_node;
+	while (n) {
+		struct nvmap_handle_ref *ref;
+		ref = rb_entry(n, struct nvmap_handle_ref, node);
+		if (IS_ERR(ref)) {
+			pr_warn("Ref error in client!\n");
+			client_unlock(client);
+			return NULL;
+		}
+		if (ref->handle == handle) {
+			return_ref = ref;
+			break;
+		}
+		else if ((uintptr_t)handle > (uintptr_t)ref->handle)
+			n = n->rb_right;
+		else
+			n = n->rb_left;
+	}
+
+	client_unlock(client);
+	return return_ref;
+
+}
+
+int nvmap_client_create_fd(struct nvmap_client *client)
+{
+	int flags = O_CLOEXEC;
+	int start_fd = CONFIG_NVMAP_FD_START;
+
+#ifdef CONFIG_NVMAP_DEFER_FD_RECYCLE
+	if (client->next_fd < CONFIG_NVMAP_FD_START)
+		client->next_fd = CONFIG_NVMAP_FD_START;
+	start_fd = client->next_fd++;
+	if (client->next_fd >= CONFIG_NVMAP_DEFER_FD_RECYCLE_MAX_FD)
+		client->next_fd = CONFIG_NVMAP_FD_START;
+#endif
+	/* Allocate fd from start_fd(>=1024) onwards to overcome
+	 * __FD_SETSIZE limitation issue for select(),
+	 * pselect() syscalls.
+	 */
+	// TODO: What is this current?
+	return tegra_alloc_fd(current->files, start_fd, flags);
+}
+
+int nvmap_client_give_dmabuf_new_fd(struct nvmap_client *client,
+				struct dma_buf *dmabuf)
+{
+	int fd;
+
+	fd = nvmap_client_create_fd(client);
+	if (fd > 0)
+		nvmap_dmabuf_install_fd(dmabuf, fd);
+	return fd;
+}
+
+void nvmap_client_warn_if_bad_heap(struct nvmap_client *client,
+				u32 heap_type, u32 userflags)
+{
+	if (heap_type != NVMAP_HEAP_CARVEOUT_VPR && client && !client->warned) {
+		char task_comm[TASK_COMM_LEN];
+		client->warned = 1;
+		get_task_comm(task_comm, client->task);
+		pr_err("PID %d: %s: TAG: 0x%04x WARNING: "
+				"NVMAP_HANDLE_WRITE_COMBINE "
+				"should be used in place of "
+				"NVMAP_HANDLE_UNCACHEABLE on ARM64\n",
+				client->task->pid, task_comm,
+				userflags >> 16);
+	}
+}
+
+void nvmap_client_warn_if_no_tag(struct nvmap_client *client,
+					unsigned int flags)
+{
+	int tag = flags >> 16;
+	char task_comm[TASK_COMM_LEN];
+
+	if (!tag && client && !client->tag_warned) {
+		client->tag_warned = 1;
+		get_task_comm(task_comm, client->task);
+		pr_err("PID %d: %s: WARNING: "
+			"All NvMap Allocations must have a tag "
+			"to identify the subsystem allocating memory."
+			"Please pass the tag to the API call"
+			" NvRmMemHanldeAllocAttr() or relevant. \n",
+			client->task->pid, task_comm);
+	}
+}
+
+/**************************************************************************
+ * Client Print methods
+ * ************************************************************************/
+
+void nvmap_client_stringify(struct nvmap_client *client, struct seq_file *s)
+{
+	char task_comm[TASK_COMM_LEN];
+	if (!client->task) {
+		seq_printf(s, "%-18s %18s %8u", client->name, "kernel", 0);
+		return;
+	}
+	get_task_comm(task_comm, client->task);
+	seq_printf(s, "%-18s %18s %8u", client->name, task_comm,
+		   client->task->pid);
+}
+
+void nvmap_client_allocations_stringify(struct nvmap_client *client,
+				  struct seq_file *s, u32 heap_type)
+{
+	struct nvmap_device *dev = nvmap_dev;
+	struct nvmap_handle_ref *ref;
+	struct nvmap_handle *handle;
+	struct rb_node *n;
+
+	client_lock(client);
+	mutex_lock(&dev->tags_lock);
+
+	n = rb_first(&client->handle_refs);
+	for (; n != NULL; n = rb_next(n)) {
+		ref = rb_entry(n, struct nvmap_handle_ref, node);
+		handle = ref->handle;
+
+		nvmap_handle_stringify(handle, s, heap_type,
+						atomic_read(&ref->dupes));
+	}
+
+	mutex_unlock(&dev->tags_lock);
+	client_unlock(client);
+}
+
+void nvmap_client_maps_stringify(struct nvmap_client *client,
+				struct seq_file *s, u32 heap_type)
+{
+	struct nvmap_device *dev = nvmap_dev;
+	struct nvmap_handle_ref *ref;
+	struct nvmap_handle *handle;
+	struct rb_node *n;
+
+	client_lock(client);
+	mutex_lock(&dev->tags_lock);
+
+	n = rb_first(&client->handle_refs);
+	for (; n != NULL; n = rb_next(n)) {
+		ref = rb_entry(n, struct nvmap_handle_ref, node);
+		handle = ref->handle;
+
+		nvmap_handle_maps_stringify(handle, s, heap_type,
+							client->task->pid);
+	}
+
+	mutex_unlock(&dev->tags_lock);
+	client_unlock(client);
+
+}
+
+int nvmap_client_show_by_pid(struct nvmap_client *client, struct seq_file *s,
+				pid_t pid)
+{
+	struct rb_node *n;
+	struct nvmap_handle_ref *ref;
+	struct nvmap_handle *handle;
+	int ret = 0;
+
+	if (client->task->pid != pid)
+		return 0;
+
+	client_lock(client);
+
+	n = rb_first(&client->handle_refs);
+	for (; n != NULL; n = rb_next(n)) {
+		ref = rb_entry(n, struct nvmap_handle_ref, node);
+		handle = ref->handle;
+		ret = nvmap_handle_pid_show(handle, s, client->task->pid);
+		if (ret)
+			break;
+	}
+
+	client_unlock(client);
+
+	return 0;
+}
+
+u64 nvmap_client_calc_mss(struct nvmap_client *client, u32 heap_type)
+{
+	struct nvmap_handle_ref *ref;
+	struct nvmap_handle *handle;
+	struct rb_node *n;
+	u64 total = 0;
+
+	client_lock(client);
+
+	n = rb_first(&client->handle_refs);
+	for (; n != NULL; n = rb_next(n)) {
+		ref = rb_entry(n, struct nvmap_handle_ref, node);
+		handle = ref->handle;
+
+		total += nvmap_handle_share_size(handle, heap_type);
+	}
+
+	client_unlock(client);
+
+	return total;
+}
+
+#include <linux/swap.h>
+#include <linux/swapops.h>
+#include <linux/backing-dev.h>
+#include <linux/ptrace.h>
+
+#define PSS_SHIFT 12
+
+#ifndef PTRACE_MODE_READ_FSCREDS
+#define PTRACE_MODE_READ_FSCREDS PTRACE_MODE_READ
+#endif
+
+struct procrank_stats {
+	struct vm_area_struct *vma;
+	u64 pss;
+};
+
+static int procrank_pte_entry(pte_t *pte, unsigned long addr, unsigned long end,
+		struct mm_walk *walk)
+{
+	struct procrank_stats *mss = walk->private;
+	struct vm_area_struct *vma = mss->vma;
+	struct page *page = NULL;
+	int mapcount;
+
+	if (pte_present(*pte))
+		page = vm_normal_page(vma, addr, *pte);
+	else if (is_swap_pte(*pte)) {
+		swp_entry_t swpent = pte_to_swp_entry(*pte);
+
+		if (is_migration_entry(swpent))
+			page = migration_entry_to_page(swpent);
+	}
+
+	if (!page)
+		return 0;
+
+	mapcount = page_mapcount(page);
+	if (mapcount >= 2)
+		mss->pss += (PAGE_SIZE << PSS_SHIFT) / mapcount;
+	else
+		mss->pss += (PAGE_SIZE << PSS_SHIFT);
+
+	return 0;
+}
+
+
+void nvmap_client_calc_iovmm_mss(struct nvmap_client *client, u64 *pss,
+				   u64 *total)
+{
+	struct rb_node *n;
+	struct nvmap_handle_ref *ref;
+	struct nvmap_handle *h;
+	struct procrank_stats mss;
+	struct mm_walk procrank_walk = {
+		.pte_entry = procrank_pte_entry,
+		.private = &mss,
+	};
+	struct mm_struct *mm;
+
+	memset(&mss, 0, sizeof(mss));
+	*pss = 0;
+	*total = 0;
+
+	mm = mm_access(client->task,
+			PTRACE_MODE_READ_FSCREDS);
+
+	if (!mm || IS_ERR(mm)) {
+		return;
+	}
+
+	down_read(&mm->mmap_sem);
+	procrank_walk.mm = mm;
+
+	client_lock(client);
+
+	n = rb_first(&client->handle_refs);
+	for (; n != NULL; n = rb_next(n)) {
+		ref = rb_entry(n, struct nvmap_handle_ref, node);
+		h = ref->handle;
+
+		*total += nvmap_handle_procrank_walk(h, &procrank_walk,
+				client->task->pid);
+	}
+
+	up_read(&mm->mmap_sem);
+	mmput(mm);
+	*pss = (mss.pss >> PSS_SHIFT);
+
+	client_unlock(client);
+}
+
+struct nvmap_client *nvmap_client_from_list(struct list_head *n)
+{
+	return list_entry(n, struct nvmap_client, list);
+}
+
+void nvmap_client_del_list(struct nvmap_client *client)
+{
+	list_del(&client->list);
+}
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_client.h b/drivers/video/tegra/nvmap/nv2/nvmap_client.h
new file mode 100644
index 000000000000..b5c18d7d6b49
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_client.h
@@ -0,0 +1,65 @@
+/*
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __NVMAP_CLIENT_H
+#define __NVMAP_CLIENT_H
+
+struct nvmap_client *nvmap_client_create(struct list_head *dev_client_list,
+						const char *name);
+void nvmap_client_destroy(struct nvmap_client *client);
+
+void nvmap_client_remove_ref(struct nvmap_client *client,
+					struct nvmap_handle_ref *ref);
+void nvmap_client_add_ref(struct nvmap_client *client,
+			   struct nvmap_handle_ref *ref);
+struct nvmap_handle_ref *nvmap_client_to_handle_ref(struct nvmap_client *client,
+					struct nvmap_handle *handle);
+
+int nvmap_client_add_handle(struct nvmap_client *client,
+			   struct nvmap_handle *handle);
+void nvmap_client_remove_handle(struct nvmap_client *client,
+			   struct nvmap_handle *handle);
+
+int nvmap_client_create_handle(struct nvmap_client *client, size_t size);
+
+int nvmap_client_create_fd(struct nvmap_client *client);
+
+
+void nvmap_client_warn_if_bad_heap(struct nvmap_client *client,
+				u32 heap_type, u32 userflags);
+void nvmap_client_warn_if_no_tag(struct nvmap_client *client,
+					unsigned int flags);
+
+pid_t nvmap_client_pid(struct nvmap_client *client);
+
+void nvmap_client_stats_alloc(struct nvmap_client *client, size_t size);
+
+const char *nvmap_client_name(struct nvmap_client *client);
+
+void nvmap_client_stringify(struct nvmap_client *client, struct seq_file *s);
+void nvmap_client_allocations_stringify(struct nvmap_client *client,
+				  struct seq_file *s, u32 heap_type);
+void nvmap_client_maps_stringify(struct nvmap_client *client,
+				struct seq_file *s, u32 heap_type);
+u64 nvmap_client_calc_mss(struct nvmap_client *client, u32 heap_type);
+int nvmap_client_show_by_pid(struct nvmap_client *client, struct seq_file *s,
+				pid_t pid);
+u64 nvmap_handle_procrank_walk(struct nvmap_handle *h, struct mm_walk *walk,
+		pid_t client_pid);
+void nvmap_client_calc_iovmm_mss(struct nvmap_client *client, u64 *pss,
+				   u64 *total);
+
+struct nvmap_client *nvmap_client_from_list(struct list_head *n);
+void nvmap_client_del_list(struct nvmap_client *client);
+
+#endif /* __NVMAP_CLIENT_H */
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_dev.c b/drivers/video/tegra/nvmap/nv2/nvmap_dev.c
new file mode 100644
index 000000000000..c737902bcb21
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_dev.c
@@ -0,0 +1,915 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_dev.c
+ *
+ * User-space interface to nvmap
+ *
+ * Copyright (c) 2011-2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#include <linux/backing-dev.h>
+#include <linux/bitmap.h>
+#include <linux/debugfs.h>
+#include <linux/delay.h>
+#include <linux/io.h>
+#include <linux/kernel.h>
+#include <linux/device.h>
+#include <linux/oom.h>
+#include <linux/platform_device.h>
+#include <linux/seq_file.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/uaccess.h>
+#include <linux/vmalloc.h>
+#include <linux/nvmap.h>
+#include <linux/module.h>
+#include <linux/resource.h>
+#include <linux/security.h>
+#include <linux/stat.h>
+#include <linux/kthread.h>
+#include <linux/highmem.h>
+#include <linux/lzo.h>
+#include <linux/swap.h>
+#include <linux/swapops.h>
+#include <linux/of.h>
+#include <linux/iommu.h>
+#include <linux/version.h>
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+#include <linux/sched/mm.h>
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0)
+#include <linux/backing-dev.h>
+#endif
+
+#include <asm/cputype.h>
+
+#define CREATE_TRACE_POINTS
+#include <trace/events/nvmap.h>
+
+#include "nvmap_ioctl.h"
+#include "nvmap_ioctl.h"
+#include "nvmap_client.h"
+#include "nvmap_handle.h"
+#include "nvmap_dev.h"
+#include "nvmap_carveout.h"
+#include "nvmap_cache.h"
+#include "nvmap_stats.h"
+
+// TODO remove global variables
+extern bool nvmap_convert_carveout_to_iovmm;
+extern bool nvmap_convert_iovmm_to_carveout;
+extern u32 nvmap_max_handle_count;
+
+#define NVMAP_CARVEOUT_KILLER_RETRY_TIME 100 /* msecs */
+
+struct nvmap_device *nvmap_dev;
+EXPORT_SYMBOL(nvmap_dev);
+ulong nvmap_init_time;
+
+static struct device_dma_parameters nvmap_dma_parameters = {
+	.max_segment_size = UINT_MAX,
+};
+
+static int nvmap_open(struct inode *inode, struct file *filp);
+static int nvmap_release(struct inode *inode, struct file *filp);
+static long nvmap_ioctl(struct file *filp, unsigned int cmd, unsigned long arg);
+static int nvmap_map(struct file *filp, struct vm_area_struct *vma);
+#if !defined(CONFIG_MMU) && (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+static unsigned nvmap_mmap_capabilities(struct file *filp);
+#endif
+
+static const struct file_operations nvmap_user_fops = {
+	.owner		= THIS_MODULE,
+	.open		= nvmap_open,
+	.release	= nvmap_release,
+	.unlocked_ioctl	= nvmap_ioctl,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl = nvmap_ioctl,
+#endif
+	.mmap		= nvmap_map,
+#if !defined(CONFIG_MMU) && (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+	.mmap_capabilities = nvmap_mmap_capabilities,
+#endif
+};
+
+u32 nvmap_cpu_access_mask(void)
+{
+	return nvmap_dev->cpu_access_mask;
+}
+
+struct nvmap_carveout_node *nvmap_dev_to_carveout(struct nvmap_device *dev, int i)
+{
+	return nvmap_carveout_index(dev->heaps, i);
+}
+
+const struct file_operations debug_handles_by_pid_fops;
+
+struct nvmap_pid_data {
+	struct rb_node node;
+	pid_t pid;
+	struct kref refcount;
+	struct dentry *handles_file;
+};
+
+static void nvmap_pid_release_locked(struct kref *kref)
+{
+	struct nvmap_pid_data *p = container_of(kref, struct nvmap_pid_data,
+			refcount);
+	debugfs_remove(p->handles_file);
+	rb_erase(&p->node, &nvmap_dev->pids);
+	kfree(p);
+}
+
+static void nvmap_pid_get_locked(struct nvmap_device *dev, pid_t pid)
+{
+	struct rb_root *root = &dev->pids;
+	struct rb_node **new = &(root->rb_node), *parent = NULL;
+	struct nvmap_pid_data *p;
+	char name[16];
+
+	while (*new) {
+		p = container_of(*new, struct nvmap_pid_data, node);
+		parent = *new;
+
+		if (p->pid > pid) {
+			new = &((*new)->rb_left);
+		} else if (p->pid < pid) {
+			new = &((*new)->rb_right);
+		} else {
+			kref_get(&p->refcount);
+			return;
+		}
+	}
+
+	p = kzalloc(sizeof(*p), GFP_KERNEL);
+	if (!p)
+		return;
+
+	snprintf(name, sizeof(name), "%d", pid);
+	p->pid = pid;
+	kref_init(&p->refcount);
+	p->handles_file = debugfs_create_file(name, S_IRUGO,
+			dev->handles_by_pid, p,
+			&debug_handles_by_pid_fops);
+
+	if (IS_ERR_OR_NULL(p->handles_file)) {
+		kfree(p);
+	} else {
+		rb_link_node(&p->node, parent, new);
+		rb_insert_color(&p->node, root);
+	}
+}
+
+static struct nvmap_pid_data *nvmap_pid_find_locked(struct nvmap_device *dev,
+		pid_t pid)
+{
+	struct rb_node *node = dev->pids.rb_node;
+
+	while (node) {
+		struct nvmap_pid_data *p = container_of(node,
+				struct nvmap_pid_data, node);
+
+		if (p->pid > pid)
+			node = node->rb_left;
+		else if (p->pid < pid)
+			node = node->rb_right;
+		else
+			return p;
+	}
+	return NULL;
+}
+
+static void nvmap_pid_put_locked(struct nvmap_device *dev, pid_t pid)
+{
+	struct nvmap_pid_data *p = nvmap_pid_find_locked(dev, pid);
+	if (p)
+		kref_put(&p->refcount, nvmap_pid_release_locked);
+}
+
+static int nvmap_open(struct inode *inode, struct file *filp)
+{
+	struct miscdevice *miscdev = filp->private_data;
+	struct nvmap_device *dev = dev_get_drvdata(miscdev->parent);
+	struct nvmap_client *client;
+	int ret;
+	__attribute__((unused)) struct rlimit old_rlim, new_rlim;
+
+	ret = nonseekable_open(inode, filp);
+	if (unlikely(ret))
+		return ret;
+
+	BUG_ON(dev != nvmap_dev);
+
+	mutex_lock(&dev->clients_lock);
+
+	client = nvmap_client_create(&dev->clients, "user");
+	if (!client)
+		return -ENOMEM;
+
+	if (!IS_ERR_OR_NULL(dev->handles_by_pid)) {
+		pid_t pid = nvmap_client_pid(client);
+		nvmap_pid_get_locked(dev, pid);
+	}
+	mutex_unlock(&dev->clients_lock);
+
+	filp->private_data = client;
+	return 0;
+}
+
+static int nvmap_release(struct inode *inode, struct file *filp)
+{
+	struct nvmap_client *client = filp->private_data;
+
+	if(!client)
+		return 0;
+
+	mutex_lock(&nvmap_dev->clients_lock);
+	if (!IS_ERR_OR_NULL(nvmap_dev->handles_by_pid)) {
+		pid_t pid = nvmap_client_pid(client);
+		nvmap_pid_put_locked(nvmap_dev, pid);
+	}
+	nvmap_client_del_list(client);
+	mutex_unlock(&nvmap_dev->clients_lock);
+
+	// TODO: client->count is always 1 so we dont need below line
+	// Remove client count and this comment
+	//if (!atomic_dec_return(&client->count))
+	nvmap_client_destroy(client);
+
+	return 0;
+}
+
+static int nvmap_map(struct file *filp, struct vm_area_struct *vma)
+{
+	char task_comm[TASK_COMM_LEN];
+
+	get_task_comm(task_comm, current);
+	pr_err("error: mmap not supported on nvmap file, pid=%d, %s\n",
+		task_tgid_nr(current), task_comm);
+	return -EPERM;
+}
+
+static long nvmap_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
+{
+	int err = 0;
+	void __user *uarg = (void __user *)arg;
+
+	if (_IOC_TYPE(cmd) != NVMAP_IOC_MAGIC)
+		return -ENOTTY;
+
+	if (_IOC_NR(cmd) > NVMAP_IOC_MAXNR)
+		return -ENOTTY;
+
+	if (_IOC_DIR(cmd) & _IOC_READ)
+		err = !access_ok(VERIFY_WRITE, uarg, _IOC_SIZE(cmd));
+	if (!err && (_IOC_DIR(cmd) & _IOC_WRITE))
+		err = !access_ok(VERIFY_READ, uarg, _IOC_SIZE(cmd));
+
+	if (err)
+		return -EFAULT;
+
+	err = -ENOTTY;
+
+	switch (cmd) {
+	case NVMAP_IOC_CREATE:
+	case NVMAP_IOC_CREATE_64:
+	case NVMAP_IOC_FROM_FD:
+		err = nvmap_ioctl_create(filp, cmd, uarg);
+		break;
+
+	case NVMAP_IOC_FROM_VA:
+		err = nvmap_ioctl_create_from_va(filp, uarg);
+		break;
+
+	case NVMAP_IOC_GET_FD:
+		err = nvmap_ioctl_getfd(filp, uarg);
+		break;
+
+	case NVMAP_IOC_GET_IVM_HEAPS:
+		err = nvmap_ioctl_get_ivc_heap(filp, uarg);
+		break;
+
+	case NVMAP_IOC_FROM_IVC_ID:
+		err = nvmap_ioctl_create_from_ivc(filp, uarg);
+		break;
+
+	case NVMAP_IOC_GET_IVC_ID:
+		err = nvmap_ioctl_get_ivcid(filp, uarg);
+		break;
+
+	case NVMAP_IOC_ALLOC:
+	case NVMAP_IOC_ALLOC_IVM:
+		err = nvmap_ioctl_alloc(filp, cmd, uarg);
+		break;
+
+	case NVMAP_IOC_VPR_FLOOR_SIZE:
+		err = nvmap_ioctl_vpr_floor_size(filp, uarg);
+		break;
+
+	case NVMAP_IOC_FREE:
+		err = nvmap_ioctl_free(filp, arg);
+		break;
+
+#ifdef CONFIG_COMPAT
+	case NVMAP_IOC_WRITE_32:
+	case NVMAP_IOC_READ_32:
+		err = nvmap_ioctl_rw_handle(filp, cmd == NVMAP_IOC_READ_32,
+			uarg, sizeof(struct nvmap_rw_handle_32));
+		break;
+#endif
+
+	case NVMAP_IOC_WRITE:
+	case NVMAP_IOC_READ:
+		err = nvmap_ioctl_rw_handle(filp, cmd == NVMAP_IOC_READ, uarg,
+			sizeof(struct nvmap_rw_handle));
+		break;
+
+	case NVMAP_IOC_WRITE_64:
+	case NVMAP_IOC_READ_64:
+		err = nvmap_ioctl_rw_handle(filp, cmd == NVMAP_IOC_READ_64,
+			uarg, sizeof(struct nvmap_rw_handle_64));
+		break;
+
+#ifdef CONFIG_COMPAT
+	case NVMAP_IOC_CACHE_32:
+		err = nvmap_ioctl_cache_maint(filp, uarg,
+			sizeof(struct nvmap_cache_op_32));
+		break;
+#endif
+
+	case NVMAP_IOC_CACHE:
+		err = nvmap_ioctl_cache_maint(filp, uarg,
+			sizeof(struct nvmap_cache_op));
+		break;
+
+	case NVMAP_IOC_CACHE_64:
+		err = nvmap_ioctl_cache_maint(filp, uarg,
+			sizeof(struct nvmap_cache_op_64));
+		break;
+
+	case NVMAP_IOC_CACHE_LIST:
+	case NVMAP_IOC_RESERVE:
+		err = nvmap_ioctl_cache_maint_list(filp, uarg,
+						   cmd == NVMAP_IOC_RESERVE);
+		break;
+
+	case NVMAP_IOC_GUP_TEST:
+		err = nvmap_ioctl_gup_test(filp, uarg);
+		break;
+
+	/* Depreacted IOCTL's */
+	case NVMAP_IOC_ALLOC_KIND:
+		pr_warn("NVMAP_IOC_ALLOC_KIND is deprecated. Use NVMAP_IOC_ALLOC.\n");
+		break;
+
+#ifdef CONFIG_COMPAT
+	case NVMAP_IOC_MMAP_32:
+#endif
+	case NVMAP_IOC_MMAP:
+		pr_warn("NVMAP_IOC_MMAP is deprecated. Use mmap().\n");
+		break;
+
+#ifdef CONFIG_COMPAT
+	case NVMAP_IOC_UNPIN_MULT_32:
+	case NVMAP_IOC_PIN_MULT_32:
+		pr_warn("NVMAP_IOC_[UN]PIN_MULT is deprecated. "
+			"User space must never pin NvMap handles to "
+			"allow multiple IOVA spaces.\n");
+		break;
+#endif
+
+	case NVMAP_IOC_UNPIN_MULT:
+	case NVMAP_IOC_PIN_MULT:
+		pr_warn("NVMAP_IOC_[UN]PIN_MULT/ is deprecated. "
+			"User space must never pin NvMap handles to "
+			"allow multiple IOVA spaces.\n");
+		break;
+
+	case NVMAP_IOC_FROM_ID:
+	case NVMAP_IOC_GET_ID:
+		pr_warn("NVMAP_IOC_GET_ID/FROM_ID pair is deprecated. "
+			"Use the pair NVMAP_IOC_GET_FD/FROM_FD.\n");
+		break;
+
+	case NVMAP_IOC_SHARE:
+		pr_warn("NVMAP_IOC_SHARE is deprecated. Use NVMAP_IOC_GET_FD.\n");
+		break;
+
+	case NVMAP_IOC_SET_TAG_LABEL:
+		err = nvmap_ioctl_set_tag_label(filp, uarg);
+		break;
+
+	case NVMAP_IOC_GET_AVAILABLE_HEAPS:
+		err = nvmap_ioctl_get_available_heaps(filp, uarg);
+		break;
+
+	case NVMAP_IOC_GET_HEAP_SIZE:
+		err = nvmap_ioctl_get_heap_size(filp, uarg);
+		break;
+
+	default:
+		pr_warn("Unknown NVMAP_IOC = 0x%x\n", cmd);
+	}
+	return err;
+}
+
+#define DEBUGFS_OPEN_FOPS(name) \
+static int nvmap_debug_##name##_open(struct inode *inode, \
+					    struct file *file) \
+{ \
+	return single_open(file, nvmap_debug_##name##_show, \
+			    inode->i_private); \
+} \
+\
+const struct file_operations debug_##name##_fops = { \
+	.open = nvmap_debug_##name##_open, \
+	.read = seq_read, \
+	.llseek = seq_lseek, \
+	.release = single_release, \
+}
+
+#define K(x) (x >> 10)
+
+#define PSS_SHIFT 12
+static void nvmap_get_total_mss(u64 *pss, u64 *total, u32 heap_type)
+{
+	struct nvmap_device *dev = nvmap_dev;
+	struct rb_node *n;
+
+	*total = 0;
+	if (pss)
+		*pss = 0;
+	if (!dev)
+		return;
+
+	spin_lock(&dev->handle_lock);
+
+	n = rb_first(&dev->handles);
+	for (; n != NULL; n = rb_next(n)) {
+		struct nvmap_handle *h = nvmap_handle_from_node(n);
+
+		*total += nvmap_handle_total_mss(h, heap_type);
+		if (pss)
+			*pss += nvmap_handle_total_pss(h, heap_type);
+
+	}
+
+	spin_unlock(&dev->handle_lock);
+}
+
+static int nvmap_debug_allocations_show(struct seq_file *s, void *unused)
+{
+	u64 total;
+	struct nvmap_client *client;
+	struct list_head *n;
+	u32 heap_type = (u32)(uintptr_t)s->private;
+
+	mutex_lock(&nvmap_dev->clients_lock);
+	seq_printf(s, "%-18s %18s %8s %11s\n",
+		"CLIENT", "PROCESS", "PID", "SIZE");
+	seq_printf(s, "%-18s %18s %8s %11s %8s %6s %6s %6s %6s %6s %6s %8s\n",
+			"", "", "BASE", "SIZE", "FLAGS", "REFS",
+			"DUPES", "PINS", "KMAPS", "UMAPS", "SHARE", "UID");
+	list_for_each(n, &nvmap_dev->clients) {
+		u64 client_total;
+
+		client = nvmap_client_from_list(n);
+		nvmap_client_stringify(client, s);
+		client_total = nvmap_client_calc_mss(client, heap_type);
+		seq_printf(s, " %10lluK\n", K(client_total));
+		nvmap_client_allocations_stringify(client, s, heap_type);
+		seq_printf(s, "\n");
+	}
+	mutex_unlock(&nvmap_dev->clients_lock);
+	nvmap_get_total_mss(NULL, &total, heap_type);
+	seq_printf(s, "%-18s %-18s %8s %10lluK\n", "total", "", "", K(total));
+	return 0;
+}
+
+DEBUGFS_OPEN_FOPS(allocations);
+
+static int nvmap_debug_all_allocations_show(struct seq_file *s, void *unused)
+{
+	u32 heap_type = (u32)(uintptr_t)s->private;
+	struct nvmap_handle *handle;
+	struct rb_node *n;
+
+
+	spin_lock(&nvmap_dev->handle_lock);
+
+	seq_printf(s, "%8s %11s %9s %6s %6s %6s %6s %8s\n",
+			"BASE", "SIZE", "USERFLAGS", "REFS",
+			"KMAPS", "UMAPS", "SHARE", "UID");
+
+	/* for each handle */
+	n = rb_first(&nvmap_dev->handles);
+	for (; n != NULL; n = rb_next(n)) {
+		handle = nvmap_handle_from_node(n);
+		nvmap_handle_all_allocations_show(handle, s, heap_type);
+	}
+
+	spin_unlock(&nvmap_dev->handle_lock);
+
+	return 0;
+}
+
+DEBUGFS_OPEN_FOPS(all_allocations);
+
+static int nvmap_debug_orphan_handles_show(struct seq_file *s, void *unused)
+{
+	u32 heap_type = (u32)(uintptr_t)s->private;
+	struct rb_node *n;
+
+
+	spin_lock(&nvmap_dev->handle_lock);
+	seq_printf(s, "%8s %11s %9s %6s %6s %6s %8s\n",
+			"BASE", "SIZE", "USERFLAGS", "REFS",
+			"KMAPS", "UMAPS", "UID");
+
+	/* for each handle */
+	n = rb_first(&nvmap_dev->handles);
+	for (; n != NULL; n = rb_next(n)) {
+		struct nvmap_handle *handle = nvmap_handle_from_node(n);
+		nvmap_handle_orphans_allocations_show(handle, s, heap_type);
+	}
+
+	spin_unlock(&nvmap_dev->handle_lock);
+
+	return 0;
+}
+
+DEBUGFS_OPEN_FOPS(orphan_handles);
+
+static int nvmap_debug_maps_show(struct seq_file *s, void *unused)
+{
+	u64 total;
+	struct nvmap_client *client;
+	struct list_head *n;
+	u32 heap_type = (u32)(uintptr_t)s->private;
+
+	mutex_lock(&nvmap_dev->clients_lock);
+	seq_printf(s, "%-18s %18s %8s %11s\n",
+		"CLIENT", "PROCESS", "PID", "SIZE");
+	seq_printf(s, "%-18s %18s %8s %11s %8s %6s %9s %21s %18s\n",
+		"", "", "BASE", "SIZE", "FLAGS", "SHARE", "UID",
+		"MAPS", "MAPSIZE");
+
+	list_for_each(n, &nvmap_dev->clients) {
+		u64 client_total;
+
+		client = nvmap_client_from_list(n);
+		nvmap_client_stringify(client, s);
+		client_total = nvmap_client_calc_mss(client, heap_type);
+		seq_printf(s, " %10lluK\n", K(client_total));
+		nvmap_client_maps_stringify(client, s, heap_type);
+		seq_printf(s, "\n");
+	}
+	mutex_unlock(&nvmap_dev->clients_lock);
+
+	nvmap_get_total_mss(NULL, &total, heap_type);
+	seq_printf(s, "%-18s %-18s %8s %10lluK\n", "total", "", "", K(total));
+	return 0;
+}
+
+DEBUGFS_OPEN_FOPS(maps);
+
+static int nvmap_debug_clients_show(struct seq_file *s, void *unused)
+{
+	u64 total;
+	struct nvmap_client *client;
+	struct list_head *n;
+	ulong heap_type = (ulong)s->private;
+
+	mutex_lock(&nvmap_dev->clients_lock);
+	seq_printf(s, "%-18s %18s %8s %11s\n",
+		"CLIENT", "PROCESS", "PID", "SIZE");
+	list_for_each(n, &nvmap_dev->clients) {
+		u64 client_total;
+
+		client = nvmap_client_from_list(n);
+		nvmap_client_stringify(client, s);
+		client_total = nvmap_client_calc_mss(client, heap_type);
+		seq_printf(s, " %10lluK\n", K(client_total));
+	}
+	mutex_unlock(&nvmap_dev->clients_lock);
+	nvmap_get_total_mss(NULL, &total, heap_type);
+	seq_printf(s, "%-18s %18s %8s %10lluK\n", "total", "", "", K(total));
+	return 0;
+}
+
+DEBUGFS_OPEN_FOPS(clients);
+
+static int nvmap_debug_handles_by_pid_show(struct seq_file *s, void *unused)
+{
+	struct nvmap_pid_data *p = s->private;
+	struct nvmap_client *client;
+	struct nvmap_debugfs_handles_header header;
+	struct list_head *n;
+	int ret;
+
+	header.version = 1;
+	ret = seq_write(s, &header, sizeof(header));
+	if (ret < 0)
+		return ret;
+
+	mutex_lock(&nvmap_dev->clients_lock);
+
+	list_for_each(n, &nvmap_dev->clients) {
+		client = nvmap_client_from_list(n);
+		ret = nvmap_client_show_by_pid(client, s, p->pid);
+		if (ret)
+			break;
+	}
+
+	mutex_unlock(&nvmap_dev->clients_lock);
+	return ret;
+}
+
+DEBUGFS_OPEN_FOPS(handles_by_pid);
+
+#define PRINT_MEM_STATS_NOTE(x) \
+do { \
+	seq_printf(s, "Note: total memory is precise account of pages " \
+		"allocated by NvMap.\nIt doesn't match with all clients " \
+		"\"%s\" accumulated as shared memory \nis accounted in " \
+		"full in each clients \"%s\" that shared memory.\n", #x, #x); \
+} while (0)
+
+static int nvmap_debug_lru_allocations_show(struct seq_file *s, void *unused)
+{
+	struct list_head *n;
+
+	int total_handles = 0, migratable_handles = 0;
+	size_t total_size = 0, migratable_size = 0;
+
+	seq_printf(s, "%-18s %18s %8s %11s %8s %6s %6s %6s %6s %6s %8s\n",
+			"", "", "", "", "", "",
+			"", "PINS", "KMAPS", "UMAPS", "UID");
+
+	spin_lock(&nvmap_dev->lru_lock);
+	list_for_each(n, &nvmap_dev->lru_handles) {
+		struct nvmap_handle *h = nvmap_handle_from_lru(n);
+		u64 size = nvmap_handle_size(h);
+
+		total_handles++;
+		total_size += size;
+		if (nvmap_handle_is_migratable(h)) {
+			migratable_handles++;
+			migratable_size += size;
+		}
+		nvmap_handle_lru_show(h, s);
+	}
+	seq_printf(s, "total_handles = %d, migratable_handles = %d,"
+		"total_size=%zuK, migratable_size=%zuK\n",
+		total_handles, migratable_handles,
+		K(total_size), K(migratable_size));
+	spin_unlock(&nvmap_dev->lru_lock);
+	PRINT_MEM_STATS_NOTE(SIZE);
+	return 0;
+}
+
+DEBUGFS_OPEN_FOPS(lru_allocations);
+
+static int nvmap_debug_iovmm_procrank_show(struct seq_file *s, void *unused)
+{
+	u64 pss, total;
+	struct nvmap_client *client;
+	struct nvmap_device *dev = s->private;
+	struct list_head *n;
+	u64 total_memory, total_pss;
+
+	mutex_lock(&dev->clients_lock);
+	seq_printf(s, "%-18s %18s %8s %11s %11s\n",
+		"CLIENT", "PROCESS", "PID", "PSS", "SIZE");
+	list_for_each(n, &dev->clients) {
+		client = nvmap_client_from_list(n);
+		nvmap_client_stringify(client, s);
+		nvmap_client_calc_iovmm_mss(client, &pss, &total);
+		seq_printf(s, " %10lluK %10lluK\n", K(pss), K(total));
+	}
+	mutex_unlock(&dev->clients_lock);
+
+	nvmap_get_total_mss(&total_pss, &total_memory, NVMAP_HEAP_IOVMM);
+	seq_printf(s, "%-18s %18s %8s %10lluK %10lluK\n",
+		"total", "", "", K(total_pss), K(total_memory));
+	return 0;
+}
+
+DEBUGFS_OPEN_FOPS(iovmm_procrank);
+
+ulong nvmap_iovmm_get_used_pages(void)
+{
+	u64 total;
+
+	nvmap_get_total_mss(NULL, &total, NVMAP_HEAP_IOVMM);
+	return total >> PAGE_SHIFT;
+}
+
+static void nvmap_iovmm_debugfs_init(void)
+{
+	if (!IS_ERR_OR_NULL(nvmap_dev->debug_root)) {
+		struct dentry *iovmm_root =
+			debugfs_create_dir("iovmm", nvmap_dev->debug_root);
+		if (!IS_ERR_OR_NULL(iovmm_root)) {
+			debugfs_create_file("clients", S_IRUGO, iovmm_root,
+				(void *)(uintptr_t)NVMAP_HEAP_IOVMM,
+				&debug_clients_fops);
+			debugfs_create_file("allocations", S_IRUGO, iovmm_root,
+				(void *)(uintptr_t)NVMAP_HEAP_IOVMM,
+				&debug_allocations_fops);
+			debugfs_create_file("all_allocations", S_IRUGO,
+				iovmm_root, (void *)(uintptr_t)NVMAP_HEAP_IOVMM,
+				&debug_all_allocations_fops);
+			debugfs_create_file("orphan_handles", S_IRUGO,
+				iovmm_root, (void *)(uintptr_t)NVMAP_HEAP_IOVMM,
+				&debug_orphan_handles_fops);
+			debugfs_create_file("maps", S_IRUGO, iovmm_root,
+				(void *)(uintptr_t)NVMAP_HEAP_IOVMM,
+				&debug_maps_fops);
+			debugfs_create_file("procrank", S_IRUGO, iovmm_root,
+				nvmap_dev, &debug_iovmm_procrank_fops);
+		}
+	}
+}
+
+int __init nvmap_probe(struct platform_device *pdev)
+{
+	struct nvmap_platform_data *plat;
+	struct nvmap_device *dev;
+	struct dentry *nvmap_debug_root;
+	unsigned int i;
+	int e;
+	int generic_carveout_present = 0;
+	ulong start_time = sched_clock();
+
+	if (WARN_ON(nvmap_dev != NULL)) {
+		dev_err(&pdev->dev, "only one nvmap device may be present\n");
+		e = -ENODEV;
+		goto finish;
+	}
+
+	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
+	if (!dev) {
+		dev_err(&pdev->dev, "out of memory for device\n");
+		e = -ENOMEM;
+		goto finish;
+	}
+
+	nvmap_init(pdev);
+	plat = pdev->dev.platform_data;
+	if (!plat) {
+		dev_err(&pdev->dev, "no platform data?\n");
+		e = -ENODEV;
+		goto free_dev;
+	}
+
+	nvmap_dev = dev;
+	nvmap_dev->plat = plat;
+	/*
+	 * dma_parms need to be set with desired max_segment_size to avoid
+	 * DMA map API returning multiple IOVA's for the buffer size > 64KB.
+	 */
+	pdev->dev.dma_parms = &nvmap_dma_parameters;
+	dev->dev_user.minor = MISC_DYNAMIC_MINOR;
+	dev->dev_user.name = "nvmap";
+	dev->dev_user.fops = &nvmap_user_fops;
+	dev->dev_user.parent = &pdev->dev;
+	dev->handles = RB_ROOT;
+
+	if (of_property_read_bool(pdev->dev.of_node,
+				"no-cache-maint-by-set-ways"))
+		nvmap_cache_maint_by_set_ways = 0;
+
+	nvmap_override_cache_ops();
+#ifdef CONFIG_NVMAP_PAGE_POOLS
+	e = nvmap_page_pool_init(dev);
+	if (e)
+		goto fail;
+#endif
+
+	spin_lock_init(&dev->handle_lock);
+	INIT_LIST_HEAD(&dev->clients);
+	dev->pids = RB_ROOT;
+	mutex_init(&dev->clients_lock);
+	INIT_LIST_HEAD(&dev->lru_handles);
+	spin_lock_init(&dev->lru_lock);
+	dev->tags = RB_ROOT;
+	mutex_init(&dev->tags_lock);
+
+	e = misc_register(&dev->dev_user);
+	if (e) {
+		dev_err(&pdev->dev, "unable to register miscdevice %s\n",
+			dev->dev_user.name);
+		goto fail;
+	}
+
+	nvmap_debug_root = debugfs_create_dir("nvmap", NULL);
+	nvmap_dev->debug_root = nvmap_debug_root;
+	if (IS_ERR_OR_NULL(nvmap_debug_root))
+		dev_err(&pdev->dev, "couldn't create debug files\n");
+
+	debugfs_create_u32("max_handle_count", S_IRUGO,
+			nvmap_debug_root, &nvmap_max_handle_count);
+
+	nvmap_dev->dynamic_dma_map_mask = ~0;
+	nvmap_dev->cpu_access_mask = ~0;
+	for (i = 0; i < plat->nr_carveouts; i++)
+		(void)nvmap_carveout_create(&plat->carveouts[i]);
+
+	nvmap_iovmm_debugfs_init();
+#ifdef CONFIG_NVMAP_PAGE_POOLS
+	nvmap_page_pool_debugfs_init(nvmap_dev->debug_root);
+#endif
+	nvmap_cache_debugfs_init(nvmap_dev->debug_root);
+	nvmap_dev->handles_by_pid = debugfs_create_dir("handles_by_pid",
+							nvmap_debug_root);
+#if defined(CONFIG_DEBUG_FS)
+	debugfs_create_ulong("nvmap_init_time", S_IRUGO | S_IWUSR,
+				nvmap_dev->debug_root, &nvmap_init_time);
+#endif
+	nvmap_stats_init(nvmap_debug_root);
+	platform_set_drvdata(pdev, dev);
+
+	e = nvmap_dmabuf_stash_init();
+	if (e)
+		goto fail_heaps;
+
+	for (i = 0; i < dev->nr_carveouts; i++)
+		if (nvmap_carveout_heap_bit(
+					nvmap_dev_to_carveout(dev, i)) &
+					NVMAP_HEAP_CARVEOUT_GENERIC)
+			generic_carveout_present = 1;
+
+	if (generic_carveout_present) {
+		if (!iommu_present(&platform_bus_type))
+			nvmap_convert_iovmm_to_carveout = 1;
+		else if (!of_property_read_bool(pdev->dev.of_node,
+				"dont-convert-iovmm-to-carveout"))
+			nvmap_convert_iovmm_to_carveout = 1;
+	} else {
+		BUG_ON(!iommu_present(&platform_bus_type));
+		nvmap_convert_carveout_to_iovmm = 1;
+	}
+
+#ifdef CONFIG_NVMAP_PAGE_POOLS
+	if (nvmap_convert_iovmm_to_carveout)
+		nvmap_page_pool_fini(dev);
+#endif
+
+	goto finish;
+fail_heaps:
+	for (i = 0; i < dev->nr_carveouts; i++) {
+		struct nvmap_carveout_node *node =
+						nvmap_dev_to_carveout(dev, i);
+		nvmap_carveout_destroy(node);
+	}
+fail:
+#ifdef CONFIG_NVMAP_PAGE_POOLS
+	nvmap_page_pool_fini(nvmap_dev);
+#endif
+	kfree(dev->heaps);
+	if (dev->dev_user.minor != MISC_DYNAMIC_MINOR)
+		misc_deregister(&dev->dev_user);
+	nvmap_dev = NULL;
+free_dev:
+	kfree(dev);
+finish:
+	nvmap_init_time += sched_clock() - start_time;
+	return e;
+}
+
+int nvmap_remove(struct platform_device *pdev)
+{
+	struct nvmap_device *dev = platform_get_drvdata(pdev);
+	struct rb_node *n;
+	struct nvmap_handle *h;
+	int i;
+
+	misc_deregister(&dev->dev_user);
+
+	while ((n = rb_first(&dev->handles))) {
+		h = nvmap_handle_from_node(n);
+		nvmap_handle_destroy(h);
+	}
+
+	for (i = 0; i < dev->nr_carveouts; i++) {
+		struct nvmap_carveout_node *node =
+				nvmap_dev_to_carveout(dev, i);
+		nvmap_carveout_destroy(node);
+	}
+	kfree(dev->heaps);
+
+	kfree(dev);
+	nvmap_dev = NULL;
+	return 0;
+}
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_dev.h b/drivers/video/tegra/nvmap/nv2/nvmap_dev.h
new file mode 100644
index 000000000000..d860d29771d6
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_dev.h
@@ -0,0 +1,63 @@
+/*
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __NVMAP_DEV_H
+#define __NVMAP_DEV_H
+
+#include <linux/platform_device.h>
+#include <linux/miscdevice.h>
+#include <linux/nvmap.h>
+
+#include "nvmap_heap.h"
+#include "nvmap_pp.h"
+#include "nvmap_structs.h"
+
+
+struct nvmap_device {
+	struct rb_root	handles;
+	spinlock_t	handle_lock;
+	struct miscdevice dev_user;
+	// TODO: heaps should be a double pointer
+	struct nvmap_carveout_node *heaps;
+	int nr_heaps;
+	int nr_carveouts;
+#ifdef CONFIG_NVMAP_PAGE_POOLS
+	struct nvmap_page_pool pool;
+#endif
+	struct list_head clients;
+	struct rb_root pids;
+	struct mutex	clients_lock;
+	struct list_head lru_handles;
+	spinlock_t	lru_lock;
+	struct dentry *handles_by_pid;
+	struct dentry *debug_root;
+	struct nvmap_platform_data *plat;
+	struct rb_root	tags;
+	struct mutex	tags_lock;
+	u32 dynamic_dma_map_mask;
+	u32 cpu_access_mask;
+};
+
+int nvmap_probe(struct platform_device *pdev);
+int nvmap_remove(struct platform_device *pdev);
+int nvmap_init(struct platform_device *pdev);
+
+extern struct nvmap_device *nvmap_dev;
+
+u32 nvmap_cpu_access_mask(void);
+
+struct nvmap_carveout_node *nvmap_dev_to_carveout(struct nvmap_device *dev, int i);
+
+int nvmap_dmabuf_stash_init(void);
+
+#endif /* __NVMAP_DEV_H */
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_dmabuf.c b/drivers/video/tegra/nvmap/nv2/nvmap_dmabuf.c
new file mode 100644
index 000000000000..b7750f75d78f
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_dmabuf.c
@@ -0,0 +1,88 @@
+/*
+ * dma_buf exporter for nvmap
+ *
+ * Copyright (c) 2012-2018, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"nvmap: %s() " fmt, __func__
+
+#include <linux/list.h>
+#include <linux/slab.h>
+#include <linux/nvmap.h>
+#include <linux/dma-buf.h>
+#include <linux/version.h>
+
+#include <trace/events/nvmap.h>
+
+#include "nvmap_dmabuf.h"
+
+extern bool dmabuf_is_nvmap(struct dma_buf *dmabuf);
+extern struct dma_buf_ops nvmap_dma_buf_ops;
+
+int nvmap_dmabuf_is_nvmap(struct dma_buf *dmabuf)
+{
+	return dmabuf_is_nvmap(dmabuf);
+}
+
+struct dma_buf *nvmap_dmabuf_from_fd(int fd)
+{
+	struct dma_buf *dmabuf;
+
+	dmabuf = dma_buf_get(fd);
+	if (IS_ERR(dmabuf))
+		return dmabuf;
+	dma_buf_put(dmabuf);
+	return dmabuf;
+}
+
+struct nvmap_handle * nvmap_dmabuf_to_handle(struct dma_buf *dmabuf)
+{
+	struct nvmap_handle_info *info;
+	if (!dmabuf_is_nvmap(dmabuf)) {
+		return ERR_PTR(-EINVAL);
+	}
+
+	info = dmabuf->priv;
+	return info->handle;
+}
+
+void nvmap_dmabuf_install_fd(struct dma_buf *dmabuf, int fd)
+{
+	fd_install(fd, dmabuf->file);
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0)
+static struct dma_buf *__dma_buf_export(void * priv,
+					size_t size)
+{
+	DEFINE_DMA_BUF_EXPORT_INFO(exp_info);
+
+	exp_info.priv = priv;
+	exp_info.ops = &nvmap_dma_buf_ops;
+	exp_info.size = size;
+	exp_info.flags = O_RDWR;
+	exp_info.exp_flags = DMABUF_CAN_DEFER_UNMAP |
+				DMABUF_SKIP_CACHE_SYNC;
+
+	return dma_buf_export(&exp_info);
+}
+#else
+#define __dma_buf_export(priv, size) \
+	dma_buf_export(priv, &nvmap_dma_buf_ops, size, O_RDWR, NULL)
+#endif
+/*
+ * Make a dmabuf object for an nvmap handle.
+ */
+struct dma_buf *nvmap_dmabuf_create(void * priv, size_t size)
+{
+	return __dma_buf_export(priv, size);
+}
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_dmabuf.h b/drivers/video/tegra/nvmap/nv2/nvmap_dmabuf.h
new file mode 100644
index 000000000000..53c4bf39868b
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_dmabuf.h
@@ -0,0 +1,33 @@
+/*
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __NVMAP_DMABUF_H
+#define __NVMAP_DMABUF_H
+
+#include "nvmap_structs.h"
+
+struct nvmap_handle_info {
+	struct nvmap_handle *handle;
+	struct list_head maps;
+	struct mutex maps_lock;
+};
+
+struct dma_buf *nvmap_dmabuf_create(void * priv, size_t size);
+void nvmap_dmabuf_install_fd(struct dma_buf *dmabuf, int fd);
+struct nvmap_handle * nvmap_dmabuf_to_handle(struct dma_buf *dmabuf);
+struct dma_buf *nvmap_dmabuf_from_fd(int fd);
+int nvmap_dmabuf_is_nvmap(struct dma_buf *dmabuf);
+
+void nvmap_dmabufs_free(struct list_head *dmabuf_list);
+
+#endif /* __NVMAP_DMABUF_H */
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_dmabuf_ops.c b/drivers/video/tegra/nvmap/nv2/nvmap_dmabuf_ops.c
new file mode 100644
index 000000000000..94b302816fac
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_dmabuf_ops.c
@@ -0,0 +1,634 @@
+/*
+ * dma_buf exporter for nvmap
+ *
+ * Copyright (c) 2012-2018, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"nvmap: %s() " fmt, __func__
+
+#include <linux/list.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/export.h>
+#include <linux/nvmap.h>
+#include <linux/dma-buf.h>
+#include <linux/spinlock.h>
+#include <linux/mutex.h>
+#include <linux/atomic.h>
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+#include <linux/stringify.h>
+#include <linux/of.h>
+#include <linux/platform/tegra/tegra_fd.h>
+#include <linux/version.h>
+#include <linux/iommu.h>
+
+#include <trace/events/nvmap.h>
+
+#include "nvmap_handle.h"
+// TODO: refactor dmabuf_ops and remove this
+#include "nvmap_handle_priv.h"
+
+#include "nvmap_dmabuf.h"
+#include "nvmap_dev.h"
+#include "nvmap_vma.h"
+#include "nvmap_client.h"
+
+#include "nvmap_ioctl.h"
+
+struct nvmap_handle_dmabuf_priv {
+	void *priv;
+	struct device *dev;
+	void (*priv_release)(void *priv);
+	struct list_head list;
+};
+
+// TODO: Remove these references to nvmap_fault
+extern struct vm_operations_struct nvmap_vma_ops;
+extern void nvmap_vma_open(struct vm_area_struct *vma);
+
+
+/**
+ * List node for maps of nvmap handles via the dma_buf API. These store the
+ * necessary info for stashing mappings.
+ *
+ * @iommu_domain Domain for which this SGT is valid - for supporting multi-asid.
+ * @dir DMA direction.
+ * @sgt The scatter gather table to stash.
+ * @refs Reference counting.
+ * @maps_entry Entry on a given attachment's list of maps.
+ * @stash_entry Entry on the stash list.
+ * @owner The owner of this struct. There can be only one.
+ */
+struct nvmap_handle_sgt {
+	struct iommu_domain *domain;
+	enum dma_data_direction dir;
+	struct sg_table *sgt;
+	struct device *dev;
+
+	atomic_t refs;
+
+	struct list_head maps_entry;
+	struct list_head stash_entry; /* lock the stash before accessing. */
+
+	struct nvmap_handle_info *owner;
+} ____cacheline_aligned_in_smp;
+
+static DEFINE_MUTEX(nvmap_stashed_maps_lock);
+static LIST_HEAD(nvmap_stashed_maps);
+static struct kmem_cache *handle_sgt_cache;
+struct dma_buf_ops nvmap_dma_buf_ops;
+
+static bool nvmap_attach_handle_same_asid(struct dma_buf_attachment *attach,
+					struct nvmap_handle_sgt *nvmap_sgt)
+{
+	return iommu_get_domain_for_dev(attach->dev) == nvmap_sgt->domain;
+
+}
+
+void nvmap_dmabufs_free(struct list_head *dmabuf_list)
+{
+	struct nvmap_handle_dmabuf_priv *curr, *next;
+
+	list_for_each_entry_safe(curr, next, dmabuf_list, list) {
+		curr->priv_release(curr->priv);
+		list_del(&curr->list);
+		kzfree(curr);
+	}
+}
+/*
+ * Initialize a kmem cache for allocating nvmap_handle_sgt's.
+ */
+int nvmap_dmabuf_stash_init(void)
+{
+	handle_sgt_cache = KMEM_CACHE(nvmap_handle_sgt, 0);
+	if (IS_ERR_OR_NULL(handle_sgt_cache)) {
+		pr_err("Failed to make kmem cache for nvmap_handle_sgt.\n");
+		return -ENOMEM;
+	}
+
+	return 0;
+}
+
+static int nvmap_dmabuf_attach(struct dma_buf *dmabuf, struct device *dev,
+			       struct dma_buf_attachment *attach)
+{
+	struct nvmap_handle_info *info = dmabuf->priv;
+
+	trace_nvmap_dmabuf_attach(dmabuf, dev);
+
+	dev_dbg(dev, "%s() 0x%p\n", __func__, info->handle);
+	return 0;
+}
+
+static void nvmap_dmabuf_detach(struct dma_buf *dmabuf,
+				struct dma_buf_attachment *attach)
+{
+	struct nvmap_handle_info *info = dmabuf->priv;
+
+	trace_nvmap_dmabuf_detach(dmabuf, attach->dev);
+
+	dev_dbg(attach->dev, "%s() 0x%p\n", __func__, info->handle);
+}
+
+/*
+ * Make sure this mapping is no longer stashed - this corresponds to a "hit". If
+ * the mapping is not stashed this is just a no-op.
+ */
+static void __nvmap_dmabuf_del_stash(struct nvmap_handle_sgt *nvmap_sgt)
+{
+	mutex_lock(&nvmap_stashed_maps_lock);
+	if (list_empty(&nvmap_sgt->stash_entry)) {
+		mutex_unlock(&nvmap_stashed_maps_lock);
+		return;
+	}
+
+	pr_debug("Removing map from stash.\n");
+	list_del_init(&nvmap_sgt->stash_entry);
+	mutex_unlock(&nvmap_stashed_maps_lock);
+}
+
+static inline bool access_vpr_phys(struct device *dev)
+{
+	if (!device_is_iommuable(dev))
+		return true;
+
+	/*
+	 * Assumes gpu nodes always have DT entry, this is valid as device
+	 * specifying access-vpr-phys will do so through its DT entry.
+	 */
+	if (!dev->of_node)
+		return false;
+
+	return !!of_find_property(dev->of_node, "access-vpr-phys", NULL);
+}
+
+/*
+ * Free an sgt completely. This will bypass the ref count. This also requires
+ * the nvmap_sgt's owner's lock is already taken.
+ */
+static void __nvmap_dmabuf_free_sgt_locked(struct nvmap_handle_sgt *nvmap_sgt)
+{
+	struct nvmap_handle_info *info = nvmap_sgt->owner;
+	u32 heap_type;
+	DEFINE_DMA_ATTRS(attrs);
+
+	list_del(&nvmap_sgt->maps_entry);
+
+	heap_type = nvmap_handle_heap_type(info->handle);
+
+	if (!(nvmap_dev->dynamic_dma_map_mask & heap_type)) {
+		sg_dma_address(nvmap_sgt->sgt->sgl) = 0;
+	} else if (heap_type == NVMAP_HEAP_CARVEOUT_VPR &&
+			access_vpr_phys(nvmap_sgt->dev)) {
+		sg_dma_address(nvmap_sgt->sgt->sgl) = 0;
+	} else {
+		dma_set_attr(DMA_ATTR_SKIP_IOVA_GAP, __DMA_ATTR(attrs));
+		dma_set_attr(DMA_ATTR_SKIP_CPU_SYNC, __DMA_ATTR(attrs));
+		dma_unmap_sg_attrs(nvmap_sgt->dev,
+				   nvmap_sgt->sgt->sgl, nvmap_sgt->sgt->nents,
+				   nvmap_sgt->dir, __DMA_ATTR(attrs));
+	}
+	__nvmap_free_sg_table(NULL, info->handle, nvmap_sgt->sgt);
+
+	WARN(atomic_read(&nvmap_sgt->refs), "nvmap: Freeing reffed SGT!");
+	kmem_cache_free(handle_sgt_cache, nvmap_sgt);
+}
+
+/*
+ * Evict an entry from the IOVA stash. This does not do anything to the actual
+ * mapping itself - this merely takes the passed nvmap_sgt out of the stash
+ * and decrements the necessary cache stats.
+ */
+static void __nvmap_dmabuf_evict_stash_locked(
+			struct nvmap_handle_sgt *nvmap_sgt)
+{
+	if (!list_empty(&nvmap_sgt->stash_entry))
+		list_del_init(&nvmap_sgt->stash_entry);
+}
+
+/*
+ * Locks the stash before doing the eviction.
+ */
+static void __nvmap_dmabuf_evict_stash(struct nvmap_handle_sgt *nvmap_sgt)
+{
+	mutex_lock(&nvmap_stashed_maps_lock);
+	__nvmap_dmabuf_evict_stash_locked(nvmap_sgt);
+	mutex_unlock(&nvmap_stashed_maps_lock);
+}
+
+/*
+ * Prepare an SGT for potential stashing later on.
+ */
+static int __nvmap_dmabuf_prep_sgt_locked(struct dma_buf_attachment *attach,
+				   enum dma_data_direction dir,
+				   struct sg_table *sgt)
+{
+	struct nvmap_handle_sgt *nvmap_sgt;
+	struct nvmap_handle_info *info = attach->dmabuf->priv;
+
+	pr_debug("Prepping SGT.\n");
+	nvmap_sgt = kmem_cache_alloc(handle_sgt_cache, GFP_KERNEL);
+	if (IS_ERR_OR_NULL(nvmap_sgt)) {
+		pr_err("Prepping SGT failed.\n");
+		return -ENOMEM;
+	}
+
+	nvmap_sgt->domain = iommu_get_domain_for_dev(attach->dev);
+	nvmap_sgt->dir = dir;
+	nvmap_sgt->sgt = sgt;
+	nvmap_sgt->dev = attach->dev;
+	nvmap_sgt->owner = info;
+	INIT_LIST_HEAD(&nvmap_sgt->stash_entry);
+	atomic_set(&nvmap_sgt->refs, 1);
+	list_add(&nvmap_sgt->maps_entry, &info->maps);
+	return 0;
+}
+
+/*
+ * Called when an SGT is no longer being used by a device. This will not
+ * necessarily free the SGT - instead it may stash the SGT.
+ */
+static void __nvmap_dmabuf_stash_sgt_locked(struct dma_buf_attachment *attach,
+				    enum dma_data_direction dir,
+				    struct sg_table *sgt)
+{
+	struct nvmap_handle_sgt *nvmap_sgt;
+	struct nvmap_handle_info *info = attach->dmabuf->priv;
+
+	pr_debug("Stashing SGT - if necessary.\n");
+	list_for_each_entry(nvmap_sgt, &info->maps, maps_entry) {
+		if (nvmap_sgt->sgt == sgt) {
+			if (!atomic_sub_and_test(1, &nvmap_sgt->refs))
+				goto done;
+
+			__nvmap_dmabuf_free_sgt_locked(nvmap_sgt);
+			goto done;
+		}
+	}
+
+done:
+	return;
+}
+
+/*
+ * Checks if there is already a map for this attachment. If so increment the
+ * ref count on said map and return the associated sg_table. Otherwise return
+ * NULL.
+ *
+ * If it turns out there is a map, this also checks to see if the map needs to
+ * be removed from the stash - if so, the map is removed.
+ */
+static struct sg_table *__nvmap_dmabuf_get_sgt_locked(
+	struct dma_buf_attachment *attach, enum dma_data_direction dir)
+{
+	struct nvmap_handle_sgt *nvmap_sgt;
+	struct sg_table *sgt = NULL;
+	struct nvmap_handle_info *info = attach->dmabuf->priv;
+
+	pr_debug("Getting SGT from stash.\n");
+	list_for_each_entry(nvmap_sgt, &info->maps, maps_entry) {
+		if (!nvmap_attach_handle_same_asid(attach, nvmap_sgt))
+			continue;
+
+		/* We have a hit. */
+		pr_debug("Stash hit (%s)!\n", dev_name(attach->dev));
+		sgt = nvmap_sgt->sgt;
+		atomic_inc(&nvmap_sgt->refs);
+		__nvmap_dmabuf_del_stash(nvmap_sgt);
+		break;
+	}
+
+	return sgt;
+}
+
+/*
+ * If stashing is disabled then the stash related ops become no-ops.
+ */
+struct sg_table *_nvmap_dmabuf_map_dma_buf(
+	struct dma_buf_attachment *attach, enum dma_data_direction dir)
+{
+	struct nvmap_handle_info *info = attach->dmabuf->priv;
+	int ents = 0;
+	struct sg_table *sgt;
+	DEFINE_DMA_ATTRS(attrs);
+	u32 heap_type = nvmap_handle_heap_type(info->handle);
+	atomic_t *h_pin = nvmap_handle_pin(info->handle);
+
+	trace_nvmap_dmabuf_map_dma_buf(attach->dmabuf, attach->dev);
+
+	nvmap_lru_reset(nvmap_handle_lru(info->handle));
+	mutex_lock(&info->maps_lock);
+
+	atomic_inc(h_pin);
+
+	sgt = __nvmap_dmabuf_get_sgt_locked(attach, dir);
+	if (sgt)
+		goto cache_hit;
+
+	sgt = __nvmap_sg_table(NULL, info->handle);
+	if (IS_ERR(sgt)) {
+		atomic_dec(h_pin);
+		mutex_unlock(&info->maps_lock);
+		return sgt;
+	}
+
+	if (!nvmap_handle_is_allocated(info->handle)) {
+		goto err_map;
+	} else if (!(nvmap_dev->dynamic_dma_map_mask & heap_type)) {
+		sg_dma_address(sgt->sgl) = info->handle->carveout->base;
+	} else if (heap_type == NVMAP_HEAP_CARVEOUT_VPR &&
+			access_vpr_phys(attach->dev)) {
+		sg_dma_address(sgt->sgl) = 0;
+	} else {
+		dma_set_attr(DMA_ATTR_SKIP_IOVA_GAP, __DMA_ATTR(attrs));
+		dma_set_attr(DMA_ATTR_SKIP_CPU_SYNC, __DMA_ATTR(attrs));
+		ents = dma_map_sg_attrs(attach->dev, sgt->sgl,
+					sgt->nents, dir, __DMA_ATTR(attrs));
+		if (ents <= 0)
+			goto err_map;
+	}
+
+	if (__nvmap_dmabuf_prep_sgt_locked(attach, dir, sgt)) {
+		WARN(1, "No mem to prep sgt.\n");
+		goto err_prep;
+	}
+
+cache_hit:
+	attach->priv = sgt;
+	mutex_unlock(&info->maps_lock);
+	return sgt;
+
+err_prep:
+	dma_unmap_sg_attrs(attach->dev, sgt->sgl, sgt->nents, dir, __DMA_ATTR(attrs));
+err_map:
+	__nvmap_free_sg_table(NULL, info->handle, sgt);
+	atomic_dec(h_pin);
+	mutex_unlock(&info->maps_lock);
+	return ERR_PTR(-ENOMEM);
+}
+
+__weak struct sg_table *nvmap_dmabuf_map_dma_buf(
+	struct dma_buf_attachment *attach, enum dma_data_direction dir)
+{
+	return _nvmap_dmabuf_map_dma_buf(attach, dir);
+}
+
+void _nvmap_dmabuf_unmap_dma_buf(struct dma_buf_attachment *attach,
+				       struct sg_table *sgt,
+				       enum dma_data_direction dir)
+{
+	struct nvmap_handle_info *info = attach->dmabuf->priv;
+
+	trace_nvmap_dmabuf_unmap_dma_buf(attach->dmabuf, attach->dev);
+
+	mutex_lock(&info->maps_lock);
+	if (!atomic_add_unless(nvmap_handle_pin(info->handle), -1, 0)) {
+		mutex_unlock(&info->maps_lock);
+		WARN(1, "Unpinning handle that has yet to be pinned!\n");
+		return;
+	}
+	__nvmap_dmabuf_stash_sgt_locked(attach, dir, sgt);
+	mutex_unlock(&info->maps_lock);
+}
+
+__weak void nvmap_dmabuf_unmap_dma_buf(struct dma_buf_attachment *attach,
+				       struct sg_table *sgt,
+				       enum dma_data_direction dir)
+{
+	_nvmap_dmabuf_unmap_dma_buf(attach, sgt, dir);
+}
+
+static void nvmap_dmabuf_release(struct dma_buf *dmabuf)
+{
+	struct nvmap_handle_info *info = dmabuf->priv;
+	struct nvmap_handle_sgt *nvmap_sgt;
+
+	trace_nvmap_dmabuf_release(info->handle->owner ?
+			   nvmap_client_name(info->handle->owner) : "unknown",
+			   info->handle,
+			   dmabuf);
+
+	mutex_lock(&info->handle->lock);
+	BUG_ON(dmabuf != info->handle->dmabuf);
+	info->handle->dmabuf = NULL;
+	mutex_unlock(&info->handle->lock);
+
+	mutex_lock(&info->maps_lock);
+	while (!list_empty(&info->maps)) {
+		nvmap_sgt = list_first_entry(&info->maps,
+					     struct nvmap_handle_sgt,
+					     maps_entry);
+		__nvmap_dmabuf_evict_stash(nvmap_sgt);
+		__nvmap_dmabuf_free_sgt_locked(nvmap_sgt);
+	}
+	mutex_unlock(&info->maps_lock);
+
+	nvmap_handle_put(info->handle);
+	kfree(info);
+}
+
+static int nvmap_dmabuf_begin_cpu_access(struct dma_buf *dmabuf,
+					  size_t start, size_t len,
+					  enum dma_data_direction dir)
+{
+	struct nvmap_handle_info *info = dmabuf->priv;
+
+	trace_nvmap_dmabuf_begin_cpu_access(dmabuf, start, len);
+	return nvmap_handle_cache_maint(info->handle,
+						start, start + len,
+						NVMAP_CACHE_OP_WB_INV);
+}
+
+static void nvmap_dmabuf_end_cpu_access(struct dma_buf *dmabuf,
+				       size_t start, size_t len,
+				       enum dma_data_direction dir)
+{
+	struct nvmap_handle_info *info = dmabuf->priv;
+
+	trace_nvmap_dmabuf_end_cpu_access(dmabuf, start, len);
+	nvmap_handle_cache_maint(info->handle,
+				   start, start + len,
+				   NVMAP_CACHE_OP_WB);
+}
+
+static void *nvmap_dmabuf_kmap(struct dma_buf *dmabuf, unsigned long page_num)
+{
+	struct nvmap_handle_info *info = dmabuf->priv;
+
+	trace_nvmap_dmabuf_kmap(dmabuf);
+	return __nvmap_kmap(info->handle, page_num);
+}
+
+static void nvmap_dmabuf_kunmap(struct dma_buf *dmabuf,
+		unsigned long page_num, void *addr)
+{
+	struct nvmap_handle_info *info = dmabuf->priv;
+
+	trace_nvmap_dmabuf_kunmap(dmabuf);
+	__nvmap_kunmap(info->handle, page_num, addr);
+}
+
+static void *nvmap_dmabuf_kmap_atomic(struct dma_buf *dmabuf,
+				      unsigned long page_num)
+{
+	WARN(1, "%s() can't be called from atomic\n", __func__);
+	return NULL;
+}
+
+int __nvmap_map(struct nvmap_handle *h, struct vm_area_struct *vma)
+{
+	struct nvmap_vma_priv *priv;
+	u32 heap_type;
+
+	h = nvmap_handle_get(h);
+	if (!h)
+		return -EINVAL;
+
+	heap_type = nvmap_handle_heap_type(h);
+	if (!(heap_type & nvmap_dev->cpu_access_mask)) {
+		nvmap_handle_put(h);
+		return -EPERM;
+	}
+
+	/*
+	 * Don't allow mmap on VPR memory as it would be mapped
+	 * as device memory. User space shouldn't be accessing
+	 * device memory.
+	 */
+	if (heap_type == NVMAP_HEAP_CARVEOUT_VPR)  {
+		nvmap_handle_put(h);
+		return -EPERM;
+	}
+
+	priv = kzalloc(sizeof(*priv), GFP_KERNEL);
+	if (!priv) {
+		nvmap_handle_put(h);
+		return -ENOMEM;
+	}
+	priv->handle = h;
+
+	vma->vm_flags |= VM_SHARED | VM_DONTEXPAND |
+			  VM_DONTDUMP | VM_DONTCOPY |
+			  (h->heap_pgalloc ? 0 : VM_PFNMAP);
+	vma->vm_ops = &nvmap_vma_ops;
+	BUG_ON(vma->vm_private_data != NULL);
+	vma->vm_private_data = priv;
+	vma->vm_page_prot = nvmap_handle_pgprot(h, vma->vm_page_prot);
+	nvmap_vma_open(vma);
+	return 0;
+}
+
+static int nvmap_dmabuf_mmap(struct dma_buf *dmabuf, struct vm_area_struct *vma)
+{
+	struct nvmap_handle_info *info = dmabuf->priv;
+
+	trace_nvmap_dmabuf_mmap(dmabuf);
+
+	return __nvmap_map(info->handle, vma);
+}
+
+static void *nvmap_dmabuf_vmap(struct dma_buf *dmabuf)
+{
+	struct nvmap_handle_info *info = dmabuf->priv;
+
+	trace_nvmap_dmabuf_vmap(dmabuf);
+	return __nvmap_mmap(info->handle);
+}
+
+static void nvmap_dmabuf_vunmap(struct dma_buf *dmabuf, void *vaddr)
+{
+	struct nvmap_handle_info *info = dmabuf->priv;
+
+	trace_nvmap_dmabuf_vunmap(dmabuf);
+	__nvmap_munmap(info->handle, vaddr);
+}
+
+static int nvmap_dmabuf_set_private(struct dma_buf *dmabuf,
+		struct device *dev, void *priv, void (*delete)(void *priv))
+{
+	struct nvmap_handle_info *info = dmabuf->priv;
+	struct nvmap_handle *handle = info->handle;
+	struct nvmap_handle_dmabuf_priv *curr = NULL;
+	int ret = 0;
+
+	mutex_lock(&handle->lock);
+	list_for_each_entry(curr, &handle->dmabuf_priv, list) {
+		if (curr->dev == dev) {
+			ret = -EEXIST;
+			goto unlock;
+		}
+	}
+
+	curr = kmalloc(sizeof(*curr), GFP_KERNEL);
+	if (!curr) {
+		ret = -ENOMEM;
+		goto unlock;
+	}
+	curr->priv = priv;
+	curr->dev = dev;
+	curr->priv_release = delete;
+	list_add_tail(&curr->list, &handle->dmabuf_priv);
+unlock:
+	mutex_unlock(&handle->lock);
+	return ret;
+}
+
+static void *nvmap_dmabuf_get_private(struct dma_buf *dmabuf,
+		struct device *dev)
+{
+	void *priv = NULL;
+	struct nvmap_handle_info *info = dmabuf->priv;
+	struct nvmap_handle *handle = info->handle;
+	struct nvmap_handle_dmabuf_priv *curr = NULL;
+
+	mutex_lock(&handle->lock);
+	list_for_each_entry(curr, &handle->dmabuf_priv, list) {
+		if (curr->dev == dev) {
+			priv = curr->priv;
+			goto unlock;
+		}
+	}
+unlock:
+	mutex_unlock(&handle->lock);
+	return priv;
+}
+
+struct dma_buf_ops nvmap_dma_buf_ops = {
+	.attach		= nvmap_dmabuf_attach,
+	.detach		= nvmap_dmabuf_detach,
+	.map_dma_buf	= nvmap_dmabuf_map_dma_buf,
+	.unmap_dma_buf	= nvmap_dmabuf_unmap_dma_buf,
+	.release	= nvmap_dmabuf_release,
+	.begin_cpu_access = nvmap_dmabuf_begin_cpu_access,
+	.end_cpu_access = nvmap_dmabuf_end_cpu_access,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	.map_atomic	= nvmap_dmabuf_kmap_atomic,
+	.map		= nvmap_dmabuf_kmap,
+	.unmap		= nvmap_dmabuf_kunmap,
+#else
+	.kmap_atomic	= nvmap_dmabuf_kmap_atomic,
+	.kmap		= nvmap_dmabuf_kmap,
+	.kunmap		= nvmap_dmabuf_kunmap,
+#endif
+	.mmap		= nvmap_dmabuf_mmap,
+	.vmap		= nvmap_dmabuf_vmap,
+	.vunmap		= nvmap_dmabuf_vunmap,
+	.set_drvdata	= nvmap_dmabuf_set_private,
+	.get_drvdata	= nvmap_dmabuf_get_private,
+};
+
+bool dmabuf_is_nvmap(struct dma_buf *dmabuf)
+{
+	return dmabuf->ops == &nvmap_dma_buf_ops;
+}
+EXPORT_SYMBOL(dmabuf_is_nvmap);
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_dmabuf_t19x.c b/drivers/video/tegra/nvmap/nv2/nvmap_dmabuf_t19x.c
new file mode 100644
index 000000000000..5892349635f0
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_dmabuf_t19x.c
@@ -0,0 +1,102 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_dmabuf_t19x.c
+ *
+ * Copyright (c) 2016-2017, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"nvmap: %s() " fmt, __func__
+
+#include <linux/slab.h>
+#include <linux/vmalloc.h>
+#include <linux/of.h>
+#include <linux/miscdevice.h>
+#include <linux/nvmap_t19x.h>
+
+#include "nvmap_handle.h"
+#include "nvmap_cache.h"
+#include "nvmap_dmabuf.h"
+#include "nvmap_dev.h"
+
+// TODO: Re-name this function so it can be in a header file
+struct sg_table *_nvmap_dmabuf_map_dma_buf(
+	struct dma_buf_attachment *attach, enum dma_data_direction dir);
+void _nvmap_dmabuf_unmap_dma_buf(struct dma_buf_attachment *attach,
+				       struct sg_table *sgt,
+				       enum dma_data_direction dir);
+
+extern bool of_dma_is_coherent(struct device_node *np);
+
+static void nvmap_handle_t19x_free(void *ptr)
+{
+	struct nvmap_handle_t19x *handle_t19x =
+		(struct nvmap_handle_t19x *)ptr;
+	int outstanding_nc_pin = atomic_read(&handle_t19x->nc_pin);
+
+	WARN(outstanding_nc_pin,
+		"outstanding dma maps from %d coherent devices",
+		outstanding_nc_pin);
+	kfree(handle_t19x);
+}
+
+struct sg_table *nvmap_dmabuf_map_dma_buf(
+	struct dma_buf_attachment *attach, enum dma_data_direction dir)
+{
+	struct nvmap_handle_info *info = attach->dmabuf->priv;
+	struct nvmap_handle *handle = info->handle;
+	struct nvmap_handle_t19x *handle_t19x = NULL;
+	struct device *dev = nvmap_dev->dev_user.parent;
+	struct sg_table *sg_table;
+	struct dma_buf *dmabuf = nvmap_handle_to_dmabuf(handle);
+
+	if (!nvmap_version_t19x)
+		goto dmabuf_map;
+
+	handle_t19x = dma_buf_get_drvdata(dmabuf, dev);
+	if (!handle_t19x && !of_dma_is_coherent(attach->dev->of_node)) {
+		handle_t19x = kmalloc(sizeof(*handle_t19x), GFP_KERNEL);
+		if (WARN(!handle_t19x, "No memory!!"))
+			return ERR_PTR(-ENOMEM);
+
+		atomic_set(&handle_t19x->nc_pin, 0);
+		dma_buf_set_drvdata(dmabuf, dev,
+				handle_t19x, nvmap_handle_t19x_free);
+	}
+
+	if (!of_dma_is_coherent(attach->dev->of_node))
+		atomic_inc(&handle_t19x->nc_pin);
+
+dmabuf_map:
+	sg_table = _nvmap_dmabuf_map_dma_buf(attach, dir);
+	/* no need to free handle_t19x, it is freed with handle */
+	if (IS_ERR(sg_table))
+		if (handle_t19x)
+			atomic_dec(&handle_t19x->nc_pin);
+
+	return sg_table;
+}
+
+void nvmap_dmabuf_unmap_dma_buf(struct dma_buf_attachment *attach,
+	 struct sg_table *sgt, enum dma_data_direction dir)
+{
+	struct nvmap_handle_info *info = attach->dmabuf->priv;
+	struct nvmap_handle *handle = info->handle;
+	struct device *dev = nvmap_dev->dev_user.parent;
+	struct nvmap_handle_t19x *handle_t19x;
+	struct dma_buf *dmabuf = nvmap_handle_to_dmabuf(handle);
+
+	_nvmap_dmabuf_unmap_dma_buf(attach, sgt, dir);
+
+	handle_t19x = dma_buf_get_drvdata(dmabuf, dev);
+	if (handle_t19x && !of_dma_is_coherent(attach->dev->of_node))
+		atomic_dec(&handle_t19x->nc_pin);
+}
+
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_fault.c b/drivers/video/tegra/nvmap/nv2/nvmap_fault.c
new file mode 100644
index 000000000000..df13d926276b
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_fault.c
@@ -0,0 +1,189 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_fault.c
+ *
+ * Copyright (c) 2011-2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"%s: " fmt, __func__
+
+#include <linux/highmem.h>
+#include <linux/mm.h>
+#include <linux/mm_types.h>
+#include <linux/version.h>
+#include <linux/vmalloc.h>
+#include <linux/slab.h>
+#include <linux/nvmap.h>
+
+#include "nvmap_vma.h"
+#include "nvmap_handle.h"
+
+#include <trace/events/nvmap.h>
+
+
+/* to ensure that the backing store for the VMA isn't freed while a fork'd
+ * reference still exists, nvmap_vma_open increments the reference count on
+ * the handle, and nvmap_vma_close decrements it. alternatively, we could
+ * disallow copying of the vma, or behave like pmem and zap the pages. FIXME.
+*/
+void nvmap_vma_open(struct vm_area_struct *vma)
+{
+	struct nvmap_vma_priv *priv = vma->vm_private_data;
+	struct nvmap_handle *handle = NULL;
+	ulong vma_open_count;
+	int err;
+
+	if (!priv) {
+		WARN(1, "VMA missing priv");
+	}
+
+	handle = priv->handle;
+	if (!handle) {
+		WARN(1, "VMA priv misses handle");
+	}
+
+	// TODO: Pretty sure no one uses this ref count
+	nvmap_handle_umap_inc(handle);
+
+	vma_open_count = atomic_inc_return(&priv->count);
+	if (vma_open_count == 1) {
+		err = nvmap_handle_open_vma(handle);
+		if (err) {
+			WARN(1, "handle_open_vma failed");
+		}
+	}
+
+
+	err = nvmap_handle_add_vma(handle, vma);
+	if (err) {
+		WARN(1, "vma not tracked");
+	}
+}
+
+static void nvmap_vma_close(struct vm_area_struct *vma)
+{
+	struct nvmap_vma_priv *priv = vma->vm_private_data;
+	struct nvmap_handle *h;
+	ulong vma_open_count;
+	int err;
+
+	if (!priv)
+		return;
+
+	h = priv->handle;
+	BUG_ON(!h);
+
+	err = nvmap_handle_del_vma(h, vma);
+	if (err) {
+		WARN(1, "Handle del vma failed");
+		return;
+	}
+
+	nvmap_handle_umap_dec(h);
+
+	vma_open_count = __atomic_add_unless(&priv->count, -1, 0);
+	if (vma_open_count == 1) {
+		err = nvmap_handle_close_vma(h);
+		if (err)
+			WARN(1, "Handle close vma failed");
+
+		// TODO: There is NO handle_get in vma_open
+		//  This is PROBABLY a bug
+		if (priv->handle)
+			nvmap_handle_put(priv->handle);
+		vma->vm_private_data = NULL;
+		kfree(priv);
+	}
+}
+
+static int nvmap_vma_fault(struct vm_area_struct *vma, struct vm_fault *vmf,
+				unsigned long vmf_address)
+{
+	struct page *page = NULL;
+	struct nvmap_vma_priv *priv;
+	unsigned long offs;
+	int err;
+
+	offs = (unsigned long)(vmf_address - vma->vm_start);
+	priv = vma->vm_private_data;
+	if (!priv || !priv->handle)
+		return VM_FAULT_SIGBUS;
+
+	offs += priv->offs;
+	/* if the VMA was split for some reason, vm_pgoff will be the VMA's
+	 * offset from the original VMA */
+	offs += (vma->vm_pgoff << PAGE_SHIFT);
+
+
+	err = nvmap_handle_fault_vma(priv->handle, offs, &page);
+	if (err){
+		if (page != NULL) {
+			vm_insert_pfn(vma, (unsigned long)vmf_address,
+							page_to_pfn(page));
+		}
+		return err;
+	}
+
+	if (page)
+		get_page(page);
+	vmf->page = page;
+	return (page) ? 0 : VM_FAULT_SIGBUS;
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+static int nvmap_vma_fault_k414(struct vm_fault *vmf)
+{
+	struct vm_area_struct *vma = vmf->vma;
+	unsigned long vmf_address = vmf->address;
+
+	return nvmap_vma_fault(vma, vmf, vmf->address);
+
+}
+#else
+static int nvmap_vma_fault_k409(struct vm_area_struct *vma,
+					struct vm_fault *vmf)
+{
+
+	return nvmap_vma_fault(vma, vmf, (unsigned long) vmf->virtual_address);
+}
+#endif
+
+static bool nvmap_fixup_prot(struct vm_area_struct *vma,
+		unsigned long addr, pgoff_t pgoff)
+{
+	struct nvmap_vma_priv *priv;
+	unsigned long offs;
+
+	priv = vma->vm_private_data;
+	if (!priv || !priv->handle)
+		return false;
+
+	offs = pgoff << PAGE_SHIFT;
+	offs += priv->offs;
+
+	return nvmap_handle_fixup_prot_vma(priv->handle, offs);
+}
+
+struct vm_operations_struct nvmap_vma_ops = {
+	.open		= nvmap_vma_open,
+	.close		= nvmap_vma_close,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	.fault		= nvmap_vma_fault_k414,
+#else
+	.fault		= nvmap_vma_fault_k409,
+#endif
+	.fixup_prot	= nvmap_fixup_prot,
+};
+
+int is_nvmap_vma(struct vm_area_struct *vma)
+{
+	return vma->vm_ops == &nvmap_vma_ops;
+}
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_handle.c b/drivers/video/tegra/nvmap/nv2/nvmap_handle.c
new file mode 100644
index 000000000000..49027013a16c
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_handle.c
@@ -0,0 +1,1015 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_handle.c
+ *
+ * Handle allocation and freeing routines for nvmap
+ *
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"%s: " fmt, __func__
+
+#include <linux/err.h>
+#include <linux/io.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/mm.h>
+#include <linux/rbtree.h>
+#include <linux/dma-buf.h>
+#include <linux/moduleparam.h>
+#include <linux/nvmap.h>
+#include <linux/slab.h>
+#include <linux/vmalloc.h>
+#include <linux/version.h>
+#include <linux/uaccess.h>
+
+#include <soc/tegra/chip-id.h>
+
+#include <asm/pgtable.h>
+
+#include <trace/events/nvmap.h>
+
+#include "nvmap_stats.h"
+
+#include "nvmap_misc.h"
+#include "nvmap_cache.h"
+#include "nvmap_heap_alloc.h"
+#include "nvmap_carveout.h"
+#include "nvmap_carveout.h"
+#include "nvmap_dev.h"
+#include "nvmap_dmabuf.h"
+#include "nvmap_vma.h"
+#include "nvmap_tag.h"
+
+#include "nvmap_handle.h"
+#include "nvmap_handle_priv.h"
+
+// TODO: Add page coloring
+
+static void handle_add_to_dev(struct nvmap_handle *h, struct nvmap_device *dev);
+static int handle_remove_from_dev(struct nvmap_handle *h,
+						struct nvmap_device *dev);
+// TODO: Remove these global variables
+extern size_t cache_maint_inner_threshold;
+extern int nvmap_cache_maint_by_set_ways;
+extern struct static_key nvmap_disable_vaddr_for_cache_maint;
+
+static struct nvmap_handle_info *handle_create_info(struct nvmap_handle *handle)
+{
+
+	struct nvmap_handle_info *info = kzalloc(sizeof(*info), GFP_KERNEL);
+
+	if (!info) {
+		return ERR_PTR(-ENOMEM);
+	}
+
+	info->handle = handle;
+	INIT_LIST_HEAD(&info->maps);
+	mutex_init(&info->maps_lock);
+
+	return info;
+}
+
+struct dma_buf *nvmap_handle_to_dmabuf(struct nvmap_handle *handle)
+{
+	return handle->dmabuf;
+}
+
+void nvmap_handle_install_fd(struct nvmap_handle *handle, int fd)
+{
+	nvmap_dmabuf_install_fd(handle->dmabuf, fd);
+	// TODO: why is this get_dma_buf here?
+	get_dma_buf(handle->dmabuf);
+}
+
+void nvmap_handle_kmap_inc(struct nvmap_handle *h)
+{
+	atomic_inc(&h->kmap_count);
+}
+
+void nvmap_handle_kmap_dec(struct nvmap_handle *h)
+{
+	atomic_dec(&h->kmap_count);
+}
+
+void nvmap_handle_umap_inc(struct nvmap_handle *h)
+{
+	atomic_inc(&h->umap_count);
+}
+
+void nvmap_handle_umap_dec(struct nvmap_handle *h)
+{
+	atomic_dec(&h->umap_count);
+}
+
+size_t nvmap_handle_size(struct nvmap_handle *h)
+{
+	return h->size;
+}
+
+int nvmap_handle_is_allocated(struct nvmap_handle *h)
+{
+	return h->alloc;
+}
+
+size_t nvmap_handle_ivm_id(struct nvmap_handle *h)
+{
+	return h->ivm_id;
+}
+
+u32 nvmap_handle_heap_type(struct nvmap_handle *h)
+{
+	return h->heap_type;
+}
+
+u32 nvmap_handle_userflag(struct nvmap_handle *h)
+{
+	return h->userflags;
+}
+
+u32 nvmap_handle_flags(struct nvmap_handle *h)
+{
+	return h->flags;
+}
+
+
+bool nvmap_handle_is_heap(struct nvmap_handle *h)
+{
+	return h->heap_pgalloc;
+}
+
+bool nvmap_handle_track_dirty(struct nvmap_handle *h)
+{
+	if (!h->heap_pgalloc)
+		return false;
+
+	return h->userflags & (NVMAP_HANDLE_CACHE_SYNC |
+			       NVMAP_HANDLE_CACHE_SYNC_AT_RESERVE);
+}
+
+struct nvmap_handle *nvmap_handle_from_fd(int fd)
+{
+	struct nvmap_handle *handle = ERR_PTR(-EINVAL);
+	struct dma_buf *dmabuf;
+
+	dmabuf = nvmap_dmabuf_from_fd(fd);
+	if (IS_ERR(dmabuf))
+		return ERR_CAST(dmabuf);
+
+	handle = nvmap_dmabuf_to_handle(dmabuf);
+	if (IS_ERR(handle))
+		return ERR_CAST(handle);
+
+	return handle;
+}
+
+struct list_head *nvmap_handle_lru(struct nvmap_handle *h)
+{
+	return &h->lru;
+}
+
+atomic_t *nvmap_handle_pin(struct nvmap_handle *h)
+{
+	return &h->pin;
+}
+
+/*
+ * NOTE: this does not ensure the continued existence of the underlying
+ * dma_buf. If you want ensure the existence of the dma_buf you must get an
+ * nvmap_handle_ref as that is what tracks the dma_buf refs.
+ */
+struct nvmap_handle *nvmap_handle_get(struct nvmap_handle *h)
+{
+	if (WARN_ON(!virt_addr_valid(h))) {
+		pr_err("%s: invalid handle\n", current->group_leader->comm);
+		return NULL;
+	}
+
+	if (unlikely(atomic_inc_return(&h->ref) <= 1)) {
+		pr_err("%s: %s attempt to get a freed handle\n",
+			__func__, current->group_leader->comm);
+		atomic_dec(&h->ref);
+		return NULL;
+	}
+	return h;
+}
+
+void nvmap_handle_put(struct nvmap_handle *h)
+{
+	int cnt;
+
+	if (WARN_ON(!virt_addr_valid(h)))
+		return;
+	cnt = atomic_dec_return(&h->ref);
+
+	if (WARN_ON(cnt < 0)) {
+		pr_err("%s: %s put to negative references\n",
+			__func__, current->comm);
+	} else if (cnt == 0) {
+		nvmap_handle_destroy(h);
+	}
+}
+
+struct nvmap_handle *nvmap_handle_create(size_t size)
+{
+	void *err = ERR_PTR(-ENOMEM);
+	struct nvmap_handle *h;
+	struct nvmap_handle_info *info;
+
+	if (!size)
+		return ERR_PTR(-EINVAL);
+
+	h = kzalloc(sizeof(*h), GFP_KERNEL);
+	if (!h)
+		return ERR_PTR(-ENOMEM);
+
+	/* This reference of 1 is the reference the dmabuf has on the handle
+	 * It's removed when the dma_buf is released through
+	 *    nvmap_dmabuf_release
+	 */
+	atomic_set(&h->ref, 1);
+	atomic_set(&h->pin, 0);
+
+	h->orig_size = size;
+	h->size = PAGE_ALIGN(size);
+	h->flags = NVMAP_HANDLE_WRITE_COMBINE;
+	h->peer = NVMAP_IVM_INVALID_PEER;
+	mutex_init(&h->lock);
+	INIT_LIST_HEAD(&h->vmas);
+	INIT_LIST_HEAD(&h->lru);
+	INIT_LIST_HEAD(&h->dmabuf_priv);
+
+
+	info = handle_create_info(h);
+	if (IS_ERR(info)) {
+		err = info;
+		goto  make_info_fail;
+	}
+
+	h->dmabuf = nvmap_dmabuf_create(info, h->size);
+	if (IS_ERR_OR_NULL(h->dmabuf)) {
+		err = ERR_PTR(-ENOMEM);
+		goto make_dmabuf_fail;
+	}
+
+	handle_add_to_dev(h, nvmap_dev);
+
+	return h;
+
+make_dmabuf_fail:
+	kfree(info);
+make_info_fail:
+	kfree(h);
+	return err;
+}
+
+static void handle_pgalloc_free(struct nvmap_pgalloc *pgalloc, size_t size,
+				int from_va)
+{
+	int i;
+	unsigned int nr_page;
+	unsigned int page_index = 0;
+
+	nr_page = DIV_ROUND_UP(size, PAGE_SIZE);
+
+	BUG_ON(size & ~PAGE_MASK);
+	BUG_ON(!pgalloc->pages);
+
+	for (i = 0; i < nr_page; i++)
+		pgalloc->pages[i] = nvmap_to_page(pgalloc->pages[i]);
+
+#ifdef CONFIG_NVMAP_PAGE_POOLS
+	if (!from_va)
+		page_index = nvmap_page_pool_fill_lots(&nvmap_dev->pool,
+					pgalloc->pages, nr_page);
+#endif
+
+	for (i = page_index; i < nr_page; i++) {
+		if (from_va)
+			put_page(pgalloc->pages[i]);
+		else
+			__free_page(pgalloc->pages[i]);
+	}
+
+	nvmap_altfree(pgalloc->pages, nr_page * sizeof(struct page *));
+}
+
+static void handle_dealloc(struct nvmap_handle *h)
+{
+	if (!h->alloc)
+		return;
+
+	nvmap_stats_inc(NS_RELEASE, h->size);
+	nvmap_stats_dec(NS_TOTAL, h->size);
+	if (!h->heap_pgalloc) {
+		if (h->vaddr) {
+			struct vm_struct *vm;
+			void *addr = h->vaddr;
+
+			addr -= (h->carveout->base & ~PAGE_MASK);
+			vm = find_vm_area(addr);
+			BUG_ON(!vm);
+			free_vm_area(vm);
+		}
+
+		nvmap_heap_free(h->carveout);
+		nvmap_handle_kmap_dec(h);
+		h->vaddr = NULL;
+		return;
+	} else if (nvmap_heap_type_is_dma(h->heap_type)){
+		nvmap_heap_dealloc_dma_pages(h->size, h->heap_type,
+						h->pgalloc.pages);
+	} else {
+		if (h->vaddr) {
+			nvmap_handle_kmap_dec(h);
+
+			vm_unmap_ram(h->vaddr, h->size >> PAGE_SHIFT);
+			h->vaddr = NULL;
+		}
+
+		handle_pgalloc_free(&h->pgalloc, h->size, h->from_va);
+	}
+}
+
+/* adds a newly-created handle to the device master tree */
+static void handle_add_to_dev(struct nvmap_handle *h, struct nvmap_device *dev)
+{
+	struct rb_node **p;
+	struct rb_node *parent = NULL;
+
+	spin_lock(&dev->handle_lock);
+	p = &dev->handles.rb_node;
+	while (*p) {
+		struct nvmap_handle *b;
+
+		parent = *p;
+		b = rb_entry(parent, struct nvmap_handle, node);
+		if (h > b)
+			p = &parent->rb_right;
+		else
+			p = &parent->rb_left;
+	}
+	rb_link_node(&h->node, parent, p);
+	rb_insert_color(&h->node, &dev->handles);
+	nvmap_lru_add(&h->lru);
+	spin_unlock(&dev->handle_lock);
+}
+
+/* remove a handle from the device's tree of all handles; called
+ * when freeing handles. */
+static int handle_remove_from_dev(struct nvmap_handle *h,
+						struct nvmap_device *dev)
+{
+	spin_lock(&dev->handle_lock);
+
+	/* re-test inside the spinlock if the handle really has no clients;
+	 * only remove the handle if it is unreferenced */
+	if (atomic_add_return(0, &h->ref) > 0) {
+		spin_unlock(&dev->handle_lock);
+		return -EBUSY;
+	}
+	smp_rmb();
+	BUG_ON(atomic_read(&h->ref) < 0);
+	BUG_ON(atomic_read(&h->pin) != 0);
+
+	nvmap_lru_del(&h->lru);
+	rb_erase(&h->node, &dev->handles);
+
+	spin_unlock(&dev->handle_lock);
+	return 0;
+}
+
+void nvmap_handle_add_owner(struct nvmap_handle *handle,
+					struct nvmap_client *client)
+{
+	if (!handle->owner)
+		handle->owner = client;
+}
+
+void nvmap_handle_destroy(struct nvmap_handle *h)
+{
+	nvmap_dmabufs_free(&h->dmabuf_priv);
+
+	if (handle_remove_from_dev(h, nvmap_dev) != 0)
+		return;
+
+	handle_dealloc(h);
+
+	NVMAP_TAG_TRACE(trace_nvmap_destroy_handle,
+		NULL, get_current()->pid, 0, NVMAP_TP_ARGS_H(h));
+	kfree(h);
+}
+
+static void heap_alloc_and_set_handle(
+			 struct nvmap_handle *h, unsigned int orig_heap)
+{
+	unsigned int heap_type;
+	struct page **pages;
+
+	BUG_ON(orig_heap & (orig_heap - 1));
+
+	heap_type = nvmap_heap_type_conversion(orig_heap);
+
+	if (nvmap_heap_type_is_carveout(heap_type)) {
+		int err;
+
+		err = nvmap_handle_alloc_carveout(h, heap_type, NULL);
+		if (!err) {
+			h->heap_type = heap_type;
+			h->heap_pgalloc = false;
+			goto success;
+		}
+
+		pages = nvmap_heap_alloc_dma_pages(h->size, heap_type);
+		if (IS_ERR_OR_NULL(pages))
+			return;
+		h->pgalloc.pages = pages;
+		h->pgalloc.contig = 0;
+
+		atomic_set(&h->pgalloc.ndirty, 0);
+		h->heap_type = NVMAP_HEAP_CARVEOUT_VPR;
+		h->heap_pgalloc = true;
+	} else if (nvmap_heap_type_is_iovmm(heap_type)) {
+		pages = nvmap_heap_alloc_iovmm_pages(h->size,
+			h->userflags & NVMAP_HANDLE_PHYS_CONTIG);
+		if (IS_ERR_OR_NULL(pages))
+			return;
+
+		h->pgalloc.pages = pages;
+		h->pgalloc.contig =
+			h->userflags & NVMAP_HANDLE_PHYS_CONTIG;
+		atomic_set(&h->pgalloc.ndirty, 0);
+		h->heap_type = NVMAP_HEAP_IOVMM;
+		h->heap_pgalloc = true;
+	}
+
+success:
+	/* barrier to ensure all handle alloc data
+	 * is visible before alloc is seen by other
+	 * processors.
+	 */
+	mb();
+	h->alloc = true;
+	return;
+}
+
+/* TODO: Change this to return an alloc_handle or something
+ *       stop this and the next function from editing handle,
+ *       and push the functions back into heap_alloc
+ */
+static void heap_alloc_handle_from_heaps(
+					struct nvmap_handle *handle,
+					const unsigned int *alloc_policy,
+					unsigned int heap_mask)
+{
+	while (!handle->alloc && *alloc_policy) {
+		unsigned int heap_type;
+
+		heap_type = *alloc_policy++;
+		heap_type &= heap_mask;
+
+		if (!heap_type)
+			continue;
+
+		heap_mask &= ~heap_type;
+
+		while (heap_type && !handle->alloc) {
+			unsigned int heap;
+
+			/* iterate possible heaps MSB-to-LSB, since higher-
+			 * priority carveouts will have higher usage masks */
+			heap = 1 << __fls(heap_type);
+			heap_alloc_and_set_handle(handle, heap);
+			heap_type &= ~heap;
+		}
+	}
+
+}
+
+int nvmap_handle_alloc(
+		       struct nvmap_handle *h, unsigned int heap_mask,
+		       size_t align,
+		       u8 kind,
+		       unsigned int flags,
+		       int peer)
+{
+	const unsigned int *alloc_policy;
+	int nr_page;
+	int err = -ENOMEM;
+
+	if (!h)
+		return -EINVAL;
+
+	if (h->alloc) {
+		return -EEXIST;
+	}
+
+	h->userflags = flags;
+	h->peer = NVMAP_IVM_INVALID_PEER;
+
+	nr_page = ((h->size + PAGE_SIZE - 1) >> PAGE_SHIFT);
+	/* Force mapping to uncached for VPR memory. */
+	if (heap_mask & (NVMAP_HEAP_CARVEOUT_VPR | ~nvmap_dev->cpu_access_mask))
+		h->flags = NVMAP_HANDLE_UNCACHEABLE;
+	else
+		h->flags = (flags & NVMAP_HANDLE_CACHE_FLAG);
+	h->align = max_t(size_t, align, L1_CACHE_BYTES);
+
+	alloc_policy = nvmap_heap_mask_to_policy(heap_mask, nr_page);
+	if (!alloc_policy) {
+		err = -EINVAL;
+		goto out;
+	}
+
+	heap_alloc_handle_from_heaps(h, alloc_policy, heap_mask);
+
+out:
+	return err;
+}
+
+struct nvmap_handle *nvmap_handle_from_ivmid(u64 ivm_id)
+{
+	struct nvmap_handle *handle = NULL;
+	struct rb_node *n;
+
+	spin_lock(&nvmap_dev->handle_lock);
+
+	n = nvmap_dev->handles.rb_node;
+	for (n = rb_first(&nvmap_dev->handles); n; n = rb_next(n)) {
+		handle = rb_entry(n, struct nvmap_handle, node);
+		if (handle->ivm_id == ivm_id) {
+			BUG_ON(!virt_addr_valid(handle));
+
+			spin_unlock(&nvmap_dev->handle_lock);
+			return handle;
+		}
+	}
+
+	spin_unlock(&nvmap_dev->handle_lock);
+	return NULL;
+}
+
+int nvmap_handle_alloc_from_va(struct nvmap_handle *h,
+			       ulong addr,
+			       unsigned int flags)
+{
+	h = nvmap_handle_get(h);
+	if (!h)
+		return -EINVAL;
+
+	if (h->alloc) {
+		nvmap_handle_put(h);
+		return -EEXIST;
+	}
+
+	h->userflags = flags;
+	h->flags = (flags & NVMAP_HANDLE_CACHE_FLAG);
+	h->align = PAGE_SIZE;
+
+	h->pgalloc.pages = nvmap_heap_alloc_from_va(h->size, addr);
+	if (!h->pgalloc.pages) {
+		nvmap_handle_put(h);
+		return -ENOMEM;
+	}
+
+	atomic_set(&h->pgalloc.ndirty, 0);
+	h->heap_type = NVMAP_HEAP_IOVMM;
+	h->heap_pgalloc = true;
+	h->from_va = true;
+
+	mb();
+	h->alloc = true;
+
+	nvmap_handle_put(h);
+	return 0;
+}
+
+int nvmap_handle_alloc_carveout(struct nvmap_handle *handle,
+					      unsigned long type,
+					      phys_addr_t *start)
+{
+	struct nvmap_carveout_node *co_heap;
+	struct nvmap_device *dev = nvmap_dev;
+	int i;
+
+	for (i = 0; i < dev->nr_carveouts; i++) {
+		struct nvmap_heap_block *block;
+		co_heap = nvmap_dev_to_carveout(dev, i);
+
+		if (!(nvmap_carveout_heap_bit(co_heap) & type))
+			continue;
+
+		if (type & NVMAP_HEAP_CARVEOUT_IVM)
+			handle->size = ALIGN(handle->size, NVMAP_IVM_ALIGNMENT);
+
+		block = nvmap_carveout_alloc(co_heap, start,
+						handle->size,
+						handle->align,
+						handle->flags,
+						handle->peer);
+		if (block) {
+			handle->carveout = block;
+			handle->ivm_id = nvmap_carveout_ivm(co_heap, block,
+								handle->size);
+			return 0;
+		}
+	}
+	return -1;
+
+}
+
+int nvmap_handle_alloc_from_ivmid(struct nvmap_handle *handle, u64 ivm_id)
+{
+	phys_addr_t offs = nvmap_ivmid_to_offset(ivm_id);
+	int peer = nvmap_ivmid_to_peer(ivm_id);
+	int err;
+
+	handle->peer = peer;
+
+	err = nvmap_handle_alloc_carveout(handle, NVMAP_HEAP_CARVEOUT_IVM,
+								&offs);
+	if (err) {
+		return -1;
+	}
+
+	handle->heap_type = NVMAP_HEAP_CARVEOUT_IVM;
+	handle->heap_pgalloc = false;
+	handle->ivm_id = ivm_id;
+
+	mb();
+	handle->alloc = true;
+
+	return 0;
+}
+
+void nvmap_handle_zap(struct nvmap_handle *handle, u64 offset, u64 size)
+{
+	if (!handle->heap_pgalloc)
+		return;
+
+	/* if no dirty page is present, no need to zap */
+	if (nvmap_handle_track_dirty(handle)
+			&& !atomic_read(&handle->pgalloc.ndirty))
+		return;
+
+	if (!size) {
+		offset = 0;
+		size = handle->size;
+	}
+
+	size = PAGE_ALIGN((offset & ~PAGE_MASK) + size);
+
+	mutex_lock(&handle->lock);
+	nvmap_vma_zap(&handle->vmas, offset, size);
+	mutex_unlock(&handle->lock);
+}
+
+static int handle_cache_maint_heap_page_inner(struct nvmap_handle *handle,
+				unsigned int op,
+				unsigned long start, unsigned long end)
+{
+	if (static_key_false(&nvmap_disable_vaddr_for_cache_maint))
+		return 0;
+
+	if (!handle->vaddr) {
+		/* TODO: We need better naming than mapping and then unmapping */
+		if (nvmap_handle_mmap(handle))
+			nvmap_handle_munmap(handle, handle->vaddr);
+		else
+			return 1;
+	}
+	/* Fast inner cache maintenance using single mapping */
+	nvmap_cache_maint_inner(op, handle->vaddr + start, end - start);
+	return 0;
+}
+
+void nvmap_handle_cache_maint_heap_page(struct nvmap_handle *handle,
+				unsigned long op,
+				unsigned long start, unsigned long end,
+				int outer)
+{
+	int err;
+
+	if (handle->userflags & NVMAP_HANDLE_CACHE_SYNC) {
+		/*
+		 * zap user VA->PA mappings so that any access to the pages
+		 * will result in a fault and can be marked dirty
+		 */
+		nvmap_handle_mkclean(handle, start, end-start);
+		nvmap_handle_zap(handle, start, end - start);
+	}
+
+	err = handle_cache_maint_heap_page_inner(handle, op, start, end);
+	if (err && outer) {
+		nvmap_cache_maint_heap_page_outer(handle->pgalloc.pages, op,
+							start, end);
+	}
+}
+
+/*
+ * This is the equivalent of __nvmap_do_cache_maint
+ * The clean_only_dirty flag has been removed because it is always passed as
+ * false
+ */
+int nvmap_handle_cache_maint(struct nvmap_handle *handle, unsigned long start,
+		unsigned long end, unsigned int op)
+{
+	int ret = 0;
+
+	if ((start >= handle->size) || (end > handle->size)) {
+		return -EFAULT;
+	}
+
+	if (!(handle->heap_type & nvmap_dev->cpu_access_mask)) {
+		return -EPERM;
+	}
+
+	if (!handle || !handle->alloc)
+		return -EFAULT;
+
+	nvmap_handle_kmap_inc(handle);
+
+	if (op == NVMAP_CACHE_OP_INV)
+		op = NVMAP_CACHE_OP_WB_INV;
+
+	if (!end)
+		end = handle->size;
+
+	wmb();
+	if (handle->flags == NVMAP_HANDLE_UNCACHEABLE ||
+	    handle->flags == NVMAP_HANDLE_WRITE_COMBINE || start == end)
+		goto out;
+
+	if (start > handle->size || end > handle->size) {
+		pr_warn("cache maintenance outside handle\n");
+		ret = -EINVAL;
+		goto out;
+	}
+
+	if (nvmap_cache_can_fast_maint(start, end, op)) {
+		if (handle->userflags & NVMAP_HANDLE_CACHE_SYNC) {
+			nvmap_handle_mkclean(handle, 0, handle->size);
+			nvmap_handle_zap(handle, 0, handle->size);
+		}
+		nvmap_cache_fast_maint(op);
+	} else if (handle->heap_pgalloc) {
+		nvmap_handle_cache_maint_heap_page(handle, op, start, end,
+			(handle->flags != NVMAP_HANDLE_INNER_CACHEABLE));
+	} else {
+		ret = nvmap_cache_maint_phys_range(op,
+				start + handle->carveout->base,
+				end   + handle->carveout->base);
+	}
+
+out:
+	/* TODO: Add more stats counting here */
+	nvmap_stats_inc(NS_CFLUSH_RQ, end - start);
+	nvmap_handle_kmap_dec(handle);
+	return ret;
+
+}
+
+static void cache_maint_large(struct nvmap_handle **handles, u64 total,
+					u64 thresh, int op, int nr)
+{
+	int i;
+
+	for (i = 0; i < nr; i++) {
+		if (handles[i]->userflags & NVMAP_HANDLE_CACHE_SYNC) {
+			nvmap_handle_mkclean(handles[i], 0, handles[i]->size);
+			nvmap_handle_zap(handles[i], 0, handles[i]->size);
+		}
+	}
+
+	if (op == NVMAP_CACHE_OP_WB)
+		nvmap_cache_inner_clean_all();
+	else
+		nvmap_cache_inner_flush_all();
+
+	nvmap_stats_inc(NS_CFLUSH_RQ, total);
+	nvmap_stats_inc(NS_CFLUSH_DONE, thresh);
+	trace_nvmap_cache_flush(total,
+			nvmap_stats_read(NS_ALLOC),
+			nvmap_stats_read(NS_CFLUSH_RQ),
+			nvmap_stats_read(NS_CFLUSH_DONE));
+}
+
+static int handles_get_total_cache_size(struct nvmap_handle **handles,
+			u64 *sizes, int op, int nr)
+{
+	int i;
+	int total = 0;
+
+	for (i = 0; i < nr; i++) {
+		bool inner, outer;
+
+		nvmap_handle_get_cacheability(handles[i], &inner, &outer);
+
+		if (!inner && !outer)
+			continue;
+
+		if ((op == NVMAP_CACHE_OP_WB)
+				&& nvmap_handle_track_dirty(handles[i])) {
+			/* TODO: shouldn't this be shifted by page size? */
+			total += atomic_read(&handles[i]->pgalloc.ndirty);
+		} else {
+			total += sizes[i];
+		}
+	}
+
+	return total;
+}
+
+/*
+ * Perform cache op on the list of memory regions within passed handles.
+ * A memory region within handle[i] is identified by offsets[i], sizes[i]
+ *
+ * This will optimze the op if it can.
+ * In the case that all the handles together are larger than the inner cache
+ * maint threshold it is possible to just do an entire inner cache flush.
+ *
+ * NOTE: this omits outer cache operations which is fine for ARM64
+ */
+int nvmap_handles_cache_maint(struct nvmap_handle **handles,
+				u64 *offsets, u64 *sizes, int op, int nr)
+{
+	int i;
+	int err;
+	u64 total = 0;
+	u64 thresh = ~0;
+
+	/*
+	 * As io-coherency is enabled by default from T194 onwards,
+	 * Don't do cache maint from CPU side. The HW, SCF will do.
+	 */
+	if (tegra_get_chip_id() == TEGRA194)
+		return 0;
+
+	WARN(!IS_ENABLED(CONFIG_ARM64),
+		"cache list operation may not function properly");
+
+	if (nvmap_cache_maint_by_set_ways)
+		thresh = cache_maint_inner_threshold;
+
+	total = handles_get_total_cache_size(handles, sizes, op, nr);
+	if (!total)
+		return 0;
+
+	/* Full flush in the case the passed list is bigger than our
+	 * threshold. */
+	if (total >= thresh) {
+		cache_maint_large(handles, total, thresh, op, nr);
+	} else {
+		for (i = 0; i < nr; i++) {
+			err = nvmap_handle_cache_maint(handles[i],
+							offsets[i],
+							offsets[i] + sizes[i],
+							op);
+			if (err) {
+				pr_err("cache maint per handle failed [%d]\n",
+						err);
+				return err;
+			}
+		}
+	}
+
+	return 0;
+}
+
+static int handle_read(struct nvmap_handle *h, unsigned long h_offs,
+			 unsigned long sys_addr, void *addr,
+			 unsigned long elem_size)
+{
+	if (!(h->userflags & NVMAP_HANDLE_CACHE_SYNC_AT_RESERVE)) {
+		nvmap_handle_cache_maint(h, h_offs, h_offs + elem_size,
+							NVMAP_CACHE_OP_INV);
+	}
+
+	return copy_to_user((void *)sys_addr, addr, elem_size);
+}
+
+static int handle_write(struct nvmap_handle *h, unsigned long h_offs,
+			 unsigned long sys_addr, void *addr,
+			 unsigned long elem_size)
+{
+	int ret = 0;
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 9, 0)
+	if (h->heap_type == NVMAP_HEAP_CARVEOUT_VPR) {
+		uaccess_enable();
+		memcpy_toio(addr, (void *)sys_addr, elem_size);
+		uaccess_disable();
+		ret = 0;
+	} else {
+		ret = copy_from_user(addr, (void *)sys_addr, elem_size);
+	}
+#else
+	ret = copy_from_user(addr, (void *)sys_addr, elem_size);
+#endif
+
+	if (ret)
+		return ret;
+
+	if (!(h->userflags & NVMAP_HANDLE_CACHE_SYNC_AT_RESERVE))
+		nvmap_handle_cache_maint(h, h_offs, h_offs + elem_size,
+						NVMAP_CACHE_OP_WB_INV);
+
+	return ret;
+}
+
+ssize_t nvmap_handle_rw(struct nvmap_handle *h,
+			 unsigned long h_offs, unsigned long h_stride,
+			 unsigned long sys_addr, unsigned long sys_stride,
+			 unsigned long elem_size, unsigned long count,
+			 int is_read)
+{
+	ssize_t copied = 0;
+	void *addr;
+	int ret = 0;
+
+	if (!(h->heap_type & nvmap_dev->cpu_access_mask))
+		return -EPERM;
+
+	if (!elem_size || !count)
+		return -EINVAL;
+
+	if (!h->alloc)
+		return -EFAULT;
+
+	/*
+	 * TODO: Add an english description of this
+	 * I think it is:
+	 * If all of the strides are equal, and the offset is byte aligned,
+	 * then do all of the copying in one go
+	 */
+	if (elem_size == h_stride && elem_size == sys_stride && (h_offs % 8 == 0)) {
+		elem_size *= count;
+		h_stride = elem_size;
+		sys_stride = elem_size;
+		count = 1;
+	}
+
+	if (elem_size > sys_stride || elem_size > h_stride)
+		return -EINVAL;
+
+	if (elem_size > h->size ||
+			h_offs >= h->size ||
+			h_stride * (count - 1) + elem_size > (h->size - h_offs) ||
+			sys_stride * count > (h->size - h_offs))
+		return -EINVAL;
+
+	if (!h->vaddr) {
+		if (nvmap_handle_mmap(h) == NULL)
+			return -ENOMEM;
+		nvmap_handle_munmap(h, h->vaddr);
+	}
+
+	addr = h->vaddr + h_offs;
+
+	while (count--) {
+		if (h_offs + elem_size > h->size) {
+			pr_warn("read/write outside of handle\n");
+			ret = -EFAULT;
+			break;
+		}
+
+		if (is_read) {
+			ret = handle_read(h, h_offs,
+					sys_addr, addr, elem_size);
+		} else {
+			ret = handle_write(h, h_offs,
+					sys_addr, addr, elem_size);
+		}
+
+		if (ret)
+			break;
+
+		copied += elem_size;
+		sys_addr += sys_stride;
+		h_offs += h_stride;
+		addr += h_stride;
+	}
+
+	if (ret)
+		return ret;
+
+	return copied;
+}
+
+struct nvmap_handle *nvmap_handle_from_node(struct rb_node *n)
+{
+	return rb_entry(n, struct nvmap_handle, node);
+}
+
+struct nvmap_handle *nvmap_handle_from_lru(struct list_head *n)
+{
+	return list_entry(n, struct nvmap_handle, lru);
+}
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_handle.h b/drivers/video/tegra/nvmap/nv2/nvmap_handle.h
new file mode 100644
index 000000000000..c5ee6214ad7f
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_handle.h
@@ -0,0 +1,175 @@
+/*
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __NVMAP_HANDLE_H
+#define __NVMAP_HANDLE_H
+
+#include <linux/dma-buf.h>
+#include "nvmap_structs.h"
+
+struct nvmap_handle;
+
+enum NVMAP_PROT_OP {
+	NVMAP_HANDLE_PROT_NONE = 1,
+	NVMAP_HANDLE_PROT_RESTORE = 2,
+};
+
+struct nvmap_handle *nvmap_handle_create(size_t size);
+struct nvmap_handle *nvmap_handle_create_from_dmabuf(
+			struct nvmap_client * client, struct dma_buf *dmabuf);
+void nvmap_handle_add_owner(struct nvmap_handle *handle,
+					struct nvmap_client *client);
+void nvmap_handle_destroy(struct nvmap_handle *handle);
+void nvmap_handle_install_fd(struct nvmap_handle *handle, int fd);
+
+struct nvmap_handle *nvmap_handle_get(struct nvmap_handle *h);
+void nvmap_handle_put(struct nvmap_handle *h);
+
+int nvmap_handle_alloc(struct nvmap_handle *h,
+				unsigned int heap_mask,
+				size_t align,
+				u8 kind,
+				unsigned int flags,
+				int peer);
+
+int nvmap_handle_alloc_from_ivmid(struct nvmap_handle *handle, u64 ivm_id);
+
+int nvmap_handle_alloc_carveout(struct nvmap_handle *handle,
+					      unsigned long type,
+					      phys_addr_t *start);
+int nvmap_handle_alloc_from_va(struct nvmap_handle *h,
+			       ulong addr,
+			       unsigned int flags);
+int nvmap_handle_alloc_from_ivmid(struct nvmap_handle *handle, u64 ivm_id);
+
+struct nvmap_handle *nvmap_handle_from_fd(int fd);
+struct nvmap_handle *nvmap_handle_from_ivmid(u64 ivm_id);
+
+int nvmap_handle_cache_maint(struct nvmap_handle *handle, unsigned long start,
+		unsigned long end, unsigned int op);
+int nvmap_handles_cache_maint(struct nvmap_handle **handles,
+				u64 *offsets, u64 *sizes, int op, int nr);
+void nvmap_handle_zap(struct nvmap_handle *handle, u64 offset, u64 size);
+
+void *nvmap_handle_mmap(struct nvmap_handle *h);
+void nvmap_handle_munmap(struct nvmap_handle *h, void *addr);
+
+int nvmap_handles_reserve(struct nvmap_handle **handles, u64 *offsets,
+						u64 *sizes, int op, int nr);
+ssize_t nvmap_handle_rw(struct nvmap_handle *h,
+			 unsigned long h_offs, unsigned long h_stride,
+			 unsigned long sys_addr, unsigned long sys_stride,
+			 unsigned long elem_size, unsigned long count,
+			 int is_read);
+
+int nvmap_handle_owns_vma(struct nvmap_handle *h, struct vm_area_struct *vma);
+int nvmap_handle_add_vma(struct nvmap_handle *handle,
+					struct vm_area_struct *vma);
+int nvmap_handle_del_vma(struct nvmap_handle *handle,
+					struct vm_area_struct *vma);
+int nvmap_handle_open_vma(struct nvmap_handle *handle);
+int nvmap_handle_close_vma(struct nvmap_handle *handle);
+int nvmap_handle_fault_vma(struct nvmap_handle *handle,
+		unsigned long offs, struct page **page_ptr);
+bool nvmap_handle_fixup_prot_vma(struct nvmap_handle *handle,
+					unsigned long offs);
+
+struct dma_buf *nvmap_handle_to_dmabuf(struct nvmap_handle *handle);
+
+pgprot_t nvmap_handle_pgprot(struct nvmap_handle *handle, pgprot_t prot);
+
+void nvmap_handle_kmap_inc(struct nvmap_handle *h);
+void nvmap_handle_kmap_dec(struct nvmap_handle *h);
+void nvmap_handle_umap_inc(struct nvmap_handle *h);
+void nvmap_handle_umap_dec(struct nvmap_handle *h);
+
+size_t nvmap_handle_size(struct nvmap_handle *handle);
+
+int nvmap_handle_is_allocated(struct nvmap_handle *h);
+size_t nvmap_handle_ivm_id(struct nvmap_handle *h);
+u32 nvmap_handle_heap_type(struct nvmap_handle *h);
+// TODO: What is difference between userflags and flags?
+u32 nvmap_handle_userflag(struct nvmap_handle *h);
+u32 nvmap_handle_flags(struct nvmap_handle *h);
+
+bool nvmap_handle_is_heap(struct nvmap_handle *h);
+bool nvmap_handle_track_dirty(struct nvmap_handle *h);
+pgprot_t nvmap_handle_pgprot(struct nvmap_handle *h, pgprot_t prot);
+
+// TODO Remove these, only needed by dmabuf_ops
+struct list_head *nvmap_handle_lru(struct nvmap_handle *h);
+atomic_t *nvmap_handle_pin(struct nvmap_handle *h);
+
+// TODO: Rename these
+void *__nvmap_kmap(struct nvmap_handle *h, unsigned int pagenum);
+void __nvmap_kunmap(struct nvmap_handle *h, unsigned int pagenum,
+		  void *addr);
+void *__nvmap_mmap(struct nvmap_handle *h);
+void __nvmap_munmap(struct nvmap_handle *h, void *addr);
+struct sg_table *__nvmap_sg_table(struct nvmap_client *client,
+		struct nvmap_handle *h);
+void __nvmap_free_sg_table(struct nvmap_client *client,
+		struct nvmap_handle *h, struct sg_table *sgt);
+
+void nvmap_handle_stringify(struct nvmap_handle *handle,
+				  struct seq_file *s, u32 heap_type,
+				  int ref_dupes);
+void nvmap_handle_maps_stringify(struct nvmap_handle *handle,
+				  struct seq_file *s, u32 heap_type,
+				  pid_t client_pid);
+int nvmap_handle_pid_show(struct nvmap_handle *handle, struct seq_file *s,
+					pid_t client_pid);
+
+void nvmap_handle_all_allocations_show(struct nvmap_handle *handle,
+				  struct seq_file *s, u32 heap_type);
+void nvmap_handle_orphans_allocations_show(struct nvmap_handle *handle,
+				  struct seq_file *s, u32 heap_type);
+
+u64 nvmap_handle_share_size(struct nvmap_handle *handle, u32 heap_type);
+u64 nvmap_handle_total_mss(struct nvmap_handle *h, u32 heap_type);
+u64 nvmap_handle_total_pss(struct nvmap_handle *h, u32 heap_type);
+
+int nvmap_handle_is_migratable(struct nvmap_handle *h);
+void nvmap_handle_lru_show(struct nvmap_handle *h, struct seq_file *s);
+struct nvmap_handle *nvmap_handle_from_node(struct rb_node *n);
+
+struct nvmap_handle *nvmap_handle_from_lru(struct list_head *n);
+
+#include "nvmap_dev.h"
+
+static inline void nvmap_lru_add(struct list_head *handle_lru)
+{
+	spin_lock(&nvmap_dev->lru_lock);
+	BUG_ON(!list_empty(handle_lru));
+	list_add_tail(handle_lru, &nvmap_dev->lru_handles);
+	spin_unlock(&nvmap_dev->lru_lock);
+}
+
+static inline void nvmap_lru_del(struct list_head *handle_lru)
+{
+	spin_lock(&nvmap_dev->lru_lock);
+	list_del(handle_lru);
+	INIT_LIST_HEAD(handle_lru);
+	spin_unlock(&nvmap_dev->lru_lock);
+}
+
+static inline void nvmap_lru_reset(struct list_head *handle_lru)
+{
+	spin_lock(&nvmap_dev->lru_lock);
+	BUG_ON(list_empty(handle_lru));
+	list_del(handle_lru);
+	list_add_tail(handle_lru, &nvmap_dev->lru_handles);
+	spin_unlock(&nvmap_dev->lru_lock);
+}
+
+#endif /* __NVMAP_HANDLE_H */
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_handle_map.c b/drivers/video/tegra/nvmap/nv2/nvmap_handle_map.c
new file mode 100644
index 000000000000..23f2dc722923
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_handle_map.c
@@ -0,0 +1,156 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap.c
+ *
+ * Memory manager for Tegra GPU
+ *
+ * Copyright (c) 2009-2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"nvmap: %s() " fmt, __func__
+
+#include <linux/err.h>
+#include <linux/io.h>
+#include <linux/vmalloc.h>
+#include <linux/nvmap.h>
+
+#include <trace/events/nvmap.h>
+
+#include <asm/pgtable.h>
+
+#include "nvmap_handle.h"
+#include "nvmap_handle_priv.h"
+#include "nvmap_client.h"
+#include "nvmap_dev.h"
+#include "nvmap_misc.h"
+#include "nvmap_carveout.h"
+
+pgprot_t nvmap_handle_pgprot(struct nvmap_handle *h, pgprot_t prot)
+{
+	if (h->flags == NVMAP_HANDLE_UNCACHEABLE) {
+#ifdef CONFIG_ARM64
+		nvmap_client_warn_if_bad_heap(h->owner,
+						h->heap_type, h->userflags);
+#endif
+		return pgprot_noncached(prot);
+	}
+	else if (h->flags == NVMAP_HANDLE_WRITE_COMBINE)
+		return pgprot_writecombine(prot);
+	return prot;
+}
+
+static void *handle_pgalloc_mmap(struct nvmap_handle *h)
+{
+	struct page **pages;
+	void *vaddr;
+
+	pgprot_t prot = nvmap_handle_pgprot(h, PG_PROT_KERNEL);
+
+	pages = nvmap_alloc_pages(h->pgalloc.pages, h->size >> PAGE_SHIFT);
+	if (!pages)
+		return NULL;
+
+	vaddr = vm_map_ram(pages, h->size >> PAGE_SHIFT, -1, prot);
+	nvmap_altfree(pages, (h->size >> PAGE_SHIFT) * sizeof(*pages));
+	if (!vaddr && !h->vaddr)
+		return NULL;
+
+	if (vaddr && atomic_long_cmpxchg(&h->vaddr, 0, (long)vaddr)) {
+		vm_unmap_ram(vaddr, h->size >> PAGE_SHIFT);
+	}
+	return h->vaddr;
+}
+
+static void *handle_carveout_mmap(struct nvmap_handle *h)
+{
+	pgprot_t prot = nvmap_handle_pgprot(h, PG_PROT_KERNEL);
+	unsigned long adj_size;
+	struct vm_struct *v;
+	void *vaddr;
+	phys_addr_t co_base = h->carveout->base;
+
+	/* carveout - explicitly map the pfns into a vmalloc area */
+	// TODO: This could probably be pushed into carveout
+	adj_size = co_base & ~PAGE_MASK;
+	adj_size += h->size;
+	adj_size = PAGE_ALIGN(adj_size);
+
+	v = alloc_vm_area(adj_size, NULL);
+	if (!v)
+		return NULL;
+
+	vaddr = v->addr + (co_base & ~PAGE_MASK);
+	ioremap_page_range((ulong)v->addr, (ulong)v->addr + adj_size,
+			co_base & PAGE_MASK, prot);
+
+	if (vaddr && atomic_long_cmpxchg(&h->vaddr, 0, (long)vaddr)) {
+		struct vm_struct *vm;
+
+		vaddr -= (co_base & ~PAGE_MASK);
+		vm = find_vm_area(vaddr);
+		BUG_ON(!vm);
+		free_vm_area(vm);
+	}
+
+	return h->vaddr;
+}
+
+void *nvmap_handle_mmap(struct nvmap_handle *h)
+{
+	void *vaddr;
+
+	if (!virt_addr_valid(h))
+		return NULL;
+
+	h = nvmap_handle_get(h);
+	if (!h)
+		return NULL;
+
+	if (!h->alloc)
+		goto put_handle;
+
+	if (!(h->heap_type & nvmap_dev->cpu_access_mask))
+		goto put_handle;
+
+	if (h->vaddr)
+		return h->vaddr;
+
+	nvmap_handle_kmap_inc(h);
+
+	if (h->heap_pgalloc) {
+		vaddr = handle_pgalloc_mmap(h);
+	} else {
+		vaddr = handle_carveout_mmap(h);
+	}
+
+	/* leave the handle ref count incremented by 1, so that
+	 * the handle will not be freed while the kernel mapping exists.
+	 * nvmap_handle_put will be called by unmapping this address */
+	if (vaddr)
+		return vaddr;
+
+	/* If we fail to map then set kmaps back to original value */
+	nvmap_handle_kmap_dec(h);
+put_handle:
+	nvmap_handle_put(h);
+	return NULL;
+}
+
+void nvmap_handle_munmap(struct nvmap_handle *h, void *addr)
+{
+	if (!h || !h->alloc ||
+	    WARN_ON(!virt_addr_valid(h)) ||
+	    WARN_ON(!addr) ||
+	    !(h->heap_type & nvmap_dev->cpu_access_mask))
+		return;
+
+	nvmap_handle_put(h);
+}
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_handle_map_ops.c b/drivers/video/tegra/nvmap/nv2/nvmap_handle_map_ops.c
new file mode 100644
index 000000000000..4f6f2c257082
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_handle_map_ops.c
@@ -0,0 +1,293 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap.c
+ *
+ * Memory manager for Tegra GPU
+ *
+ * Copyright (c) 2009-2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"nvmap: %s() " fmt, __func__
+
+#include <linux/err.h>
+#include <linux/highmem.h>
+#include <linux/io.h>
+#include <linux/rbtree.h>
+#include <linux/vmalloc.h>
+#include <linux/wait.h>
+#include <linux/slab.h>
+#include <linux/export.h>
+#include <linux/version.h>
+
+#include <asm/pgtable.h>
+#include <asm/tlbflush.h>
+
+#include <linux/nvmap.h>
+#include <trace/events/nvmap.h>
+
+#include "nvmap_handle.h"
+#include "nvmap_heap.h"
+#include "nvmap_handle_priv.h"
+#include "nvmap_dev.h"
+#include "nvmap_misc.h"
+#include "nvmap_cache.h"
+
+static phys_addr_t handle_phys(struct nvmap_handle *h)
+{
+	if (h->heap_pgalloc)
+		BUG();
+	return h->carveout->base;
+}
+
+void *__nvmap_kmap(struct nvmap_handle *h, unsigned int pagenum)
+{
+	phys_addr_t paddr;
+	unsigned long kaddr;
+	pgprot_t prot;
+	struct vm_struct *area = NULL;
+
+	if (!virt_addr_valid(h))
+		return NULL;
+
+	h = nvmap_handle_get(h);
+	if (!h)
+		return NULL;
+
+	if (!h->alloc)
+		goto put_handle;
+
+	if (!(h->heap_type & nvmap_cpu_access_mask()))
+		goto put_handle;
+
+	nvmap_handle_kmap_inc(h);
+	if (pagenum >= h->size >> PAGE_SHIFT)
+		goto out;
+
+	if (h->vaddr) {
+		kaddr = (unsigned long)h->vaddr + pagenum * PAGE_SIZE;
+	} else {
+		prot = nvmap_handle_pgprot(h, PG_PROT_KERNEL);
+		area = alloc_vm_area(PAGE_SIZE, NULL);
+		if (!area)
+			goto out;
+		kaddr = (ulong)area->addr;
+
+		if (h->heap_pgalloc)
+			paddr = page_to_phys(nvmap_to_page(
+						h->pgalloc.pages[pagenum]));
+		else
+			paddr = h->carveout->base + pagenum * PAGE_SIZE;
+
+		ioremap_page_range(kaddr, kaddr + PAGE_SIZE, paddr, prot);
+	}
+	return (void *)kaddr;
+out:
+	nvmap_handle_kmap_dec(h);
+put_handle:
+	nvmap_handle_put(h);
+	return NULL;
+}
+
+void __nvmap_kunmap(struct nvmap_handle *h, unsigned int pagenum,
+		  void *addr)
+{
+	phys_addr_t paddr;
+	struct vm_struct *area = NULL;
+
+	if (!h || !h->alloc ||
+	    WARN_ON(!virt_addr_valid(h)) ||
+	    WARN_ON(!addr) ||
+	    !(h->heap_type & nvmap_cpu_access_mask()))
+		return;
+
+	if (WARN_ON(pagenum >= h->size >> PAGE_SHIFT))
+		return;
+
+	if (h->vaddr && (h->vaddr == (addr - pagenum * PAGE_SIZE)))
+		goto out;
+
+	if (h->heap_pgalloc)
+		paddr = page_to_phys(nvmap_to_page(h->pgalloc.pages[pagenum]));
+	else
+		paddr = h->carveout->base + pagenum * PAGE_SIZE;
+
+	if (h->flags != NVMAP_HANDLE_UNCACHEABLE &&
+	    h->flags != NVMAP_HANDLE_WRITE_COMBINE) {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 9, 0)
+		__dma_flush_range(addr, addr + PAGE_SIZE);
+#else
+		__dma_flush_area(addr, PAGE_SIZE);
+#endif
+		outer_flush_range(paddr, paddr + PAGE_SIZE); /* FIXME */
+	}
+
+	area = find_vm_area(addr);
+	if (area)
+		free_vm_area(area);
+	else
+		WARN(1, "Invalid address passed");
+out:
+	nvmap_handle_kmap_dec(h);
+	nvmap_handle_put(h);
+}
+
+void *__nvmap_mmap(struct nvmap_handle *h)
+{
+	pgprot_t prot;
+	void *vaddr;
+	unsigned long adj_size;
+	struct vm_struct *v;
+	struct page **pages;
+
+	if (!virt_addr_valid(h))
+		return NULL;
+
+	h = nvmap_handle_get(h);
+	if (!h)
+		return NULL;
+
+	if (!h->alloc)
+		goto put_handle;
+
+	if (!(h->heap_type & nvmap_cpu_access_mask()))
+		goto put_handle;
+
+	if (h->vaddr)
+		return h->vaddr;
+
+	nvmap_handle_kmap_inc(h);
+	prot = nvmap_handle_pgprot(h, PG_PROT_KERNEL);
+
+	if (h->heap_pgalloc) {
+		pages = nvmap_alloc_pages(h->pgalloc.pages, h->size >> PAGE_SHIFT);
+		if (!pages)
+			goto out;
+
+		vaddr = vm_map_ram(pages, h->size >> PAGE_SHIFT, -1, prot);
+		nvmap_altfree(pages, (h->size >> PAGE_SHIFT) * sizeof(*pages));
+		if (!vaddr && !h->vaddr)
+			goto out;
+
+		if (vaddr && atomic_long_cmpxchg(&h->vaddr, 0, (long)vaddr)) {
+			nvmap_handle_kmap_dec(h);
+			vm_unmap_ram(vaddr, h->size >> PAGE_SHIFT);
+		}
+		return h->vaddr;
+	}
+
+	/* carveout - explicitly map the pfns into a vmalloc area */
+	adj_size = h->carveout->base & ~PAGE_MASK;
+	adj_size += h->size;
+	adj_size = PAGE_ALIGN(adj_size);
+
+	v = alloc_vm_area(adj_size, NULL);
+	if (!v)
+		goto out;
+
+	vaddr = v->addr + (h->carveout->base & ~PAGE_MASK);
+	ioremap_page_range((ulong)v->addr, (ulong)v->addr + adj_size,
+		h->carveout->base & PAGE_MASK, prot);
+
+	if (vaddr && atomic_long_cmpxchg(&h->vaddr, 0, (long)vaddr)) {
+		struct vm_struct *vm;
+
+		vaddr -= (h->carveout->base & ~PAGE_MASK);
+		vm = find_vm_area(vaddr);
+		BUG_ON(!vm);
+		free_vm_area(vm);
+		nvmap_handle_kmap_dec(h);
+	}
+
+	/* leave the handle ref count incremented by 1, so that
+	 * the handle will not be freed while the kernel mapping exists.
+	 * nvmap_handle_put will be called by unmapping this address */
+	return h->vaddr;
+out:
+	nvmap_handle_kmap_dec(h);
+put_handle:
+	nvmap_handle_put(h);
+	return NULL;
+}
+
+void __nvmap_munmap(struct nvmap_handle *h, void *addr)
+{
+	if (!h || !h->alloc ||
+	    WARN_ON(!virt_addr_valid(h)) ||
+	    WARN_ON(!addr) ||
+	    !(h->heap_type & nvmap_cpu_access_mask()))
+		return;
+
+	nvmap_handle_put(h);
+}
+
+struct sg_table *__nvmap_sg_table(struct nvmap_client *client,
+		struct nvmap_handle *h)
+{
+	struct sg_table *sgt = NULL;
+	int err, npages;
+	struct page **pages;
+
+	if (!virt_addr_valid(h))
+		return ERR_PTR(-EINVAL);
+
+	h = nvmap_handle_get(h);
+	if (!h)
+		return ERR_PTR(-EINVAL);
+
+	if (!h->alloc) {
+		err = -EINVAL;
+		goto put_handle;
+	}
+
+	npages = PAGE_ALIGN(h->size) >> PAGE_SHIFT;
+	sgt = kzalloc(sizeof(*sgt), GFP_KERNEL);
+	if (!sgt) {
+		err = -ENOMEM;
+		goto err;
+	}
+
+	if (!h->heap_pgalloc) {
+		phys_addr_t paddr = handle_phys(h);
+		struct page *page = phys_to_page(paddr);
+
+		err = sg_alloc_table(sgt, 1, GFP_KERNEL);
+		if (err)
+			goto err;
+
+		sg_set_page(sgt->sgl, page, h->size, offset_in_page(paddr));
+	} else {
+		pages = nvmap_alloc_pages(h->pgalloc.pages, npages);
+		if (!pages) {
+			err = -ENOMEM;
+			goto err;
+		}
+		err = sg_alloc_table_from_pages(sgt, pages,
+				npages, 0, h->size, GFP_KERNEL);
+		nvmap_altfree(pages, npages * sizeof(*pages));
+		if (err)
+			goto err;
+	}
+	nvmap_handle_put(h);
+	return sgt;
+
+err:
+	kfree(sgt);
+put_handle:
+	nvmap_handle_put(h);
+	return ERR_PTR(err);
+}
+
+void __nvmap_free_sg_table(struct nvmap_client *client,
+		struct nvmap_handle *h, struct sg_table *sgt)
+{
+	sg_free_table(sgt);
+	kfree(sgt);
+}
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_handle_mm.c b/drivers/video/tegra/nvmap/nv2/nvmap_handle_mm.c
new file mode 100644
index 000000000000..2781671b64e0
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_handle_mm.c
@@ -0,0 +1,219 @@
+/*
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"%s: " fmt, __func__
+
+#include <linux/err.h>
+#include <linux/io.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/mm.h>
+#include <linux/dma-buf.h>
+#include <linux/moduleparam.h>
+#include <linux/nvmap.h>
+
+#include <asm/pgtable.h>
+
+#include <trace/events/nvmap.h>
+
+#include "nvmap_handle.h"
+#include "nvmap_handle_priv.h"
+#include "nvmap_misc.h"
+#include "nvmap_vma.h"
+
+/*
+ * FIXME: assume user space requests for reserve operations
+ * are page aligned
+ */
+static inline int nvmap_handle_mk(struct nvmap_handle *h,
+				  u32 offset, u32 size,
+				  bool (*fn)(struct page **),
+				  bool locked)
+{
+	int i, nchanged = 0;
+	u32 start_page = offset >> PAGE_SHIFT;
+	u32 end_page = PAGE_ALIGN(offset + size) >> PAGE_SHIFT;
+
+	if (!locked)
+		mutex_lock(&h->lock);
+	if (h->heap_pgalloc &&
+		(offset < h->size) &&
+		(size <= h->size) &&
+		(offset <= (h->size - size))) {
+		for (i = start_page; i < end_page; i++)
+			nchanged += fn(&h->pgalloc.pages[i]) ? 1 : 0;
+	}
+	if (!locked)
+		mutex_unlock(&h->lock);
+	return nchanged;
+}
+
+void nvmap_handle_mkclean(struct nvmap_handle *h, u32 offset, u32 size)
+{
+	int nchanged;
+
+	if (h->heap_pgalloc && !atomic_read(&h->pgalloc.ndirty))
+		return;
+	if (size == 0)
+		size = h->size;
+
+	nchanged = nvmap_handle_mk(h, offset, size, nvmap_page_mkclean, false);
+	if (h->heap_pgalloc)
+		atomic_sub(nchanged, &h->pgalloc.ndirty);
+}
+
+void nvmap_handle_mkdirty(struct nvmap_handle *h, u32 offset, u32 size)
+{
+	int nchanged;
+
+	if (h->heap_pgalloc &&
+		(atomic_read(&h->pgalloc.ndirty) == (h->size >> PAGE_SHIFT)))
+		return;
+
+	nchanged = nvmap_handle_mk(h, offset, size, nvmap_page_mkdirty, true);
+	if (h->heap_pgalloc)
+		atomic_add(nchanged, &h->pgalloc.ndirty);
+}
+
+
+
+static int handle_prot(struct nvmap_handle *handle, u64 offset,
+							u64 size, int op)
+{
+	struct nvmap_vma_list *vma_list;
+	int err = -EINVAL;
+
+	if (!handle->heap_pgalloc)
+		return err;
+
+	if ((offset >= handle->size) || (offset > handle->size - size) ||
+	    (size > handle->size)) {
+		pr_debug("%s offset: %lld h->size: %zu size: %lld\n", __func__,
+				offset, handle->size, size);
+		return err;
+	}
+
+	if (!size)
+		size = handle->size;
+
+	size = PAGE_ALIGN((offset & ~PAGE_MASK) + size);
+
+	mutex_lock(&handle->lock);
+
+	list_for_each_entry(vma_list, &handle->vmas, list) {
+		int handle_is_dirty = (nvmap_handle_track_dirty(handle) &&
+				atomic_read(&handle->pgalloc.ndirty));
+
+		err = nvmap_vma_list_prot(vma_list, offset, size,
+							handle_is_dirty, op);
+		if (err)
+			break;
+
+		if(op == NVMAP_HANDLE_PROT_RESTORE)
+			nvmap_handle_mkdirty(handle, 0, size);
+	}
+
+	mutex_unlock(&handle->lock);
+
+	return err;
+}
+
+static int handles_prot(struct nvmap_handle **handles, u64 *offsets,
+		       u64 *sizes, int op, int nr)
+{
+	int i, err = 0;
+
+	down_write(&current->mm->mmap_sem);
+	for (i = 0; i < nr; i++) {
+		err = handle_prot(handles[i], offsets[i], sizes[i], op);
+		if (err) {
+			pr_debug("%s nvmap_prot_handle failed [%d]\n",
+					__func__, err);
+			goto finish;
+		}
+	}
+finish:
+	up_write(&current->mm->mmap_sem);
+	return err;
+}
+
+int nvmap_handles_reserve(struct nvmap_handle **handles, u64 *offsets,
+						u64 *sizes, int op, int nr)
+{
+	int i, err;
+	int cache_op;
+
+	for (i = 0; i < nr; i++) {
+		if ((offsets[i] != 0) || (sizes[i] != handles[i]->size)) {
+			pr_debug("%s offset: %lld size: %lld h->size %zu\n",
+					__func__, offsets[i], sizes[i],
+					handles[i]->size);
+			return -EINVAL;
+		}
+
+		if (op == NVMAP_PAGES_PROT_AND_CLEAN)
+			continue;
+
+		/*
+		 * NOTE: This unreserves the handle even when
+		 * NVMAP_PAGES_INSERT_ON_UNRESERVE is called on some portion
+		 * of the handle
+		 */
+		atomic_set(&handles[i]->pgalloc.reserved,
+				(op == NVMAP_PAGES_RESERVE) ? 1 : 0);
+	}
+
+	err = 0;
+	switch (op) {
+		case NVMAP_PAGES_PROT_AND_CLEAN:
+			/* Fall through */
+		case NVMAP_PAGES_RESERVE:
+			err = handles_prot(handles, offsets, sizes,
+					NVMAP_HANDLE_PROT_NONE, nr);
+			break;
+		case NVMAP_INSERT_PAGES_ON_UNRESERVE:
+			err = handles_prot(handles, offsets, sizes,
+					NVMAP_HANDLE_PROT_RESTORE, nr);
+			break;
+		case NVMAP_PAGES_UNRESERVE:
+			for (i = 0; i < nr; i++)
+				if (nvmap_handle_track_dirty(handles[i]))
+					atomic_set(&handles[i]->pgalloc.ndirty, 0);
+			break;
+		default:
+			return -EINVAL;
+	}
+
+	if (!(handles[0]->userflags & NVMAP_HANDLE_CACHE_SYNC_AT_RESERVE))
+		return 0;
+
+	if ((op == NVMAP_PAGES_UNRESERVE) && handles[0]->heap_pgalloc)
+		return 0;
+
+	if (op == NVMAP_PAGES_RESERVE) {
+		cache_op = NVMAP_CACHE_OP_WB;
+	} else {
+		cache_op = NVMAP_CACHE_OP_WB_INV;
+	}
+
+	err = nvmap_handles_cache_maint(handles, offsets, sizes, cache_op, nr);
+	if (err)
+		return err;
+
+	if (op == NVMAP_PAGES_RESERVE) {
+		for (i = 0; i < nr; i++)
+			nvmap_handle_mkclean(handles[i], offsets[i], sizes[i]);
+	}
+
+	return 0;
+}
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_handle_print.c b/drivers/video/tegra/nvmap/nv2/nvmap_handle_print.c
new file mode 100644
index 000000000000..a598ee98b027
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_handle_print.c
@@ -0,0 +1,353 @@
+/*
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"%s: " fmt, __func__
+
+#include <linux/err.h>
+#include <linux/io.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/mm.h>
+#include <linux/rbtree.h>
+#include <linux/dma-buf.h>
+#include <linux/moduleparam.h>
+#include <linux/nvmap.h>
+#include <linux/slab.h>
+#include <linux/vmalloc.h>
+#include <linux/version.h>
+#include <linux/uaccess.h>
+
+#include <soc/tegra/chip-id.h>
+
+#include <asm/pgtable.h>
+
+#include <trace/events/nvmap.h>
+
+#include "nvmap_stats.h"
+
+#include "nvmap_misc.h"
+#include "nvmap_cache.h"
+#include "nvmap_heap_alloc.h"
+#include "nvmap_carveout.h"
+#include "nvmap_carveout.h"
+#include "nvmap_dev.h"
+#include "nvmap_dmabuf.h"
+#include "nvmap_vma.h"
+#include "nvmap_tag.h"
+
+#include "nvmap_handle.h"
+#include "nvmap_handle_priv.h"
+
+#define K(x) (x >> 10)
+
+void nvmap_handle_stringify(struct nvmap_handle *handle,
+				  struct seq_file *s, u32 heap_type,
+				  int ref_dupes)
+{
+	struct nvmap_device *dev = nvmap_dev;
+
+	if (handle->alloc && handle->heap_type == heap_type) {
+		phys_addr_t base = heap_type == NVMAP_HEAP_IOVMM ? 0 :
+			handle->heap_pgalloc ? 0 :
+			(handle->carveout->base);
+		size_t size = K(handle->size);
+		int i = 0;
+
+		// TODO: Remove GOTO
+next_page:
+		if ((heap_type == NVMAP_HEAP_CARVEOUT_VPR) && handle->heap_pgalloc) {
+			base = page_to_phys(handle->pgalloc.pages[i++]);
+			size = K(PAGE_SIZE);
+		}
+
+		seq_printf(s,
+				"%-18s %-18s %8llx %10zuK %8x %6u %6u %6u %6u %6u %6u %8pK %s\n",
+				"", "",
+				(unsigned long long)base, size,
+				handle->userflags,
+				atomic_read(&handle->ref),
+				ref_dupes,
+				0,
+				atomic_read(&handle->kmap_count),
+				atomic_read(&handle->umap_count),
+				atomic_read(&handle->share_count),
+				handle,
+				__nvmap_tag_name(dev, handle->userflags >> 16));
+
+		if ((heap_type == NVMAP_HEAP_CARVEOUT_VPR) && handle->heap_pgalloc) {
+			i++;
+			if (i < (handle->size >> PAGE_SHIFT))
+				goto next_page;
+		}
+	}
+}
+
+/* compute the total amount of handle physical memory that is mapped
+ * into client's virtual address space. Remember that vmas list is
+ * sorted in ascending order of handle offsets.
+ * NOTE: This function should be called while holding handle's lock mutex.
+ */
+static u64 vma_calc_mss(pid_t client_pid, struct list_head *vmas)
+{
+	struct nvmap_vma_list *vma_list = NULL;
+	struct vm_area_struct *vma = NULL;
+	u64 end_offset = 0, vma_start_offset, vma_size;
+	int64_t overlap_size;
+	u64 total = 0;
+
+	list_for_each_entry(vma_list, vmas, list) {
+
+		if (client_pid == vma_list->pid) {
+			vma = vma_list->vma;
+			vma_size = vma->vm_end - vma->vm_start;
+
+			vma_start_offset = vma->vm_pgoff << PAGE_SHIFT;
+			if (end_offset < vma_start_offset + vma_size) {
+				total += vma_size;
+
+				overlap_size = end_offset - vma_start_offset;
+				if (overlap_size > 0)
+					total -= overlap_size;
+				end_offset = vma_start_offset + vma_size;
+			}
+		}
+	}
+	return total;
+}
+
+// TODO: Find a way to unify this with the above function
+void nvmap_handle_maps_stringify(struct nvmap_handle *handle,
+				  struct seq_file *s, u32 heap_type,
+				  pid_t client_pid)
+{
+	struct nvmap_vma_list *vma_list = NULL;
+	struct vm_area_struct *vma = NULL;
+	u64 total_mapped_size, vma_size;
+
+	if (handle->alloc && handle->heap_type == heap_type) {
+		phys_addr_t base = heap_type == NVMAP_HEAP_IOVMM ? 0 :
+			handle->heap_pgalloc ? 0 :
+			(handle->carveout->base);
+		size_t size = K(handle->size);
+		int i = 0;
+
+next_page:
+		if ((heap_type == NVMAP_HEAP_CARVEOUT_VPR) && handle->heap_pgalloc) {
+			base = page_to_phys(handle->pgalloc.pages[i++]);
+			size = K(PAGE_SIZE);
+		}
+
+		seq_printf(s,
+				"%-18s %-18s %8llx %10zuK %8x %6u %16pK "
+				"%12s %12s ",
+				"", "",
+				(unsigned long long)base, K(handle->size),
+				handle->userflags,
+				atomic_read(&handle->share_count),
+				handle, "", "");
+
+		if ((heap_type == NVMAP_HEAP_CARVEOUT_VPR) && handle->heap_pgalloc) {
+			i++;
+			if (i < (handle->size >> PAGE_SHIFT))
+				goto next_page;
+		}
+
+		mutex_lock(&handle->lock);
+		total_mapped_size = vma_calc_mss(client_pid, &handle->vmas);
+		seq_printf(s, "%6lluK\n", K(total_mapped_size));
+
+		list_for_each_entry(vma_list, &handle->vmas, list) {
+
+			if (vma_list->pid == client_pid) {
+				vma = vma_list->vma;
+				vma_size = vma->vm_end - vma->vm_start;
+				seq_printf(s,
+						"%-18s %-18s %8s %11s %8s %6s %16s "
+						"%-12lx-%12lx %6lluK\n",
+						"", "", "", "", "", "", "",
+						vma->vm_start, vma->vm_end,
+						K(vma_size));
+			}
+		}
+		mutex_unlock(&handle->lock);
+	}
+
+}
+
+// TODO: Unify all the functions that print
+void nvmap_handle_all_allocations_show(struct nvmap_handle *handle,
+				  struct seq_file *s, u32 heap_type)
+{
+	int i = 0;
+
+	if (handle->alloc && handle->heap_type == heap_type) {
+		phys_addr_t base = heap_type == NVMAP_HEAP_IOVMM ? 0 :
+			handle->heap_pgalloc ? 0 :
+			(handle->carveout->base);
+		size_t size = K(handle->size);
+
+next_page:
+		if ((heap_type == NVMAP_HEAP_CARVEOUT_VPR) && handle->heap_pgalloc) {
+			base = page_to_phys(handle->pgalloc.pages[i++]);
+			size = K(PAGE_SIZE);
+		}
+
+		seq_printf(s,
+				"%8llx %10zuK %9x %6u %6u %6u %6u %8p\n",
+				(unsigned long long)base, K(handle->size),
+				handle->userflags,
+				atomic_read(&handle->ref),
+				atomic_read(&handle->kmap_count),
+				atomic_read(&handle->umap_count),
+				atomic_read(&handle->share_count),
+				handle);
+
+		if ((heap_type == NVMAP_HEAP_CARVEOUT_VPR) && handle->heap_pgalloc) {
+			i++;
+			if (i < (handle->size >> PAGE_SHIFT))
+				goto next_page;
+		}
+	}
+
+}
+void nvmap_handle_orphans_allocations_show(struct nvmap_handle *handle,
+				  struct seq_file *s, u32 heap_type)
+{
+	if(atomic_read(&handle->share_count))
+		return;
+
+	nvmap_handle_all_allocations_show(handle, s, heap_type);
+}
+
+u64 nvmap_handle_share_size(struct nvmap_handle *handle, u32 heap_type)
+{
+	if (handle->alloc && handle->heap_type == heap_type) {
+		return handle->size / atomic_read(&handle->share_count);
+	} else {
+		return 0;
+	}
+}
+
+// TODO: Unify this between handle and client
+struct procrank_stats {
+	struct vm_area_struct *vma;
+	u64 pss;
+};
+
+u64 nvmap_handle_procrank_walk(struct nvmap_handle *h, struct mm_walk *walk,
+		pid_t client_pid)
+{
+	struct nvmap_vma_list *tmp;
+	struct procrank_stats *mss = walk->private;
+
+	if (!h || !h->alloc || !h->heap_pgalloc)
+		return 0;
+
+	mutex_lock(&h->lock);
+	list_for_each_entry(tmp, &h->vmas, list) {
+		if (client_pid == tmp->pid) {
+			mss->vma = tmp->vma;
+			walk_page_range(tmp->vma->vm_start,
+					tmp->vma->vm_end,
+					walk);
+		}
+	}
+	mutex_unlock(&h->lock);
+
+	return h->size / atomic_read(&h->share_count);
+
+}
+
+u64 nvmap_handle_total_pss(struct nvmap_handle *h, u32 heap_type)
+{
+	int i;
+	u64 pss = 0;
+
+	if (!h || !h->alloc || h->heap_type != heap_type)
+		return 0;
+
+	for (i = 0; i < h->size >> PAGE_SHIFT; i++) {
+		struct page *page = nvmap_to_page(h->pgalloc.pages[i]);
+
+		if (page_mapcount(page) > 0)
+			pss += PAGE_SIZE;
+	}
+
+	return pss;
+}
+
+u64 nvmap_handle_total_mss(struct nvmap_handle *h, u32 heap_type)
+{
+	if (!h || !h->alloc || h->heap_type != heap_type)
+		return 0;
+
+	return h->size;
+}
+
+int nvmap_handle_pid_show(struct nvmap_handle *handle, struct seq_file *s,
+					pid_t client_pid)
+{
+		struct nvmap_debugfs_handles_entry entry;
+		u64 total_mapped_size = 0;
+		int i = 0;
+		int ret = 0;
+
+		if (!handle->alloc)
+			return 0;
+
+		mutex_lock(&handle->lock);
+
+		total_mapped_size = vma_calc_mss(client_pid, &handle->vmas);
+		mutex_unlock(&handle->lock);
+
+		entry.base = handle->heap_type == NVMAP_HEAP_IOVMM ? 0 :
+			     handle->heap_pgalloc ? 0 :
+			     (handle->carveout->base);
+		entry.size = handle->size;
+		entry.flags = handle->userflags;
+		entry.share_count = atomic_read(&handle->share_count);
+		entry.mapped_size = total_mapped_size;
+
+next_page:
+		if ((handle->heap_type == NVMAP_HEAP_CARVEOUT_VPR) && handle->heap_pgalloc) {
+			entry.base = page_to_phys(handle->pgalloc.pages[i++]);
+			entry.size = K(PAGE_SIZE);
+		}
+
+		ret = seq_write(s, &entry, sizeof(entry));
+		if (ret < 0)
+			return ret;
+
+		if ((handle->heap_type == NVMAP_HEAP_CARVEOUT_VPR) && handle->heap_pgalloc) {
+			i++;
+			if (i < (handle->size >> PAGE_SHIFT))
+				goto next_page;
+		}
+
+		return 0;
+}
+
+int nvmap_handle_is_migratable(struct nvmap_handle *h)
+{
+	return (!atomic_read(&h->pin) && !atomic_read(&h->kmap_count));
+}
+
+void nvmap_handle_lru_show(struct nvmap_handle *h, struct seq_file *s)
+{
+		seq_printf(s, "%-18s %18s %8s %10zuK %8s %6s %6s %6u %6u "
+			"%6u %8p\n", "", "", "", K(h->size), "", "",
+			"", atomic_read(&h->pin),
+			    atomic_read(&h->kmap_count),
+			    atomic_read(&h->umap_count),
+			    h);
+}
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_handle_priv.h b/drivers/video/tegra/nvmap/nv2/nvmap_handle_priv.h
new file mode 100644
index 000000000000..05f7bc551017
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_handle_priv.h
@@ -0,0 +1,62 @@
+/*
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __NVMAP_HANDLE_PRIV_H
+#define __NVMAP_HANDLE_PRIV_H
+
+// TODO: Remove this definition from here
+/* handles allocated using shared system memory (either IOVMM- or high-order
+ * page allocations */
+struct nvmap_pgalloc {
+	struct page **pages;
+	bool contig;			/* contiguous system memory */
+	atomic_t reserved;
+	atomic_t ndirty;	/* count number of dirty pages */
+};
+
+struct nvmap_handle {
+	struct rb_node node;	/* entry on global handle tree */
+	atomic_t ref;		/* reference count (i.e., # of duplications) */
+	atomic_t pin;		/* pin count */
+	u32 flags;		/* caching flags */
+	size_t size;		/* padded (as-allocated) size */
+	size_t orig_size;	/* original (as-requested) size */
+	size_t align;
+	struct nvmap_client *owner;
+	struct dma_buf *dmabuf;
+	union {
+		struct nvmap_pgalloc pgalloc;
+		struct nvmap_heap_block *carveout;
+	};
+	bool heap_pgalloc;	/* handle is page allocated (sysmem / iovmm) */
+	bool alloc;		/* handle has memory allocated */
+	bool from_va;		/* handle memory is from VA */
+	u32 heap_type;		/* handle heap is allocated from */
+	u32 userflags;		/* flags passed from userspace */
+	void *vaddr;		/* mapping used inside kernel */
+	struct list_head vmas;	/* list of all user vma's */
+	atomic_t umap_count;	/* number of outstanding maps from user */
+	atomic_t kmap_count;	/* number of outstanding map from kernel */
+	atomic_t share_count;	/* number of processes sharing the handle */
+	struct list_head lru;	/* list head to track the lru */
+	struct mutex lock;
+	struct list_head dmabuf_priv;
+	u64 ivm_id;
+	int peer;		/* Peer VM number */
+	int fd;
+};
+
+void nvmap_handle_mkclean(struct nvmap_handle *h, u32 offset, u32 size);
+void nvmap_handle_mkdirty(struct nvmap_handle *h, u32 offset, u32 size);
+
+#endif /* __NVMAP_HANDLE_PRIV_H */
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_handle_ref.c b/drivers/video/tegra/nvmap/nv2/nvmap_handle_ref.c
new file mode 100644
index 000000000000..991ed03590a3
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_handle_ref.c
@@ -0,0 +1,85 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_handle.c
+ *
+ * Handle allocation and freeing routines for nvmap
+ *
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"%s: " fmt, __func__
+
+#include <linux/err.h>
+#include <linux/kernel.h>
+#include <linux/slab.h>
+#include <linux/rbtree.h>
+#include <linux/dma-buf.h>
+#include <linux/moduleparam.h>
+#include <linux/nvmap.h>
+
+#include <trace/events/nvmap.h>
+
+#include "nvmap_handle.h"
+#include "nvmap_handle_priv.h"
+#include "nvmap_handle_ref.h"
+
+int nvmap_handle_ref_get(struct nvmap_handle_ref *ref)
+{
+	if (!ref)
+		return -1;
+
+	atomic_inc(&ref->dupes);
+	nvmap_handle_get(ref->handle);
+	return 0;
+}
+
+int nvmap_handle_ref_count(struct nvmap_handle_ref *ref)
+{
+	return atomic_read(&ref->dupes);
+}
+
+struct nvmap_handle_ref *nvmap_handle_ref_create(struct nvmap_handle *handle)
+{
+	struct nvmap_handle_ref *ref = NULL;
+
+	ref = kzalloc(sizeof(*ref), GFP_KERNEL);
+	if (!ref) {
+		return NULL;
+	}
+
+	atomic_set(&ref->dupes, 1);
+
+	handle = nvmap_handle_get(handle);
+	if (!handle) {
+		kfree(ref);
+		return NULL;
+	}
+
+	ref->handle = handle;
+	atomic_inc(&handle->share_count);
+
+	get_dma_buf(handle->dmabuf);
+	return ref;
+}
+
+void nvmap_handle_ref_free(struct nvmap_handle_ref *ref)
+{
+	atomic_dec(&ref->handle->share_count);
+	dma_buf_put(ref->handle->dmabuf);
+	kfree(ref);
+}
+
+int nvmap_handle_ref_put(struct nvmap_handle_ref *ref)
+{
+	// TODO: Do the same error that handle does on ref dec
+	nvmap_handle_put(ref->handle);
+	return atomic_dec_return(&ref->dupes);
+}
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_handle_ref.h b/drivers/video/tegra/nvmap/nv2/nvmap_handle_ref.h
new file mode 100644
index 000000000000..215686639710
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_handle_ref.h
@@ -0,0 +1,36 @@
+/*
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __NVMAP_HANDLE_REF_H
+#define __NVMAP_HANDLE_REF_H
+
+/* handle_ref objects are client-local references to an nvmap_handle;
+ * they are distinct objects so that handles can be unpinned and
+ * unreferenced the correct number of times when a client abnormally
+ * terminates */
+struct nvmap_handle_ref {
+	struct nvmap_handle *handle;
+	struct rb_node	node;
+	atomic_t	dupes;	/* number of times to free on file close */
+};
+
+struct nvmap_handle_ref *nvmap_handle_ref_create(struct nvmap_handle *handle);
+void nvmap_handle_ref_free(struct nvmap_handle_ref *ref);
+
+int nvmap_handle_ref_get(struct nvmap_handle_ref *ref);
+int nvmap_handle_ref_put(struct nvmap_handle_ref *ref);
+int nvmap_handle_ref_count(struct nvmap_handle_ref *ref);
+
+struct rb_node *nvmap_handle_ref_to_node(struct nvmap_handle_ref *ref);
+
+#endif /* __NVMAP_HANDLE_REF_H */
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_handle_vma.c b/drivers/video/tegra/nvmap/nv2/nvmap_handle_vma.c
new file mode 100644
index 000000000000..fef46d622d1b
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_handle_vma.c
@@ -0,0 +1,260 @@
+/*
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"nvmap: %s() " fmt, __func__
+
+#include <linux/err.h>
+#include <linux/io.h>
+#include <linux/vmalloc.h>
+#include <linux/nvmap.h>
+#include <linux/slab.h>
+#include <linux/highmem.h>
+
+#include <trace/events/nvmap.h>
+
+#include <asm/pgtable.h>
+
+#include "nvmap_handle.h"
+#include "nvmap_handle_priv.h"
+#include "nvmap_client.h"
+#include "nvmap_dev.h"
+#include "nvmap_misc.h"
+#include "nvmap_carveout.h"
+#include "nvmap_vma.h"
+#include "nvmap_cache.h"
+
+int nvmap_handle_owns_vma(struct nvmap_handle *h, struct vm_area_struct *vma)
+{
+	return nvmap_vma_belongs_to_handle(vma, h);
+}
+
+int nvmap_handle_add_vma(struct nvmap_handle *handle,
+					struct vm_area_struct *vma)
+{
+	struct nvmap_vma_list *vma_list;
+	struct nvmap_vma_list *tmp;
+	struct list_head *tmp_head = NULL;
+	bool vma_pos_found = false;
+	pid_t current_pid = task_tgid_nr(current);
+
+	// TODO: Just push this code into the vma code
+	vma_list = kmalloc(sizeof(*vma_list), GFP_KERNEL);
+	if (!vma_list)
+		return -ENOMEM;
+
+	vma_list->vma = vma;
+	vma_list->pid = current_pid;
+	vma_list->save_vm_flags = vma->vm_flags;
+	atomic_set(&vma_list->ref, 1);
+
+	mutex_lock(&handle->lock);
+	tmp_head = &handle->vmas;
+
+	/* insert vma into handle's vmas list in the increasing order of
+	 * handle offsets
+	 */
+	list_for_each_entry(tmp, &handle->vmas, list) {
+		/* if vma exists in list, just increment refcount */
+		if (tmp->vma == vma) {
+			atomic_inc(&tmp->ref);
+			kfree(vma_list);
+			goto unlock;
+		}
+
+		if (!vma_pos_found && (current_pid == tmp->pid)) {
+			if (vma->vm_pgoff < tmp->vma->vm_pgoff) {
+				tmp_head = &tmp->list;
+				vma_pos_found = true;
+			} else {
+				tmp_head = tmp->list.next;
+			}
+		}
+	}
+
+	list_add_tail(&vma_list->list, tmp_head);
+unlock:
+	mutex_unlock(&handle->lock);
+	return 0;
+}
+
+int nvmap_handle_del_vma(struct nvmap_handle *handle,
+					struct vm_area_struct *vma)
+{
+	struct nvmap_vma_list *vma_list;
+	bool vma_found = false;
+
+	mutex_lock(&handle->lock);
+
+	list_for_each_entry(vma_list, &handle->vmas, list) {
+		if (vma_list->vma != vma)
+			continue;
+		if (atomic_dec_return(&vma_list->ref) == 0) {
+			list_del(&vma_list->list);
+			kfree(vma_list);
+		}
+		vma_found = true;
+		break;
+	}
+	if (!vma_found)
+		return -EFAULT;
+
+	mutex_unlock(&handle->lock);
+
+	return 0;
+}
+
+int nvmap_handle_open_vma(struct nvmap_handle *handle)
+{
+	int nr_page, i;
+	mutex_lock(&handle->lock);
+	if (!handle->heap_pgalloc) {
+		goto finish;
+	}
+
+	nr_page = handle->size >> PAGE_SHIFT;
+	for (i = 0; i < nr_page; i++) {
+		struct page *page = nvmap_to_page(handle->pgalloc.pages[i]);
+		/* This is necessry to avoid page being accounted
+		 * under NR_FILE_MAPPED. This way NR_FILE_MAPPED would
+		 * be fully accounted under NR_FILE_PAGES. This allows
+		 * Android low mem killer detect low memory condition
+		 * precisely.
+		 * This has a side effect of inaccurate pss accounting
+		 * for NvMap memory mapped into user space. Android
+		 * procrank and NvMap Procrank both would have same
+		 * issue. Subtracting NvMap_Procrank pss from
+		 * procrank pss would give non-NvMap pss held by process
+		 * and adding NvMap memory used by process represents
+		 * entire memroy consumption by the process.
+		 */
+		atomic_inc(&page->_mapcount);
+	}
+finish:
+	mutex_unlock(&handle->lock);
+	return 0;
+}
+
+int nvmap_handle_close_vma(struct nvmap_handle *handle)
+{
+	int nr_page, i;
+
+	nr_page = handle->size >> PAGE_SHIFT;
+
+	if (!handle->heap_pgalloc)
+		return 0;
+
+	mutex_lock(&handle->lock);
+
+	for (i = 0; i < nr_page; i++) {
+		struct page *page;
+		page = nvmap_to_page(handle->pgalloc.pages[i]);
+		atomic_dec(&page->_mapcount);
+	}
+
+	mutex_unlock(&handle->lock);
+
+	return 0;
+}
+
+static void page_inner_cache_maint(struct page *page)
+{
+	void *kaddr;
+
+	/* inner cache maint */
+	kaddr  = kmap(page);
+	BUG_ON(!kaddr);
+	nvmap_cache_maint_inner(NVMAP_CACHE_OP_WB_INV, kaddr, PAGE_SIZE);
+	kunmap(page);
+
+}
+
+int nvmap_handle_fault_vma(struct nvmap_handle *handle,
+		unsigned long offs, struct page **page_ptr)
+{
+	struct page *page;
+
+	if (!handle->alloc || offs >= handle->size)
+		return VM_FAULT_SIGBUS;
+
+	if (!handle->heap_pgalloc) {
+		unsigned long pfn;
+		BUG_ON(handle->carveout->base & ~PAGE_MASK);
+		pfn = ((handle->carveout->base + offs) >> PAGE_SHIFT);
+		if (!pfn_valid(pfn)) {
+			*page_ptr = pfn_to_page(pfn);
+			return VM_FAULT_NOPAGE;
+		}
+		/* CMA memory would get here */
+		page = pfn_to_page(pfn);
+	} else {
+
+		offs >>= PAGE_SHIFT;
+		if (atomic_read(&handle->pgalloc.reserved))
+			return VM_FAULT_SIGBUS;
+		page = nvmap_to_page(handle->pgalloc.pages[offs]);
+
+		if (!nvmap_handle_track_dirty(handle))
+			goto finish;
+
+		mutex_lock(&handle->lock);
+		if (nvmap_page_dirty(handle->pgalloc.pages[offs])) {
+			mutex_unlock(&handle->lock);
+			goto finish;
+		}
+
+		page_inner_cache_maint(page);
+
+		nvmap_page_mkdirty(&handle->pgalloc.pages[offs]);
+		atomic_inc(&handle->pgalloc.ndirty);
+		mutex_unlock(&handle->lock);
+	}
+finish:
+	*page_ptr = page;
+	return 0;
+}
+
+bool nvmap_handle_fixup_prot_vma(struct nvmap_handle *handle,
+					unsigned long offs)
+{
+	struct page *page;
+
+	if (!handle->alloc)
+		return false;
+
+	if ((offs >= handle->size) || !handle->heap_pgalloc)
+		return false;
+
+	if (atomic_read(&handle->pgalloc.reserved))
+		return false;
+
+	if (!nvmap_handle_track_dirty(handle))
+		return true;
+
+	mutex_lock(&handle->lock);
+
+	offs >>= PAGE_SHIFT;
+	if (nvmap_page_dirty(handle->pgalloc.pages[offs]))
+		goto unlock;
+
+	page = nvmap_to_page(handle->pgalloc.pages[offs]);
+
+	page_inner_cache_maint(page);
+
+	nvmap_page_mkdirty(&handle->pgalloc.pages[offs]);
+	atomic_inc(&handle->pgalloc.ndirty);
+
+unlock:
+	mutex_unlock(&handle->lock);
+	return true;
+
+}
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_heap.c b/drivers/video/tegra/nvmap/nv2/nvmap_heap.c
new file mode 100644
index 000000000000..654b3d04c963
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_heap.c
@@ -0,0 +1,355 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_heap.c
+ *
+ * GPU heap allocator.
+ *
+ * Copyright (c) 2011-2018, NVIDIA Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"%s: " fmt, __func__
+
+#include <linux/debugfs.h>
+#include <linux/device.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/mm.h>
+#include <linux/mutex.h>
+#include <linux/slab.h>
+#include <linux/err.h>
+#include <linux/bug.h>
+#include <linux/stat.h>
+#include <linux/sizes.h>
+#include <linux/io.h>
+#include <linux/version.h>
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+#include <linux/sched/clock.h>
+#endif
+
+#include <linux/nvmap.h>
+#include <linux/dma-mapping.h>
+#include <linux/dma-contiguous.h>
+
+#include "nvmap_heap.h"
+#include "nvmap_dev.h"
+#include "nvmap_cache.h"
+
+/*
+ * "carveouts" are platform-defined regions of physically contiguous memory
+ * which are not managed by the OS. A platform may specify multiple carveouts,
+ * for either small special-purpose memory regions (like IRAM on Tegra SoCs)
+ * or reserved regions of main system memory.
+ *
+ * The carveout allocator returns allocations which are physically contiguous.
+ */
+
+struct kmem_cache *heap_block_cache;
+
+struct list_block {
+	struct nvmap_heap_block block;
+	struct list_head all_list;
+	unsigned int mem_prot;
+	phys_addr_t orig_addr;
+	size_t size;
+	size_t align;
+	struct nvmap_heap *heap;
+	struct list_head free_list;
+};
+
+struct nvmap_heap {
+	struct list_head all_list;
+	struct mutex lock;
+	const char *name;
+	void *arg;
+	/* heap base */
+	phys_addr_t base;
+	/* heap size */
+	size_t len;
+	struct device *cma_dev;
+	struct device *dma_dev;
+	bool is_ivm;
+	bool can_alloc; /* Used only if is_ivm == true */
+	int peer; /* Used only if is_ivm == true */
+	int vm_id; /* Used only if is_ivm == true */
+	struct nvmap_pm_ops pm_ops;
+};
+
+extern ulong nvmap_init_time;
+
+int nvmap_query_heap_peer(struct nvmap_heap *heap)
+{
+	if (!heap || !heap->is_ivm)
+		return -EINVAL;
+
+	return heap->peer;
+}
+
+size_t nvmap_query_heap_size(struct nvmap_heap *heap)
+{
+	if (!heap)
+		return -EINVAL;
+
+	return heap->len;
+}
+
+void nvmap_heap_debugfs_init(struct dentry *heap_root, struct nvmap_heap *heap)
+{
+	if (sizeof(heap->base) == sizeof(u64))
+		debugfs_create_x64("base", S_IRUGO,
+			heap_root, (u64 *)&heap->base);
+	else
+		debugfs_create_x32("base", S_IRUGO,
+			heap_root, (u32 *)&heap->base);
+	if (sizeof(heap->len) == sizeof(u64))
+		debugfs_create_x64("size", S_IRUGO,
+			heap_root, (u64 *)&heap->len);
+	else
+		debugfs_create_x32("size", S_IRUGO,
+			heap_root, (u32 *)&heap->len);
+}
+
+static void nvmap_free_mem(struct nvmap_heap *h, phys_addr_t base,
+				size_t len)
+{
+	struct device *dev = h->dma_dev;
+	DEFINE_DMA_ATTRS(attrs);
+
+	dma_set_attr(DMA_ATTR_ALLOC_EXACT_SIZE, __DMA_ATTR(attrs));
+	dev_dbg(dev, "Free base (%pa) size (%zu)\n", &base, len);
+#ifdef CONFIG_TEGRA_VIRTUALIZATION
+	if (h->is_ivm && !h->can_alloc) {
+		dma_mark_declared_memory_unoccupied(dev, base, len, __DMA_ATTR(attrs));
+	} else
+#endif
+	{
+		dma_free_attrs(dev, len,
+			        (void *)(uintptr_t)base,
+			        (dma_addr_t)base, __DMA_ATTR(attrs));
+	}
+}
+
+static struct list_block *do_heap_free(struct nvmap_heap_block *block)
+{
+	struct list_block *b = container_of(block, struct list_block, block);
+	struct nvmap_heap *heap = b->heap;
+
+	list_del(&b->all_list);
+
+	nvmap_free_mem(heap, block->base, b->size);
+	kmem_cache_free(heap_block_cache, b);
+
+	return b;
+}
+
+
+struct nvmap_heap *nvmap_block_to_heap(struct nvmap_heap_block *b)
+{
+	struct list_block *lb;
+	lb = container_of(b, struct list_block, block);
+	return lb->heap;
+}
+
+/* nvmap_heap_free: frees block b*/
+void nvmap_heap_free(struct nvmap_heap_block *b)
+{
+	struct nvmap_heap *h;
+	struct list_block *lb;
+
+	if (!b)
+		return;
+
+	h = nvmap_block_to_heap(b);
+	mutex_lock(&h->lock);
+
+	lb = container_of(b, struct list_block, block);
+	nvmap_flush_heap_block(NULL, b, lb->size, lb->mem_prot);
+	do_heap_free(b);
+	/*
+	 * If this HEAP has pm_ops defined and powering off the
+	 * RAM attached with the HEAP returns error, raise warning.
+	 */
+	if (h->pm_ops.idle) {
+		if (h->pm_ops.idle() < 0)
+			WARN_ON(1);
+	}
+
+	mutex_unlock(&h->lock);
+}
+
+/* nvmap_heap_create: create a heap object of len bytes, starting from
+ * address base.
+ */
+struct nvmap_heap *nvmap_heap_create(struct device *parent,
+				     const struct nvmap_platform_carveout *co,
+				     phys_addr_t base, size_t len, void *arg)
+{
+	struct nvmap_heap *h;
+
+	h = kzalloc(sizeof(*h), GFP_KERNEL);
+	if (!h) {
+		dev_err(parent, "%s: out of memory\n", __func__);
+		return NULL;
+	}
+
+	h->dma_dev = co->dma_dev;
+	if (co->cma_dev) {
+#ifdef CONFIG_DMA_CMA
+		struct dma_contiguous_stats stats;
+
+		if (dma_get_contiguous_stats(co->cma_dev, &stats))
+			goto fail;
+
+		base = stats.base;
+		len = stats.size;
+		h->cma_dev = co->cma_dev;
+#else
+		dev_err(parent, "invalid resize config for carveout %s\n",
+				co->name);
+		goto fail;
+#endif
+	} else if (!co->init_done) {
+		int err;
+
+		/* declare Non-CMA heap */
+		err = dma_declare_coherent_memory(h->dma_dev, 0, base, len,
+				DMA_MEMORY_NOMAP | DMA_MEMORY_EXCLUSIVE);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+		if (!err) {
+#else
+		if (err & DMA_MEMORY_NOMAP) {
+#endif
+			dev_info(parent,
+				"%s :dma coherent mem declare %pa,%zu\n",
+				co->name, &base, len);
+		} else {
+			dev_err(parent,
+				"%s: dma coherent declare fail %pa,%zu\n",
+				co->name, &base, len);
+			goto fail;
+		}
+	}
+
+	dev_set_name(h->dma_dev, "%s", co->name);
+	dma_set_coherent_mask(h->dma_dev, DMA_BIT_MASK(64));
+	h->name = co->name;
+	h->arg = arg;
+	h->base = base;
+	h->can_alloc = !!co->can_alloc;
+	h->is_ivm = co->is_ivm;
+	h->len = len;
+	h->peer = co->peer;
+	h->vm_id = co->vmid;
+	if (co->pm_ops.busy)
+		h->pm_ops.busy = co->pm_ops.busy;
+
+	if (co->pm_ops.idle)
+		h->pm_ops.idle = co->pm_ops.idle;
+
+	INIT_LIST_HEAD(&h->all_list);
+	mutex_init(&h->lock);
+	if (!co->no_cpu_access &&
+		nvmap_cache_maint_phys_range(NVMAP_CACHE_OP_WB_INV,
+						base, base + len)) {
+		dev_err(parent, "cache flush failed\n");
+		goto fail;
+	}
+	wmb();
+
+	if (co->disable_dynamic_dma_map)
+		nvmap_dev->dynamic_dma_map_mask &= ~co->usage_mask;
+
+	if (co->no_cpu_access)
+		nvmap_dev->cpu_access_mask &= ~co->usage_mask;
+
+	dev_info(parent, "created heap %s base 0x%p size (%zuKiB)\n",
+		co->name, (void *)(uintptr_t)base, len/1024);
+	return h;
+fail:
+	kfree(h);
+	return NULL;
+}
+
+/* nvmap_heap_destroy: frees all resources in heap */
+void nvmap_heap_destroy(struct nvmap_heap *heap)
+{
+	WARN_ON(!list_is_singular(&heap->all_list));
+	while (!list_empty(&heap->all_list)) {
+		struct list_block *l;
+		l = list_first_entry(&heap->all_list, struct list_block,
+				     all_list);
+		list_del(&l->all_list);
+		kmem_cache_free(heap_block_cache, l);
+	}
+	kfree(heap);
+}
+
+int nvmap_heap_init(void)
+{
+	ulong start_time = sched_clock();
+
+	heap_block_cache = KMEM_CACHE(list_block, 0);
+	if (!heap_block_cache) {
+		pr_err("%s: unable to create heap block cache\n", __func__);
+		return -ENOMEM;
+	}
+	pr_info("%s: created heap block cache\n", __func__);
+	nvmap_init_time += sched_clock() - start_time;
+	return 0;
+}
+
+void nvmap_heap_deinit(void)
+{
+	if (heap_block_cache)
+		kmem_cache_destroy(heap_block_cache);
+
+	heap_block_cache = NULL;
+}
+
+/*
+ * This routine is used to flush the carveout memory from cache.
+ * Why cache flush is needed for carveout? Consider the case, where a piece of
+ * carveout is allocated as cached and released. After this, if the same memory is
+ * allocated for uncached request and the memory is not flushed out from cache.
+ * In this case, the client might pass this to H/W engine and it could start modify
+ * the memory. As this was cached earlier, it might have some portion of it in cache.
+ * During cpu request to read/write other memory, the cached portion of this memory
+ * might get flushed back to main memory and would cause corruptions, if it happens
+ * after H/W writes data to memory.
+ *
+ * But flushing out the memory blindly on each carveout allocation is redundant.
+ *
+ * In order to optimize the carveout buffer cache flushes, the following
+ * strategy is used.
+ *
+ * The whole Carveout is flushed out from cache during its initialization.
+ * During allocation, carveout buffers are not flused from cache.
+ * During deallocation, carveout buffers are flushed, if they were allocated as cached.
+ * if they were allocated as uncached/writecombined, no cache flush is needed.
+ * Just draining store buffers is enough.
+ */
+int nvmap_flush_heap_block(struct nvmap_client *client,
+	struct nvmap_heap_block *block, size_t len, unsigned int prot)
+{
+	phys_addr_t phys = block->base;
+	phys_addr_t end = block->base + len;
+	int ret = 0;
+
+	if (prot == NVMAP_HANDLE_UNCACHEABLE || prot == NVMAP_HANDLE_WRITE_COMBINE)
+		goto out;
+
+	ret = nvmap_cache_maint_phys_range(NVMAP_CACHE_OP_WB_INV, phys, end);
+	if (ret)
+		goto out;
+out:
+	wmb();
+	return ret;
+}
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_heap.h b/drivers/video/tegra/nvmap/nv2/nvmap_heap.h
new file mode 100644
index 000000000000..bf09dd7e0528
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_heap.h
@@ -0,0 +1,59 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_heap.h
+ *
+ * GPU heap allocator.
+ *
+ * Copyright (c) 2010-2018, NVIDIA Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __NVMAP_HEAP_H
+#define __NVMAP_HEAP_H
+
+struct device;
+struct nvmap_heap;
+struct nvmap_client;
+
+struct nvmap_heap_block {
+	phys_addr_t	base;
+	unsigned int	type;
+	struct nvmap_handle *handle;
+};
+
+struct nvmap_platform_carveout;
+
+struct nvmap_heap *nvmap_heap_create(struct device *parent,
+				     const struct nvmap_platform_carveout *co,
+				     phys_addr_t base, size_t len, void *arg);
+
+void nvmap_heap_destroy(struct nvmap_heap *heap);
+
+struct nvmap_heap_block *nvmap_heap_alloc(struct nvmap_heap *heap,
+					  struct nvmap_handle *handle,
+					  phys_addr_t *start);
+
+struct nvmap_heap *nvmap_block_to_heap(struct nvmap_heap_block *b);
+
+void nvmap_heap_free(struct nvmap_heap_block *block);
+
+int __init nvmap_heap_init(void);
+
+void nvmap_heap_deinit(void);
+
+int nvmap_flush_heap_block(struct nvmap_client *client,
+	struct nvmap_heap_block *block, size_t len, unsigned int prot);
+
+void nvmap_heap_debugfs_init(struct dentry *heap_root, struct nvmap_heap *heap);
+
+int nvmap_query_heap_peer(struct nvmap_heap *heap);
+size_t nvmap_query_heap_size(struct nvmap_heap *heap);
+
+#endif
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_heap_alloc.c b/drivers/video/tegra/nvmap/nv2/nvmap_heap_alloc.c
new file mode 100644
index 000000000000..df68607d8fa9
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_heap_alloc.c
@@ -0,0 +1,361 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_alloc.c
+ *
+ * Handle allocation and freeing routines for nvmap
+ *
+ * Copyright (c) 2011-2017, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"%s: " fmt, __func__
+
+#include <linux/moduleparam.h>
+#include <trace/events/nvmap.h>
+
+#include <linux/version.h>
+
+#include "nvmap_heap_alloc.h"
+#include "nvmap_cache.h"
+#include "nvmap_misc.h"
+#include "nvmap_pp.h"
+#include "nvmap_dev.h"
+#include "nvmap_page_color.h"
+
+extern bool nvmap_convert_carveout_to_iovmm;
+extern bool nvmap_convert_iovmm_to_carveout;
+
+extern u32 nvmap_max_handle_count;
+extern u64 nvmap_big_page_allocs;
+extern u64 nvmap_total_page_allocs;
+
+/* small allocations will try to allocate from generic OS memory before
+ * any of the limited heaps, to increase the effective memory for graphics
+ * allocations, and to reduce fragmentation of the graphics heaps with
+ * sub-page splinters */
+static const unsigned int heap_policy_small[] = {
+	NVMAP_HEAP_CARVEOUT_VPR,
+	NVMAP_HEAP_CARVEOUT_IRAM,
+	NVMAP_HEAP_CARVEOUT_MASK,
+	NVMAP_HEAP_IOVMM,
+	0,
+};
+
+static const unsigned int heap_policy_large[] = {
+	NVMAP_HEAP_CARVEOUT_VPR,
+	NVMAP_HEAP_CARVEOUT_IRAM,
+	NVMAP_HEAP_IOVMM,
+	NVMAP_HEAP_CARVEOUT_MASK,
+	0,
+};
+
+static const unsigned int heap_policy_excl[] = {
+	NVMAP_HEAP_CARVEOUT_IVM,
+	NVMAP_HEAP_CARVEOUT_VIDMEM,
+	0,
+};
+
+/*
+ * set the gfp not to trigger direct/kswapd reclaims and
+ * not to use emergency reserves.
+ */
+static gfp_t nvmap_heap_big_pages_gfp(gfp_t gfp)
+{
+	return (gfp | __GFP_NOMEMALLOC) & ~__GFP_RECLAIM;
+}
+
+unsigned int nvmap_heap_type_conversion(unsigned int orig_heap)
+{
+	unsigned int type = orig_heap;
+	if (!nvmap_convert_carveout_to_iovmm
+			&& nvmap_convert_iovmm_to_carveout) {
+		if (type & NVMAP_HEAP_IOVMM) {
+			type &= ~NVMAP_HEAP_IOVMM;
+			type |= NVMAP_HEAP_CARVEOUT_GENERIC;
+		}
+	}
+	return type;
+}
+
+int nvmap_heap_type_is_carveout(unsigned int heap_type)
+{
+	unsigned int carveout_mask = NVMAP_HEAP_CARVEOUT_MASK;
+
+	if (nvmap_convert_carveout_to_iovmm) {
+		carveout_mask &= ~NVMAP_HEAP_CARVEOUT_GENERIC;
+	}
+	return (heap_type & carveout_mask) ? 1 : 0;
+}
+
+int nvmap_heap_type_is_iovmm(unsigned int heap_type)
+{
+	unsigned int iovmm_mask = NVMAP_HEAP_IOVMM;
+
+	if (nvmap_convert_carveout_to_iovmm) {
+		iovmm_mask |= NVMAP_HEAP_CARVEOUT_GENERIC;
+	}
+	return (heap_type & iovmm_mask) ? 1 : 0;
+}
+
+static struct device *heap_pgalloc_dev(unsigned long type)
+{
+	int ret = -EINVAL;
+	struct device *dma_dev;
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0)
+	ret = 0;
+#endif
+
+	if (ret || (type != NVMAP_HEAP_CARVEOUT_VPR))
+		return ERR_PTR(-EINVAL);
+
+	dma_dev = nvmap_heap_type_to_dev(type);
+	if (IS_ERR(dma_dev))
+		return dma_dev;
+
+	/* TODO: What consequences does this function have? */
+	ret = dma_set_resizable_heap_floor_size(dma_dev, 0);
+	if (ret)
+		return ERR_PTR(ret);
+	return dma_dev;
+}
+
+static int heap_big_pages_alloc_exact(struct page **pages, int starting_idx,
+					gfp_t gfp, int num_pages)
+{
+	struct page *page;
+	int idx;
+
+	page = nvmap_alloc_pages_exact(gfp,
+			num_pages << PAGE_SHIFT);
+	if (!page)
+		return -ENOMEM;
+
+	for (idx = 0; idx < num_pages; idx++)
+		pages[starting_idx + idx] = nth_page(page, idx);
+	nvmap_cache_clean_pages(&pages[starting_idx], num_pages);
+
+	return 0;
+}
+
+static int heap_big_pages_alloc(struct page **pages, int nr_page, gfp_t gfp)
+{
+	int page_index = 0;
+	int pages_per_big_pg = NVMAP_PP_BIG_PAGE_SIZE >> PAGE_SHIFT;
+	gfp_t gfp_no_reclaim = nvmap_heap_big_pages_gfp(gfp);
+	int err;
+
+#ifdef CONFIG_NVMAP_PAGE_POOLS
+	/* Get as many big pages from the pool as possible. */
+	page_index = nvmap_page_pool_alloc_lots_bp(&nvmap_dev->pool, pages,
+			nr_page);
+	pages_per_big_pg = nvmap_dev->pool.pages_per_big_pg;
+#endif
+	/* Try to allocate big pages from page allocator */
+	for (; page_index < nr_page; page_index += pages_per_big_pg) {
+
+		if (pages_per_big_pg < 1)
+			break;
+		if ((nr_page - page_index) < pages_per_big_pg)
+			break;
+
+		err = heap_big_pages_alloc_exact(pages, page_index,
+				gfp_no_reclaim, pages_per_big_pg);
+		if (err) {
+			break;
+		}
+	}
+
+	nvmap_big_page_allocs += page_index;
+
+	/* If we have page coloring then alloc the rest of pages colored */
+	if (nvmap_color_is_enabled() && page_index < nr_page) {
+		int err = nvmap_color_alloc(&nvmap_dev->pool,
+					nr_page - page_index,
+					&pages[page_index]);
+
+		if (err) {
+			while (page_index--)
+				__free_page(pages[page_index]);
+			return -1;
+		}
+
+		return nr_page;
+	}
+
+#ifdef CONFIG_NVMAP_PAGE_POOLS
+	/* Get as many 4K pages from the pool as possible. */
+	page_index += nvmap_page_pool_alloc_lots(&nvmap_dev->pool, &pages[page_index],
+			nr_page - page_index);
+#endif
+
+	return page_index;
+}
+
+struct page **nvmap_heap_alloc_iovmm_pages(size_t size, bool contiguous)
+{
+	int nr_page = size >> PAGE_SHIFT;
+	int i = 0, page_index = 0;
+	struct page **pages;
+	gfp_t gfp = GFP_NVMAP | __GFP_ZERO;
+
+	pages = nvmap_altalloc(nr_page * sizeof(*pages));
+	if (!pages)
+		return ERR_PTR(-ENOMEM);
+
+	if (contiguous) {
+		struct page *page;
+		page = nvmap_alloc_pages_exact(gfp, size);
+		if (!page)
+			goto fail;
+
+		for (i = 0; i < nr_page; i++)
+			pages[i] = nth_page(page, i);
+
+	} else {
+		page_index = heap_big_pages_alloc(pages, nr_page, gfp);
+		if (page_index < 0) {
+			i = 0;
+			goto fail;
+		}
+
+		for (i = page_index; i < nr_page; i++) {
+			pages[i] = nvmap_alloc_pages_exact(gfp, PAGE_SIZE);
+			if (!pages[i])
+				goto fail;
+		}
+		nvmap_total_page_allocs += nr_page;
+	}
+
+	/*
+	 * Make sure any data in the caches is cleaned out before
+	 * passing these pages to userspace. Many nvmap clients assume that
+	 * the buffers are clean as soon as they are allocated. nvmap
+	 * clients can pass the buffer to hardware as it is without any
+	 * explicit cache maintenance.
+	 */
+	if (page_index < nr_page)
+		nvmap_cache_clean_pages(&pages[page_index], nr_page - page_index);
+
+	return pages;
+
+fail:
+	while (i--)
+		__free_page(pages[i]);
+	nvmap_altfree(pages, nr_page * sizeof(*pages));
+	wmb();
+	return ERR_PTR(-ENOMEM);
+}
+
+struct page **nvmap_heap_alloc_dma_pages(size_t size, unsigned long type)
+{
+	struct page **pages;
+	struct device *dma_dev;
+	DEFINE_DMA_ATTRS(attrs);
+	dma_addr_t pa;
+
+	dma_dev = heap_pgalloc_dev(type);
+	if (IS_ERR(dma_dev))
+		return ERR_PTR(-EINVAL);
+
+	dma_set_attr(DMA_ATTR_ALLOC_EXACT_SIZE, __DMA_ATTR(attrs));
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0)
+	dma_set_attr(DMA_ATTR_ALLOC_SINGLE_PAGES, __DMA_ATTR(attrs));
+#endif
+
+	pages = dma_alloc_attrs(dma_dev, size, &pa,
+			GFP_KERNEL, __DMA_ATTR(attrs));
+	if (dma_mapping_error(dma_dev, pa))
+		return ERR_PTR(-ENOMEM);
+
+	return pages;
+}
+
+int nvmap_heap_type_is_dma(unsigned long type)
+{
+	struct device *dma_dev;
+
+	dma_dev = heap_pgalloc_dev(type);
+	if (IS_ERR(dma_dev))
+		return 0;
+	return 1;
+}
+
+void nvmap_heap_dealloc_dma_pages(size_t size, unsigned long type,
+				struct page **pages)
+{
+	struct device *dma_dev;
+	DEFINE_DMA_ATTRS(attrs);
+	dma_addr_t pa = ~(dma_addr_t)0;
+
+	dma_dev = heap_pgalloc_dev(type);
+	if (IS_ERR(dma_dev))
+		return;
+
+	dma_set_attr(DMA_ATTR_ALLOC_EXACT_SIZE, __DMA_ATTR(attrs));
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0)
+	dma_set_attr(DMA_ATTR_ALLOC_SINGLE_PAGES, __DMA_ATTR(attrs));
+#endif
+
+	dma_free_attrs(dma_dev, size, pages, pa,
+		       __DMA_ATTR(attrs));
+}
+
+struct page **nvmap_heap_alloc_from_va(size_t size, ulong vaddr)
+{
+	int nr_page = size >> PAGE_SHIFT;
+	struct page **pages;
+	int ret = 0;
+
+	pages = nvmap_altalloc(nr_page * sizeof(*pages));
+	if (IS_ERR_OR_NULL(pages))
+		return NULL;
+
+	ret = nvmap_get_user_pages(vaddr & PAGE_MASK, nr_page, pages);
+	if (ret) {
+		nvmap_altfree(pages, nr_page * sizeof(*pages));
+		return NULL;
+	}
+
+	nvmap_cache_clean_pages(&pages[0], nr_page);
+	return pages;
+}
+
+const unsigned int *nvmap_heap_mask_to_policy(unsigned int heap_mask, int nr_page)
+{
+	const unsigned int *alloc_policy;
+	int i;
+
+	bool alloc_from_excl = false;
+	/*
+	 * If user specifies one of the exclusive carveouts, allocation
+	 * from no other heap should be allowed.
+	 */
+	for (i = 0; i < ARRAY_SIZE(heap_policy_excl); i++) {
+		if (!(heap_mask & heap_policy_excl[i]))
+			continue;
+
+		if (heap_mask & ~(heap_policy_excl[i])) {
+			pr_err("%s alloc mixes exclusive heap %d and other heaps\n",
+			       current->group_leader->comm, heap_policy_excl[i]);
+			return NULL;
+		}
+		alloc_from_excl = true;
+	}
+
+	if (!heap_mask) {
+		return NULL;
+	}
+
+	alloc_policy = alloc_from_excl ? heap_policy_excl :
+			(nr_page == 1) ? heap_policy_small : heap_policy_large;
+	return alloc_policy;
+}
+
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_heap_alloc.h b/drivers/video/tegra/nvmap/nv2/nvmap_heap_alloc.h
new file mode 100644
index 000000000000..2975cfb666e2
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_heap_alloc.h
@@ -0,0 +1,37 @@
+/*
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __NVMAP_HEAP_ALLOC_H
+#define __NVMAP_HEAP_ALLOC_H
+
+#define GFP_NVMAP       (GFP_KERNEL | __GFP_HIGHMEM | __GFP_NOWARN)
+
+struct device *nvmap_heap_type_to_dev(unsigned long type);
+unsigned int nvmap_heap_type_conversion(unsigned int orig_heap);
+
+int nvmap_heap_type_is_carveout(unsigned int heap_type);
+int nvmap_heap_type_is_iovmm(unsigned int heap_type);
+int nvmap_heap_type_is_dma(unsigned long type);
+
+const unsigned int *nvmap_heap_mask_to_policy(unsigned int heap_mask, int nr_page);
+
+struct page **nvmap_heap_alloc_iovmm_pages(size_t size, bool contiguous);
+
+struct page **nvmap_heap_alloc_from_va(size_t size, ulong vaddr);
+
+struct page **nvmap_heap_alloc_dma_pages(size_t size, unsigned long type);
+void nvmap_heap_dealloc_dma_pages(size_t size, unsigned long type,
+				struct page **pages);
+
+
+#endif /* __NVMAP_HEAP_ALLOC_H */
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_helper.c b/drivers/video/tegra/nvmap/nv2/nvmap_helper.c
new file mode 100644
index 000000000000..beaafed75e07
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_helper.c
@@ -0,0 +1,111 @@
+/*
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/vmalloc.h>
+#include <linux/version.h>
+
+#include <trace/events/nvmap.h>
+
+#include "nvmap_misc.h"
+
+bool nvmap_convert_carveout_to_iovmm;
+bool nvmap_convert_iovmm_to_carveout;
+
+u32 nvmap_max_handle_count;
+
+/* handles may be arbitrarily large (16+MiB), and any handle allocated from
+ * the kernel (i.e., not a carveout handle) includes its array of pages. to
+ * preserve kmalloc space, if the array of pages exceeds PAGELIST_VMALLOC_MIN,
+ * the array is allocated using vmalloc. */
+#define PAGELIST_VMALLOC_MIN	(PAGE_SIZE)
+
+void *nvmap_altalloc(size_t len)
+{
+	if (len > PAGELIST_VMALLOC_MIN)
+		return vmalloc(len);
+	else
+		return kmalloc(len, GFP_KERNEL);
+}
+
+struct page **nvmap_alloc_pages(struct page **pg_pages, u32 nr_pages)
+{
+	struct page **pages;
+	int i;
+
+	pages = nvmap_altalloc(sizeof(*pages) * nr_pages);
+	if (!pages)
+		return NULL;
+
+	for (i = 0; i < nr_pages; i++)
+		pages[i] = nvmap_to_page(pg_pages[i]);
+
+	return pages;
+}
+
+struct page *nvmap_alloc_pages_exact(gfp_t gfp, size_t size)
+{
+	struct page *page, *p, *e;
+	unsigned int order;
+
+	order = get_order(size);
+	page = alloc_pages(gfp, order);
+
+	if (!page)
+		return NULL;
+
+	split_page(page, order);
+	e = nth_page(page, (1 << order));
+	for (p = nth_page(page, (size >> PAGE_SHIFT)); p < e; p++)
+		__free_page(p);
+
+	return page;
+}
+
+void nvmap_altfree(void *ptr, size_t len)
+{
+	if (!ptr)
+		return;
+
+	if (len > PAGELIST_VMALLOC_MIN)
+		vfree(ptr);
+	else
+		kfree(ptr);
+}
+
+int nvmap_get_user_pages(ulong vaddr, int nr_page, struct page **pages)
+{
+	int ret = 0;
+	int user_pages;
+	down_read(&current->mm->mmap_sem);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 9, 0)
+        user_pages = get_user_pages(current, current->mm,
+			      vaddr & PAGE_MASK, nr_page,
+			      1/*write*/, 1, /* force */
+			      pages, NULL);
+#else
+	user_pages = get_user_pages(vaddr & PAGE_MASK, nr_page,
+			      FOLL_WRITE | FOLL_FORCE,
+			      pages, NULL);
+#endif
+	up_read(&current->mm->mmap_sem);
+	if (user_pages != nr_page) {
+		ret = user_pages < 0 ? user_pages : -ENOMEM;
+		pr_err("get_user_pages requested/got: %d/%d]\n", nr_page,
+				user_pages);
+		while (--user_pages >= 0)
+			put_page(pages[user_pages]);
+	}
+	return ret;
+}
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_init.c b/drivers/video/tegra/nvmap/nv2/nvmap_init.c
new file mode 100644
index 000000000000..8d1d4d25c28d
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_init.c
@@ -0,0 +1,530 @@
+/*
+ * Copyright (c) 2014-2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt) "%s: " fmt, __func__
+
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/of_fdt.h>
+#include <linux/of_platform.h>
+#include <linux/nvmap.h>
+#include <linux/tegra-ivc.h>
+#include <linux/dma-contiguous.h>
+#include <linux/version.h>
+#include <linux/of_reserved_mem.h>
+#include <linux/platform_device.h>
+#include <linux/vmalloc.h>
+#include <linux/slab.h>
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+#include <linux/sched/clock.h>
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0)
+#include <linux/cma.h>
+#endif
+
+#include <asm/dma-contiguous.h>
+
+#include "iomap.h"
+#include "board.h"
+#include <linux/platform/tegra/common.h>
+
+#ifdef CONFIG_TEGRA_VIRTUALIZATION
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0)
+#include <soc/tegra/virt/syscalls.h>
+#else
+#include "../../../drivers/virt/tegra/syscalls.h"
+#endif
+#endif
+
+#include "nvmap_carveout.h"
+#include "nvmap_heap.h"
+#include "nvmap_dev.h"
+#include "nvmap_init.h"
+
+extern ulong nvmap_init_time;
+
+phys_addr_t __weak tegra_carveout_start;
+phys_addr_t __weak tegra_carveout_size;
+
+phys_addr_t __weak tegra_vpr_start;
+phys_addr_t __weak tegra_vpr_size;
+bool __weak tegra_vpr_resize;
+
+struct device __weak tegra_generic_dev;
+
+struct device __weak tegra_vpr_dev;
+EXPORT_SYMBOL(tegra_vpr_dev);
+
+struct device __weak tegra_iram_dev;
+struct device __weak tegra_generic_cma_dev;
+struct device __weak tegra_vpr_cma_dev;
+struct dma_resize_notifier_ops __weak vpr_dev_ops;
+
+__weak const struct of_device_id nvmap_of_ids[] = {
+	{ .compatible = "nvidia,carveouts" },
+	{ .compatible = "nvidia,carveouts-t18x" },
+	{ }
+};
+
+static struct dma_declare_info generic_dma_info = {
+	.name = "generic",
+	.size = 0,
+	.notifier.ops = NULL,
+};
+
+static struct dma_declare_info vpr_dma_info = {
+	.name = "vpr",
+	.size = SZ_32M,
+	.notifier.ops = &vpr_dev_ops,
+};
+
+static struct nvmap_platform_carveout nvmap_carveouts[] = {
+	[0] = {
+		.name		= "iram",
+		.usage_mask	= NVMAP_HEAP_CARVEOUT_IRAM,
+		.base		= 0,
+		.size		= 0,
+		.dma_dev	= &tegra_iram_dev,
+		.disable_dynamic_dma_map = true,
+	},
+	[1] = {
+		.name		= "generic-0",
+		.usage_mask	= NVMAP_HEAP_CARVEOUT_GENERIC,
+		.base		= 0,
+		.size		= 0,
+		.dma_dev	= &tegra_generic_dev,
+		.cma_dev	= &tegra_generic_cma_dev,
+		.dma_info	= &generic_dma_info,
+	},
+	[2] = {
+		.name		= "vpr",
+		.usage_mask	= NVMAP_HEAP_CARVEOUT_VPR,
+		.base		= 0,
+		.size		= 0,
+		.dma_dev	= &tegra_vpr_dev,
+		.cma_dev	= &tegra_vpr_cma_dev,
+		.dma_info	= &vpr_dma_info,
+		.enable_static_dma_map = true,
+	},
+	[3] = {
+		.name		= "vidmem",
+		.usage_mask	= NVMAP_HEAP_CARVEOUT_VIDMEM,
+		.base		= 0,
+		.size		= 0,
+		.disable_dynamic_dma_map = true,
+		.no_cpu_access = true,
+	},
+	/* Need uninitialized entries for IVM carveouts */
+	[4] = {
+		.name		= NULL,
+		.usage_mask	= NVMAP_HEAP_CARVEOUT_IVM,
+	},
+	[5] = {
+		.name		= NULL,
+		.usage_mask	= NVMAP_HEAP_CARVEOUT_IVM,
+	},
+	[6] = {
+		.name		= NULL,
+		.usage_mask	= NVMAP_HEAP_CARVEOUT_IVM,
+	},
+	[7] = {
+		.name		= NULL,
+		.usage_mask	= NVMAP_HEAP_CARVEOUT_IVM,
+	},
+};
+
+static struct nvmap_platform_data nvmap_data = {
+	.carveouts	= nvmap_carveouts,
+	.nr_carveouts	= 4,
+};
+
+static struct nvmap_platform_carveout *nvmap_get_carveout_pdata(const char *name)
+{
+	struct nvmap_platform_carveout *co;
+	for (co = nvmap_carveouts;
+	     co < nvmap_carveouts + ARRAY_SIZE(nvmap_carveouts); co++) {
+		int i = min_t(int, strcspn(name, "_"), strcspn(name, "-"));
+		/* handle IVM carveouts */
+		if ((co->usage_mask == NVMAP_HEAP_CARVEOUT_IVM) &&  !co->name)
+			goto found;
+
+		if (strncmp(co->name, name, i))
+			continue;
+found:
+		co->dma_dev = co->dma_dev ? co->dma_dev : &co->dev;
+		return co;
+	}
+	pr_err("not enough space for all nvmap carveouts\n");
+	return NULL;
+}
+
+int nvmap_register_vidmem_carveout(struct device *dma_dev,
+				phys_addr_t base, size_t size)
+{
+	struct nvmap_platform_carveout *vidmem_co;
+
+	if (!base || !size || (base != PAGE_ALIGN(base)) ||
+	    (size != PAGE_ALIGN(size)))
+		return -EINVAL;
+
+	vidmem_co = nvmap_get_carveout_pdata("vidmem");
+	if (!vidmem_co)
+		return -ENODEV;
+
+	if (vidmem_co->base || vidmem_co->size)
+		return -EEXIST;
+
+	vidmem_co->base = base;
+	vidmem_co->size = size;
+	if (dma_dev)
+		vidmem_co->dma_dev = dma_dev;
+	return nvmap_carveout_create(vidmem_co);
+}
+EXPORT_SYMBOL(nvmap_register_vidmem_carveout);
+
+#ifdef CONFIG_TEGRA_VIRTUALIZATION
+int __init nvmap_populate_ivm_carveout(struct reserved_mem *rmem)
+{
+	u32 id;
+	struct tegra_hv_ivm_cookie *ivm;
+	struct nvmap_platform_carveout *co;
+	unsigned int guestid;
+	unsigned long fdt_node = rmem->fdt_node;
+	const __be32 *prop;
+	int len;
+	char *name;
+	int ret = 0;
+
+	co = nvmap_get_carveout_pdata(rmem->name);
+	if (!co)
+		return -ENOMEM;
+
+	if (hyp_read_gid(&guestid)) {
+		pr_err("failed to read gid\n");
+		return -EINVAL;
+	}
+
+	prop = of_get_flat_dt_prop(fdt_node, "ivm", &len);
+	if (!prop) {
+		pr_err("failed to read ivm property\n");
+		return -EINVAL;
+	}
+
+	id = of_read_number(prop + 1, 1);
+	ivm = tegra_hv_mempool_reserve(id);
+	if (IS_ERR_OR_NULL(ivm)) {
+		pr_err("failed to reserve IVM memory pool %d\n", id);
+		return -ENOMEM;
+	}
+
+	/* XXX: Are these the available fields from IVM cookie? */
+	co->base     = (phys_addr_t)ivm->ipa;
+	co->peer     = ivm->peer_vmid;
+	co->size     = ivm->size;
+	co->vmid     = (int)guestid;
+
+	if (!co->base || !co->size) {
+		ret = -EINVAL;
+		goto fail;
+	}
+
+	/* See if this VM can allocate (or just create handle from ID)
+	 * generated by peer partition */
+	prop = of_get_flat_dt_prop(fdt_node, "alloc", &len);
+	if (!prop) {
+		pr_err("failed to read alloc property\n");
+		ret = -EINVAL;
+		goto fail;
+	}
+
+	name = kzalloc(32, GFP_KERNEL);
+	if (!name) {
+		ret = -ENOMEM;
+		goto fail;
+	}
+
+	co->can_alloc = of_read_number(prop, 1);
+
+	co->is_ivm    = true;
+
+	sprintf(name, "ivm%02d%02d%02d", co->vmid, co->peer, co->can_alloc);
+	pr_info("IVM carveout IPA:%p, size=%zu, peer vmid=%d, name=%s\n",
+		(void *)(uintptr_t)co->base, co->size, co->peer, name);
+
+	co->name      = name;
+	nvmap_data.nr_carveouts++;
+
+	return 0;
+
+fail:
+	co->base     = 0;
+	co->peer     = 0;
+	co->size     = 0;
+	co->vmid     = 0;
+	return ret;
+}
+#else
+int __init nvmap_populate_ivm_carveout(struct reserved_mem *rmem)
+{
+	return -EINVAL;
+}
+#endif
+
+static int __nvmap_init_legacy(struct device *dev);
+static int __nvmap_init_dt(struct platform_device *pdev)
+{
+	if (!of_match_device(nvmap_of_ids, &pdev->dev)) {
+		pr_err("Missing DT entry!\n");
+		return -EINVAL;
+	}
+
+	/* For VM_2 we need carveout. So, enabling it here */
+	__nvmap_init_legacy(&pdev->dev);
+
+	pdev->dev.platform_data = &nvmap_data;
+
+	return 0;
+}
+
+static int __init nvmap_co_device_init(struct reserved_mem *rmem,
+					struct device *dev)
+{
+	struct nvmap_platform_carveout *co = rmem->priv;
+	int err;
+
+	if (!co)
+		return -ENODEV;
+
+	if (co->usage_mask == NVMAP_HEAP_CARVEOUT_IVM)
+		return nvmap_populate_ivm_carveout(rmem);
+
+	/* if co size is 0, => co is not present. So, skip init. */
+	if (!co->size)
+		return 0;
+
+	if (!co->cma_dev) {
+		err = dma_declare_coherent_memory(co->dma_dev, 0,
+				co->base, co->size,
+				DMA_MEMORY_NOMAP | DMA_MEMORY_EXCLUSIVE);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+		if (!err) {
+#else
+		if (err & DMA_MEMORY_NOMAP) {
+#endif
+			dev_info(dev,
+				 "%s :dma coherent mem declare %pa,%zu\n",
+				 co->name, &co->base, co->size);
+			co->init_done = true;
+			err = 0;
+		} else
+			dev_err(dev,
+				"%s :dma coherent mem declare fail %pa,%zu,err:%d\n",
+				co->name, &co->base, co->size, err);
+	} else {
+		/*
+		 * When vpr memory is reserved, kmemleak tries to scan vpr
+		 * memory for pointers. vpr memory should not be accessed
+		 * from cpu so avoid scanning it. When vpr memory is removed,
+		 * the memblock_remove() API ensures that kmemleak won't scan
+		 * a removed block.
+		 */
+		if (!strncmp(co->name, "vpr", 3))
+			kmemleak_no_scan(__va(co->base));
+
+		co->dma_info->cma_dev = co->cma_dev;
+		err = dma_declare_coherent_resizable_cma_memory(
+				co->dma_dev, co->dma_info);
+		if (err)
+			dev_err(dev, "%s coherent memory declaration failed\n",
+				     co->name);
+		else
+			co->init_done = true;
+	}
+	return err;
+}
+
+static void nvmap_co_device_release(struct reserved_mem *rmem,struct device *dev)
+{
+	struct nvmap_platform_carveout *co = rmem->priv;
+
+	if (!co)
+		return;
+
+	if (co->usage_mask == NVMAP_HEAP_CARVEOUT_IVM)
+		kfree(co->name);
+}
+
+static const struct reserved_mem_ops nvmap_co_ops = {
+	.device_init	= nvmap_co_device_init,
+	.device_release	= nvmap_co_device_release,
+};
+
+int __init nvmap_co_setup(struct reserved_mem *rmem)
+{
+	struct nvmap_platform_carveout *co;
+	int ret = 0;
+	struct cma *cma;
+	ulong start = sched_clock();
+
+	co = nvmap_get_carveout_pdata(rmem->name);
+	if (!co)
+		return ret;
+
+	rmem->ops = &nvmap_co_ops;
+	rmem->priv = co;
+
+	/* IVM carveouts */
+	if (!co->name)
+		goto finish;
+
+	co->base = rmem->base;
+	co->size = rmem->size;
+
+	if (!of_get_flat_dt_prop(rmem->fdt_node, "reusable", NULL) ||
+	    of_get_flat_dt_prop(rmem->fdt_node, "no-map", NULL))
+		goto skip_cma;
+
+	WARN_ON(!rmem->base);
+	if (dev_get_cma_area(co->cma_dev)) {
+		pr_info("cma area initialed in legacy way already\n");
+		goto finish;
+	}
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	ret = cma_init_reserved_mem(rmem->base, rmem->size, 0,
+					rmem->name, &cma);
+#else
+	ret = cma_init_reserved_mem(rmem->base, rmem->size, 0, &cma);
+#endif
+	if (ret) {
+		pr_info("cma_init_reserved_mem fails for %s\n", rmem->name);
+		goto finish;
+	}
+
+	dma_contiguous_early_fixup(rmem->base, rmem->size);
+	dev_set_cma_area(co->cma_dev, cma);
+	pr_debug("tegra-carveouts carveout=%s %pa@%pa\n",
+		 rmem->name, &rmem->size, &rmem->base);
+	goto finish;
+
+skip_cma:
+	co->cma_dev = NULL;
+finish:
+	nvmap_init_time += sched_clock() - start;
+	return ret;
+}
+RESERVEDMEM_OF_DECLARE(nvmap_co, "nvidia,generic_carveout", nvmap_co_setup);
+RESERVEDMEM_OF_DECLARE(nvmap_ivm_co, "nvidia,ivm_carveout", nvmap_co_setup);
+RESERVEDMEM_OF_DECLARE(nvmap_iram_co, "nvidia,iram-carveout", nvmap_co_setup);
+RESERVEDMEM_OF_DECLARE(nvmap_vpr_co, "nvidia,vpr-carveout", nvmap_co_setup);
+
+/*
+ * This requires proper kernel arguments to have been passed.
+ */
+static int __nvmap_init_legacy(struct device *dev)
+{
+	/* Carveout. */
+	if (!nvmap_carveouts[1].base) {
+		nvmap_carveouts[1].base = tegra_carveout_start;
+		nvmap_carveouts[1].size = tegra_carveout_size;
+		if (!tegra_vpr_resize)
+			nvmap_carveouts[1].cma_dev = NULL;
+	}
+
+	/* VPR */
+	if (!nvmap_carveouts[2].base) {
+		nvmap_carveouts[2].base = tegra_vpr_start;
+		nvmap_carveouts[2].size = tegra_vpr_size;
+		if (!tegra_vpr_resize)
+			nvmap_carveouts[2].cma_dev = NULL;
+	}
+
+	return 0;
+}
+
+/*
+ * Fills in the platform data either from the device tree or with the
+ * legacy path.
+ */
+int __init nvmap_init(struct platform_device *pdev)
+{
+	int err;
+	struct reserved_mem rmem;
+
+	if (pdev->dev.of_node) {
+		err = __nvmap_init_dt(pdev);
+		if (err)
+			return err;
+	}
+
+	err = of_reserved_mem_device_init(&pdev->dev);
+	if (err)
+		pr_debug("reserved_mem_device_init fails, try legacy init\n");
+
+	/* try legacy init */
+	if (!nvmap_carveouts[1].init_done) {
+		rmem.priv = &nvmap_carveouts[1];
+		err = nvmap_co_device_init(&rmem, &pdev->dev);
+		if (err)
+			goto end;
+	}
+
+	if (!nvmap_carveouts[2].init_done) {
+		rmem.priv = &nvmap_carveouts[2];
+		err = nvmap_co_device_init(&rmem, &pdev->dev);
+	}
+
+end:
+	return err;
+}
+
+static struct platform_driver __refdata nvmap_driver = {
+	.probe		= nvmap_probe,
+	.remove		= nvmap_remove,
+
+	.driver = {
+		.name	= "tegra-carveouts",
+		.owner	= THIS_MODULE,
+		.of_match_table = nvmap_of_ids,
+		.probe_type = PROBE_PREFER_ASYNCHRONOUS,
+		.suppress_bind_attrs = true,
+	},
+};
+
+static int __init nvmap_init_driver(void)
+{
+	int e = 0;
+
+	e = nvmap_heap_init();
+	if (e)
+		goto fail;
+
+	e = platform_driver_register(&nvmap_driver);
+	if (e) {
+		nvmap_heap_deinit();
+		goto fail;
+	}
+
+fail:
+	return e;
+}
+fs_initcall(nvmap_init_driver);
+
+static void __exit nvmap_exit_driver(void)
+{
+	platform_driver_unregister(&nvmap_driver);
+	nvmap_heap_deinit();
+	nvmap_dev = NULL;
+}
+module_exit(nvmap_exit_driver);
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_init.h b/drivers/video/tegra/nvmap/nv2/nvmap_init.h
new file mode 100644
index 000000000000..979ba64b83a6
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_init.h
@@ -0,0 +1,19 @@
+/*
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __NVMAP_INIT_H
+#define __NVMAP_INIT_H
+
+int __init nvmap_co_setup(struct reserved_mem *rmem);
+
+#endif /* __NVMAP_INIT_H */
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_init_t19x.c b/drivers/video/tegra/nvmap/nv2/nvmap_init_t19x.c
new file mode 100644
index 000000000000..af2eb29b6b65
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_init_t19x.c
@@ -0,0 +1,316 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_init_t19x.c
+ *
+ * Copyright (c) 2016-2021, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"nvmap: %s() " fmt, __func__
+
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/of_reserved_mem.h>
+#include <linux/platform_device.h>
+#include <linux/nvmap_t19x.h>
+#include <linux/dma-mapping.h>
+#include <linux/dma-direction.h>
+#include <linux/nvmap.h>
+#include <linux/vmalloc.h>
+#include <linux/slab.h>
+
+#include <linux/version.h>
+
+#include "nvmap_carveout.h"
+#include "nvmap_dev.h"
+#include "nvmap_init.h"
+
+bool nvmap_version_t19x;
+
+const struct of_device_id nvmap_of_ids[] = {
+	{ .compatible = "nvidia,carveouts" },
+	{ .compatible = "nvidia,carveouts-t18x" },
+	{ .compatible = "nvidia,carveouts-t19x" },
+	{ }
+};
+
+int nvmap_register_cvsram_carveout(struct device *dma_dev,
+		phys_addr_t base, size_t size, int (*busy)(void),
+		int (*idle)(void))
+{
+	static struct nvmap_platform_carveout cvsram = {
+		.name = "cvsram",
+		.usage_mask = NVMAP_HEAP_CARVEOUT_CVSRAM,
+		.disable_dynamic_dma_map = true,
+		.no_cpu_access = true,
+	};
+
+	cvsram.pm_ops.busy = busy;
+	cvsram.pm_ops.idle = idle;
+
+	if (!base || !size || (base != PAGE_ALIGN(base)) ||
+	    (size != PAGE_ALIGN(size)))
+		return -EINVAL;
+	cvsram.base = base;
+	cvsram.size = size;
+
+	cvsram.dma_dev = &cvsram.dev;
+	return nvmap_carveout_create(&cvsram);
+}
+EXPORT_SYMBOL(nvmap_register_cvsram_carveout);
+
+static struct nvmap_platform_carveout gosmem = {
+	.name = "gosmem",
+	.usage_mask = NVMAP_HEAP_CARVEOUT_GOS,
+};
+
+static struct cv_dev_info *cvdev_info;
+static int count = 0;
+
+static void nvmap_gosmem_device_release(struct reserved_mem *rmem,
+		struct device *dev)
+{
+	int i;
+	struct reserved_mem_ops *rmem_ops =
+		(struct reserved_mem_ops *)rmem->ops;
+
+	for (i = 0; i < count; i++)
+		of_node_put(cvdev_info[i].np);
+	kfree(cvdev_info);
+	rmem_ops->device_release(rmem, dev);
+}
+
+static int __init nvmap_gosmem_device_init(struct reserved_mem *rmem,
+		struct device *dev)
+{
+	struct of_phandle_args outargs;
+	struct device_node *np;
+	DEFINE_DMA_ATTRS(attrs);
+	dma_addr_t dma_addr;
+	void *cpu_addr;
+	int ret = 0, i, idx, bytes;
+	struct reserved_mem_ops *rmem_ops =
+		(struct reserved_mem_ops *)rmem->priv;
+	struct sg_table *sgt;
+
+	dma_set_attr(DMA_ATTR_ALLOC_EXACT_SIZE, __DMA_ATTR(attrs));
+
+	np = of_find_node_by_phandle(rmem->phandle);
+	if (!np) {
+		pr_err("Can't find the node using compatible\n");
+		return -ENODEV;
+	}
+
+	if (count) {
+		pr_err("Gosmem initialized already\n");
+		return -EBUSY;
+	}
+
+	count = of_count_phandle_with_args(np, "cvdevs", NULL);
+	if (!count) {
+		pr_err("No cvdevs to use the gosmem!!\n");
+		return -EINVAL;
+	}
+
+	cpu_addr = dma_alloc_coherent(gosmem.dma_dev, count * SZ_4K,
+				&dma_addr, GFP_KERNEL);
+	if (!cpu_addr) {
+		pr_err("Failed to allocate from Gos mem carveout\n");
+		return -ENOMEM;
+	}
+
+	bytes = sizeof(*cvdev_info) * count;
+	bytes += sizeof(struct sg_table) * count * count;
+	cvdev_info = kzalloc(bytes, GFP_KERNEL);
+	if (!cvdev_info) {
+		pr_err("kzalloc failed. No memory!!!\n");
+		ret = -ENOMEM;
+		goto unmap_dma;
+	}
+
+	for (idx = 0; idx < count; idx++) {
+		struct device_node *temp;
+
+		ret = of_parse_phandle_with_args(np, "cvdevs",
+			NULL, idx, &outargs);
+		if (ret < 0) {
+			/* skip empty (null) phandles */
+			if (ret == -ENOENT)
+				continue;
+			else
+				goto free_cvdev;
+		}
+		temp = outargs.np;
+
+		cvdev_info[idx].np = of_node_get(temp);
+		if (!cvdev_info[idx].np)
+			continue;
+		cvdev_info[idx].count = count;
+		cvdev_info[idx].idx = idx;
+		cvdev_info[idx].sgt =
+			(struct sg_table *)(cvdev_info + count);
+		cvdev_info[idx].sgt += idx * count;
+		cvdev_info[idx].cpu_addr = cpu_addr + idx * SZ_4K;
+
+		for (i = 0; i < count; i++) {
+			sgt = cvdev_info[idx].sgt + i;
+
+			ret = sg_alloc_table(sgt, 1, GFP_KERNEL);
+			if (ret) {
+				pr_err("sg_alloc_table failed:%d\n", ret);
+				goto free;
+			}
+			sg_set_page(sgt->sgl, virt_to_page(cpu_addr + i * SZ_4K),
+					      SZ_4K,
+					      offset_in_page(cpu_addr + i * SZ_4K));
+		}
+	}
+	rmem->priv = &gosmem;
+	ret = rmem_ops->device_init(rmem, dev);
+	if (ret)
+		goto free;
+	return ret;
+free:
+	sgt = (struct sg_table *)(cvdev_info + count);
+	for (i = 0; i < count * count; i++)
+		sg_free_table(sgt++);
+free_cvdev:
+	kfree(cvdev_info);
+unmap_dma:
+	dma_free_coherent(gosmem.dma_dev, count * SZ_4K, cpu_addr, dma_addr);
+	return ret;
+}
+
+static struct reserved_mem_ops gosmem_rmem_ops = {
+	.device_init = nvmap_gosmem_device_init,
+	.device_release = nvmap_gosmem_device_release,
+};
+
+static int __init nvmap_gosmem_setup(struct reserved_mem *rmem)
+{
+	int ret;
+
+	rmem->priv = &gosmem;
+	ret = nvmap_co_setup(rmem);
+	if (ret)
+		return ret;
+
+	rmem->priv = (struct reserved_mem_ops *)rmem->ops;
+	rmem->ops = &gosmem_rmem_ops;
+	return 0;
+}
+RESERVEDMEM_OF_DECLARE(nvmap_co, "nvidia,gosmem", nvmap_gosmem_setup);
+
+struct cv_dev_info *nvmap_fetch_cv_dev_info(struct device *dev);
+
+static int nvmap_gosmem_notifier(struct notifier_block *nb,
+		unsigned long event, void *_dev)
+{
+	struct device *dev = _dev;
+	int ents, i, ret;
+	struct cv_dev_info *gos_owner;
+
+	if ((event != BUS_NOTIFY_BOUND_DRIVER) &&
+		(event != BUS_NOTIFY_UNBIND_DRIVER))
+		return NOTIFY_DONE;
+
+	if ((event == BUS_NOTIFY_BOUND_DRIVER) &&
+		nvmap_dev && (dev == nvmap_dev->dev_user.parent)) {
+		struct of_device_id nvmap_t19x_of_ids[] = {
+			{.compatible = "nvidia,carveouts-t19x"},
+			{ }
+		};
+
+		/*
+		 * user space IOCTL and dmabuf ops happen much later in boot
+		 * flow. So, setting the version here to ensure all of those
+		 * callbacks can safely query the proper version of nvmap
+		 */
+		if (of_match_node((struct of_device_id *)&nvmap_t19x_of_ids,
+				dev->of_node)) {
+			nvmap_version_t19x = 1;
+			/* FIX ME: Update correct value after evaluation */
+			nvmap_cache_maint_by_set_ways = 0;
+			cache_maint_inner_threshold = SZ_2M;
+		}
+		return NOTIFY_DONE;
+	}
+
+	gos_owner = nvmap_fetch_cv_dev_info(dev);
+	if (!gos_owner)
+		return NOTIFY_DONE;
+
+	ret = _dma_declare_coherent_memory(&gos_owner->offset_dev, 0, 0, SZ_256,
+			ffs(sizeof(u32)) - ffs(sizeof(u8)), DMA_MEMORY_NOMAP);
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	if (ret) {
+#else
+	if (!(ret & DMA_MEMORY_NOMAP)) {
+#endif
+		dev_err(dev, "declare coherent memory for gosmem chunk failed\n");
+		return NOTIFY_DONE;
+	}
+
+	for (i = 0; i < count; i++) {
+		DEFINE_DMA_ATTRS(attrs);
+		enum dma_data_direction dir;
+
+		dir = DMA_BIDIRECTIONAL;
+		dma_set_attr(DMA_ATTR_SKIP_IOVA_GAP, __DMA_ATTR(attrs));
+		if (cvdev_info[i].np != dev->of_node) {
+			dma_set_attr(DMA_ATTR_READ_ONLY, __DMA_ATTR(attrs));
+			dir = DMA_TO_DEVICE;
+		}
+
+		switch (event) {
+		case BUS_NOTIFY_BOUND_DRIVER:
+			ents = dma_map_sg_attrs(dev, gos_owner->sgt[i].sgl,
+					gos_owner->sgt[i].nents, dir, __DMA_ATTR(attrs));
+			if (ents != 1) {
+				pr_err("mapping gosmem chunk %d for %s failed\n",
+					i, dev_name(dev));
+				return NOTIFY_DONE;
+			}
+			break;
+		case BUS_NOTIFY_UNBIND_DRIVER:
+			dma_unmap_sg_attrs(dev, gos_owner->sgt[i].sgl,
+					gos_owner->sgt[i].nents, dir, __DMA_ATTR(attrs));
+			break;
+		default:
+			return NOTIFY_DONE;
+		};
+	}
+	return NOTIFY_DONE;
+}
+
+static struct notifier_block nvmap_gosmem_nb = {
+	.notifier_call = nvmap_gosmem_notifier,
+};
+
+static int nvmap_t19x_init(void)
+{
+	return bus_register_notifier(&platform_bus_type,
+			&nvmap_gosmem_nb);
+}
+core_initcall(nvmap_t19x_init);
+
+struct cv_dev_info *nvmap_fetch_cv_dev_info(struct device *dev)
+{
+	int i;
+
+	if (!dev || !cvdev_info || !dev->of_node)
+		return NULL;
+
+	for (i = 0; i < count; i++)
+		if (cvdev_info[i].np == dev->of_node)
+			return &cvdev_info[i];
+	return NULL;
+}
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_ioctl.c b/drivers/video/tegra/nvmap/nv2/nvmap_ioctl.c
new file mode 100644
index 000000000000..4f81f03e2288
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_ioctl.c
@@ -0,0 +1,920 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_ioctl.c
+ *
+ * User-space interface to nvmap
+ *
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"nvmap: %s() " fmt, __func__
+
+#include <linux/dma-mapping.h>
+#include <linux/export.h>
+#include <linux/fs.h>
+#include <linux/io.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/nvmap.h>
+#include <linux/vmalloc.h>
+#include <linux/highmem.h>
+#include <linux/syscalls.h>
+
+#include <asm/io.h>
+#include <asm/memory.h>
+#include <asm/uaccess.h>
+#include <soc/tegra/common.h>
+#include <trace/events/nvmap.h>
+
+#include "nvmap_heap.h"
+#include "nvmap_stats.h"
+
+#include "nvmap_structs.h"
+#include "nvmap_ioctl.h"
+#include "nvmap_ioctl.h"
+
+#include "nvmap_dmabuf.h"
+#include "nvmap_client.h"
+#include "nvmap_handle.h"
+#include "nvmap_carveout.h"
+#include "nvmap_dev.h"
+#include "nvmap_tag.h"
+#include "nvmap_misc.h"
+#include "nvmap_vma.h"
+
+struct nvmap_carveout_node;
+
+/* TODO: Remove this */
+extern struct device tegra_vpr_dev;
+
+int ioctl_create_handle_from_fd(struct nvmap_client *client,
+				int orig_fd)
+{
+	struct nvmap_handle *handle = NULL;
+	int fd;
+	int err;
+
+	handle = nvmap_handle_from_fd(orig_fd);
+	/* If we can't get a handle, then we create a new
+	 * FD for the non-nvmap buffer
+	 */
+	if (IS_ERR(handle)) {
+		struct dma_buf *dmabuf = dma_buf_get(orig_fd);
+
+		if (IS_ERR(dmabuf) || !dmabuf) {
+			return -1;
+		}
+
+		if (nvmap_dmabuf_is_nvmap(dmabuf)) {
+			return -1;
+		}
+
+		fd = nvmap_client_create_fd(client);
+		nvmap_dmabuf_install_fd(dmabuf, fd);
+
+		get_dma_buf(dmabuf);
+		return fd;
+	}
+
+	err = nvmap_client_add_handle(client, handle);
+	if (err) {
+		return -1;
+	}
+
+	fd  = nvmap_client_create_fd(client);
+	if (fd < 0) {
+		nvmap_client_remove_handle(client, handle);
+		return -1;
+	}
+	nvmap_handle_install_fd(handle, fd);
+
+	return fd;
+}
+
+int nvmap_ioctl_create(struct file *filp, unsigned int cmd, void __user *arg)
+{
+	struct nvmap_create_handle op;
+	struct nvmap_client *client = filp->private_data;
+	int err = 0;
+	int fd;
+
+	if (!client) {
+		return -ENODEV;
+	}
+
+	if (copy_from_user(&op, arg, sizeof(op))) {
+		return -EFAULT;
+	}
+
+	if (cmd == NVMAP_IOC_CREATE) {
+		op.size64 = op.size;
+	}
+
+	if ((cmd == NVMAP_IOC_CREATE) || (cmd == NVMAP_IOC_CREATE_64)) {
+		fd = nvmap_client_create_handle(client, op.size64);
+	} else if (cmd == NVMAP_IOC_FROM_FD) {
+		fd = ioctl_create_handle_from_fd(client, op.fd);
+	} else {
+		return -EFAULT;
+	}
+
+	if (fd < 0)
+		return -EFAULT;
+
+	if (cmd == NVMAP_IOC_CREATE_64) {
+		op.handle64 = fd;
+	} else {
+		op.handle = fd;
+	}
+
+	err = copy_to_user(arg, &op, sizeof(op));
+	if (err) {
+		err = -EFAULT;
+		goto failed;
+	}
+
+	return 0;
+failed:
+	// TODO: Find a way to free the handle here
+	// Needs to make sure it covers case where FD wasn't nvmap fd
+	pr_warn("Need to free handle here!\n");
+	return err;
+}
+
+int nvmap_ioctl_getfd(struct file *filp, void __user *arg)
+{
+	struct nvmap_create_handle op;
+	struct nvmap_handle *handle;
+	struct nvmap_client *client = filp->private_data;
+	int fd;
+
+	if (copy_from_user(&op, arg, sizeof(op)))
+		return -EFAULT;
+
+	handle = nvmap_handle_from_fd(op.handle);
+	/* If there's no handle install a non-nvmap dmabuf fd */
+	// TODO: Clean this up
+	if (IS_ERR(handle)) {
+		struct dma_buf *dmabuf;
+
+		dmabuf = dma_buf_get(op.handle);
+		if (IS_ERR(dmabuf))
+			return PTR_ERR(dmabuf);
+
+		fd = nvmap_client_create_fd(client);
+		if (fd < 0)
+			return fd;
+		nvmap_dmabuf_install_fd(dmabuf, fd);
+	} else {
+
+		fd = nvmap_client_create_fd(client);
+		if (fd < 0)
+			return fd;
+
+		nvmap_handle_install_fd(handle, fd);
+	}
+
+	op.fd = fd;
+	if (copy_to_user(arg, &op, sizeof(op))) {
+		put_unused_fd(op.fd);
+		return -EFAULT;
+	}
+
+	return 0;
+}
+
+static int vaddr_and_size_are_in_vma(ulong vaddr, size_t size,
+					struct vm_area_struct *vma)
+{
+	if (!vma) {
+		return 0;
+	}
+
+	if (vaddr >= vma->vm_start && vaddr + size <= vma->vm_end) {
+		return 1;
+	} else {
+		return 0;
+	}
+}
+
+int nvmap_ioctl_create_from_va(struct file *filp, void __user *arg)
+{
+	int fd;
+	int err;
+	struct nvmap_create_handle_from_va op;
+	struct nvmap_handle *handle = NULL;
+	struct nvmap_client *client = filp->private_data;
+	struct vm_area_struct *vma;
+	size_t size;
+
+	if (copy_from_user(&op, arg, sizeof(op)))
+		return -EFAULT;
+
+	/* don't allow non-page aligned addresses. */
+	if (op.va & ~PAGE_MASK)
+		return -EINVAL;
+
+	if (!client)
+		return -ENODEV;
+
+	vma = find_vma(current->mm, op.va);
+	size = (op.size) ? op.size: op.size64;
+
+	if (!vaddr_and_size_are_in_vma(op.va, size, vma)) {
+		return -EINVAL;
+	}
+
+	if (!size) {
+		size = vma->vm_end - op.va;
+	}
+
+	fd = nvmap_client_create_handle(client, size);
+	if (fd < 0) {
+		return -EFAULT;
+	}
+
+	handle = nvmap_handle_from_fd(fd);
+	if (IS_ERR_OR_NULL(handle)) {
+		return -EFAULT;
+	}
+
+	nvmap_client_warn_if_no_tag(client, op.flags);
+	err = nvmap_handle_alloc_from_va(handle, op.va, op.flags);
+
+	NVMAP_TAG_TRACE(trace_nvmap_alloc_handle_done,
+			NVMAP_TP_ARGS_CHR(client, handle, NULL));
+	if (err) {
+		nvmap_ioctl_free(filp, fd);
+		return err;
+	}
+
+	op.handle = fd;
+	if (copy_to_user(arg, &op, sizeof(op))) {
+		nvmap_ioctl_free(filp, fd);
+		return err;
+	}
+
+	return 0;
+}
+
+int nvmap_ioctl_free(struct file *filp, unsigned long fd)
+{
+	struct nvmap_client *client = filp->private_data;
+	struct nvmap_handle *handle;
+
+	if (!fd)
+		return 0;
+
+	handle  = nvmap_handle_from_fd(fd);
+	if (!IS_ERR_OR_NULL(handle)) {
+		nvmap_client_remove_handle(client, handle);
+	}
+
+	return sys_close(fd);
+}
+
+int nvmap_ioctl_alloc(struct file *filp, unsigned int cmd, void __user *arg)
+{
+	struct nvmap_client *client = filp->private_data;
+	struct nvmap_handle *h;
+	unsigned int heap_mask;
+	unsigned int align;
+	unsigned int flags;
+	int peer;
+	int err;
+	int handle;
+
+	if (cmd == NVMAP_IOC_ALLOC) {
+		struct nvmap_alloc_handle op;
+
+		if (copy_from_user(&op, arg, sizeof(op)))
+			return -EFAULT;
+
+		peer = NVMAP_IVM_INVALID_PEER;
+		align = op.align;
+		heap_mask = op.heap_mask;
+		handle = op.handle;
+		flags = op.flags;
+	} else if (cmd == NVMAP_IOC_ALLOC_IVM) {
+		struct nvmap_alloc_ivm_handle op;
+
+		if (copy_from_user(&op, arg, sizeof(op)))
+			return -EFAULT;
+		peer = op.peer;
+
+		align = op.align;
+		heap_mask = op.heap_mask;
+		handle = op.handle;
+		flags = op.flags;
+	} else {
+		return -EINVAL;
+	}
+
+	if (align & (align - 1))
+		return -EINVAL;
+
+	h = nvmap_handle_from_fd(handle);
+	if (!h)
+		return -EINVAL;
+	h = nvmap_handle_get(h);
+	if (!h)
+		return -EINVAL;
+	if (nvmap_handle_is_allocated(h)) {
+		nvmap_handle_put(h);
+		return -EEXIST;
+	}
+
+
+	/* user-space handles are aligned to page boundaries, to prevent
+	 * data leakage. */
+	align = max_t(size_t, align, PAGE_SIZE);
+
+	nvmap_client_warn_if_no_tag(client, flags);
+
+	err = nvmap_handle_alloc(h, heap_mask, align,
+				  0, /* no kind */
+				  flags & (~NVMAP_HANDLE_KIND_SPECIFIED),
+				  peer);
+
+	if (nvmap_handle_is_allocated(h)) {
+		size_t size = nvmap_handle_size(h);
+
+		nvmap_stats_inc(NS_TOTAL, size);
+		nvmap_stats_inc(NS_ALLOC, size);
+
+		nvmap_client_stats_alloc(client, size);
+
+		NVMAP_TAG_TRACE(trace_nvmap_alloc_handle_done,
+			NVMAP_TP_ARGS_CHR(client, h, NULL));
+		err = 0;
+	}
+
+	nvmap_handle_put(h);
+	return err;
+}
+
+struct nvmap_handle *nvmap_try_duplicate_by_ivmid(
+		struct nvmap_client *client, u64 ivm_id)
+{
+	struct nvmap_handle *handle = NULL;
+
+	handle = nvmap_handle_from_ivmid(ivm_id);
+	handle = nvmap_handle_get(handle);
+	if (!handle)
+		return NULL;
+
+	nvmap_client_add_handle(client, handle);
+
+	return handle;
+}
+
+static int ioctl_alloc_handle_by_ivmid(struct nvmap_client *client, u64 ivm_id)
+{
+	struct nvmap_handle *handle = NULL;
+	size_t size = 0;
+	int err;
+	int fd;
+
+	size = nvmap_ivmid_to_size(ivm_id);
+
+	fd = nvmap_client_create_handle(client, size);
+	if (fd < 0) {
+		return -1;
+	}
+
+	handle = nvmap_handle_from_fd(fd);
+	if (!handle) {
+		return -1;
+	}
+
+	err = nvmap_handle_alloc_from_ivmid(handle, ivm_id);
+	if (!err) {
+		// TODO: Make sure client create/remove frees handle
+		nvmap_client_remove_handle(client, handle);
+		return -1;
+	}
+	NVMAP_TAG_TRACE(trace_nvmap_alloc_handle_done,
+			NVMAP_TP_ARGS_CHR(NULL, handle, NULL));
+	return fd;
+
+}
+
+static int ioctl_handle_from_ivmid(struct nvmap_client *client, int ivm_id)
+{
+	struct nvmap_handle *handle;
+	int fd;
+
+	handle = nvmap_handle_from_ivmid(ivm_id);
+	if (!handle)
+		return -1;
+
+	fd = nvmap_client_create_fd(client);
+	if (fd >= 0)
+		nvmap_handle_install_fd(handle, fd);
+
+	return fd;
+}
+
+int nvmap_ioctl_create_from_ivc(struct file *filp, void __user *arg)
+{
+	struct nvmap_create_handle op;
+	struct nvmap_client *client = filp->private_data;
+	int fd;
+	int err = 0;
+
+	/* First create a new handle and then fake carveout allocation */
+	if (copy_from_user(&op, arg, sizeof(op)))
+		return -EFAULT;
+
+	if (!client)
+		return -ENODEV;
+
+	fd = ioctl_handle_from_ivmid(client, op.ivm_id);
+	if (fd < 0) {
+		fd = ioctl_alloc_handle_by_ivmid(client, op.ivm_id);
+	}
+
+	if (fd < 0) {
+		err = -1;
+		goto fail;
+	}
+
+	op.ivm_handle = fd;
+
+	if (copy_to_user(arg, &op, sizeof(op))) {
+		err = -EFAULT;
+		// TODO: What do we do here to free
+		goto fail;
+	}
+
+	return err;
+
+fail:
+	// TODO: Find a way to free the handle here
+	// Needs to make sure it covers case where FD wasn't nvmap fd
+	pr_warn("Need to free handle here!\n");
+	return err;
+}
+
+int nvmap_ioctl_vpr_floor_size(struct file *filp, void __user *arg)
+{
+	int err=0;
+	u32 floor_size;
+
+	if (copy_from_user(&floor_size, arg, sizeof(floor_size)))
+		return -EFAULT;
+
+	err = dma_set_resizable_heap_floor_size(&tegra_vpr_dev, floor_size);
+	return err;
+}
+
+int nvmap_ioctl_get_ivc_heap(struct file *filp, void __user *arg)
+{
+	struct nvmap_device *dev = nvmap_dev;
+	int i;
+	unsigned int heap_mask = 0;
+
+	for (i = 0; i < dev->nr_carveouts; i++) {
+		struct nvmap_carveout_node *co_heap = nvmap_dev_to_carveout(dev, i);
+		int peer;
+
+		if (!nvmap_carveout_is_ivm(co_heap))
+			continue;
+
+		peer = nvmap_carveout_query_peer(co_heap);
+		if (peer < 0)
+			return -EINVAL;
+
+		heap_mask |= BIT(peer);
+	}
+
+	if (copy_to_user(arg, &heap_mask, sizeof(heap_mask)))
+		return -EFAULT;
+
+	return 0;
+}
+
+int nvmap_ioctl_get_ivcid(struct file *filp, void __user *arg)
+{
+	struct nvmap_create_handle op;
+	struct nvmap_handle *h = NULL;
+
+	if (copy_from_user(&op, arg, sizeof(op))) {
+		return -EFAULT;
+	}
+
+	h = nvmap_handle_from_fd(op.ivm_handle);
+	if (!h) {
+		return -EINVAL;
+	}
+
+	if (!nvmap_handle_is_allocated(h)) {
+		return -EFAULT;
+	}
+
+	op.ivm_id = nvmap_handle_ivm_id(h);
+
+	return copy_to_user(arg, &op, sizeof(op)) ? -EFAULT : 0;
+}
+
+static int ioctl_cache_maint_copy_op_from_user(void __user *arg,
+				struct nvmap_cache_op_64 *op64, int op_size)
+{
+#ifdef CONFIG_COMPAT
+	if (op_size == sizeof(struct nvmap_cache_op_32)) {
+		struct nvmap_cache_op_32 op32;
+
+		if (copy_from_user(&op32, arg, sizeof(op32)))
+			return -EFAULT;
+
+		op64->addr = op32.addr;
+		op64->handle = op32.handle;
+		op64->len = op32.len;
+		op64->op = op32.op;
+		return 0;
+	}
+#endif
+	if (op_size == sizeof(struct nvmap_cache_op)) {
+		struct nvmap_cache_op op;
+
+		if (copy_from_user(&op, arg, sizeof(op)))
+			return -EFAULT;
+		op64->addr = op.addr;
+		op64->handle = op.handle;
+		op64->len = op.len;
+		op64->op = op.op;
+		return 0;
+	}
+
+	if (copy_from_user(&op64, arg, sizeof(op64)))
+		return -EFAULT;
+	return 0;
+}
+int nvmap_ioctl_cache_maint(struct file *filp, void __user *arg, int op_size)
+{
+	struct vm_area_struct *vma;
+	struct nvmap_handle *handle;
+	struct nvmap_cache_op_64 op;
+	int ret = 0;
+	unsigned long start;
+	unsigned long end;
+
+	ret = ioctl_cache_maint_copy_op_from_user(arg, &op, op_size);
+	if (ret)
+		return ret;
+
+	handle = nvmap_handle_from_fd(op.handle);
+	handle = nvmap_handle_get(handle);
+	if (!handle)
+		return -EINVAL;
+
+	down_read(&current->mm->mmap_sem);
+
+	vma = find_vma(current->mm, (unsigned long) op.addr);
+	if (!nvmap_vma_is_nvmap(vma) ||
+			!vaddr_and_size_are_in_vma(op.addr, op.len, vma)) {
+		ret = -EADDRNOTAVAIL;
+		goto out;
+	}
+
+	if (!nvmap_handle_owns_vma(handle, vma)) {
+		ret = -EFAULT;
+		goto out;
+	}
+
+	start = (unsigned long)op.addr - vma->vm_start +
+		(vma->vm_pgoff << PAGE_SHIFT);
+	end = start + op.len;
+
+	ret = nvmap_handle_cache_maint(handle, start, end, op.op);
+
+
+out:
+	up_read(&current->mm->mmap_sem);
+	nvmap_handle_put(handle);
+	return ret;
+}
+
+/*
+ * This function copies the rw_handle arguments into op.
+ * Returns 0 if passing, err if failing
+ */
+static int ioctl_rw_handle_copy_op_from_user(void __user *arg,
+				struct nvmap_rw_handle *op, int op_size)
+{
+	struct nvmap_rw_handle_64 op64;
+#ifdef CONFIG_COMPAT
+	struct nvmap_rw_handle_32 op32;
+#endif
+
+#ifdef CONFIG_COMPAT
+	if (op_size == sizeof(op32)) {
+		if (copy_from_user(&op32, arg, sizeof(op32)))
+			return -EFAULT;
+		op->addr = op32.addr;
+		op->handle = op32.handle;
+		op->offset = op32.offset;
+		op->elem_size = op32.elem_size;
+		op->hmem_stride = op32.hmem_stride;
+		op->user_stride = op32.user_stride;
+		op->count = op32.count;
+		return 0;
+	}
+#endif
+	if (op_size == sizeof(*op)) {
+		if (copy_from_user(op, arg, sizeof(*op)))
+			return -EFAULT;
+		return 0;
+	}
+
+	if (op_size == sizeof(op64)) {
+		if (copy_from_user(&op64, arg, sizeof(op64)))
+			return -EFAULT;
+		op->addr = op64.addr;
+		op->handle = op64.handle;
+		op->offset = op64.offset;
+		op->elem_size = op64.elem_size;
+		op->hmem_stride = op64.hmem_stride;
+		op->user_stride = op64.user_stride;
+		op->count = op64.count;
+		return 0;
+	}
+
+	pr_warn("nvmap: rw_handle copy size failed\n");
+	return -EINVAL;
+}
+
+static void ioctl_rw_handle_copy_arg_to_user(void __user *arg,
+					ssize_t copied, int op_size)
+{
+	struct nvmap_rw_handle __user *uarg = arg;
+	struct nvmap_rw_handle_64 __user *uarg64 = arg;
+#ifdef CONFIG_COMPAT
+	struct nvmap_rw_handle_32 __user *uarg32 = arg;
+#endif
+
+#ifdef CONFIG_COMPAT
+	if (op_size == sizeof(struct nvmap_rw_handle_32)) {
+		__put_user(copied, &uarg32->count);
+		return;
+	}
+#endif
+	if (op_size == sizeof(struct nvmap_rw_handle)) {
+		__put_user(copied, &uarg->count);
+		return;
+	}
+
+	if (op_size == sizeof(struct nvmap_rw_handle_64)) {
+		__put_user(copied, &uarg64->count);
+		return;
+	}
+
+	pr_warn("nvmap: rw_handle copy to uarg failed\n");
+}
+
+/*
+ * TODO: Move this function to a better location
+ */
+static int set_vpr_fail_data(void *user_addr, ulong user_stride,
+		       ulong elem_size, ulong count)
+{
+	int ret = 0;
+	void *vaddr;
+
+	vaddr = vmalloc(PAGE_SIZE);
+	if (!vaddr)
+		return -ENOMEM;
+	memset(vaddr, 0xFF, PAGE_SIZE);
+
+	while (!ret && count--) {
+		ulong size_to_copy = elem_size;
+
+		while (!ret && size_to_copy) {
+			ret = copy_to_user(user_addr, vaddr,
+				size_to_copy > PAGE_SIZE ? PAGE_SIZE : size_to_copy);
+			size_to_copy -= (size_to_copy > PAGE_SIZE ? PAGE_SIZE : size_to_copy);
+		}
+		user_addr += user_stride;
+	}
+
+	vfree(vaddr);
+	return ret;
+}
+
+int nvmap_ioctl_rw_handle(struct file *filp, int is_read, void __user *arg,
+			  size_t op_size)
+{
+	struct nvmap_client *client = filp->private_data;
+	struct nvmap_handle *h;
+	ssize_t copied;
+	struct nvmap_rw_handle op;
+	unsigned long addr, offset, elem_size, hmem_stride, user_stride;
+	unsigned long count;
+	int fd;
+	int err = 0;
+
+	err = ioctl_rw_handle_copy_op_from_user(arg, &op, op_size);
+	if (err)
+		return err;
+
+	addr = op.addr;
+	fd = op.handle;
+	offset = op.offset;
+	elem_size = op.elem_size;
+	hmem_stride = op.hmem_stride;
+	user_stride = op.user_stride;
+	count = op.count;
+
+
+	if (!addr || !count || !elem_size)
+		return -EINVAL;
+
+	h = nvmap_handle_from_fd(fd);
+	h = nvmap_handle_get(h);
+	if (!h)
+		return -EINVAL;
+
+	if (is_read && soc_is_tegra186_n_later() &&
+		nvmap_handle_heap_type(h) == NVMAP_HEAP_CARVEOUT_VPR) {
+		int ret;
+
+		/* VPR memory is not readable from CPU.
+		 * Memset buffer to all 0xFF's for backward compatibility. */
+		ret = set_vpr_fail_data((void *)addr, user_stride, elem_size, count);
+		nvmap_handle_put(h);
+
+		if (ret == 0)
+			ret = -EPERM;
+
+		return ret;
+	}
+
+	nvmap_handle_kmap_inc(h);
+	trace_nvmap_ioctl_rw_handle(client, h, is_read, offset,
+				    addr, hmem_stride,
+				    user_stride, elem_size, count);
+	copied = nvmap_handle_rw(h,
+				offset, hmem_stride,
+				addr, user_stride,
+				elem_size, count,
+				is_read);
+	nvmap_handle_kmap_dec(h);
+
+	if (copied < 0) {
+		err = copied;
+		copied = 0;
+	} else if (copied < (count * elem_size))
+		err = -EINTR;
+
+	ioctl_rw_handle_copy_arg_to_user(arg, copied, op_size);
+
+	nvmap_handle_put(h);
+
+	return err;
+}
+
+extern struct device tegra_vpr_dev;
+
+int nvmap_ioctl_gup_test(struct file *filp, void __user *arg)
+{
+	int err = -EINVAL;
+	struct nvmap_gup_test op;
+	struct vm_area_struct *vma;
+	struct nvmap_handle *handle;
+	int nr_page;
+	struct page **pages;
+
+	if (copy_from_user(&op, arg, sizeof(op)))
+		return -EFAULT;
+
+	op.result = 1;
+	vma = find_vma(current->mm, op.va);
+	if (unlikely(!vma) || (unlikely(op.va < vma->vm_start )) ||
+	    unlikely(op.va >= vma->vm_end))
+		goto exit;
+
+	handle = nvmap_handle_from_fd(op.handle);
+	nvmap_handle_get(handle);
+	if (!handle)
+		goto exit;
+
+	if (vma->vm_end - vma->vm_start != nvmap_handle_size(handle)) {
+		pr_err("handle size(0x%zx) and vma size(0x%lx) don't match\n",
+			 nvmap_handle_size(handle), vma->vm_end - vma->vm_start);
+		goto put_handle;
+	}
+
+	err = -ENOMEM;
+	nr_page = nvmap_handle_size(handle) >> PAGE_SHIFT;
+	pages = nvmap_altalloc(nr_page * sizeof(*pages));
+	if (IS_ERR_OR_NULL(pages)) {
+		err = PTR_ERR(pages);
+		goto put_handle;
+	}
+
+	err = nvmap_get_user_pages(op.va & PAGE_MASK, nr_page, pages);
+	if (err)
+		goto put_user_pages;
+
+	// TODO: Find an easy way to fix this
+	/*
+	for (i = 0; i < nr_page; i++) {
+		if (handle->pgalloc.pages[i] != pages[i]) {
+			pr_err("page pointers don't match, %p %p\n",
+			       handle->pgalloc.pages[i], pages[i]);
+			op.result = 0;
+		}
+	}
+	*/
+
+	if (op.result)
+		err = 0;
+
+	if (copy_to_user(arg, &op, sizeof(op)))
+		err = -EFAULT;
+
+put_user_pages:
+	nvmap_altfree(pages, nr_page * sizeof(*pages));
+put_handle:
+	nvmap_handle_put(handle);
+exit:
+	pr_info("GUP Test %s\n", err ? "failed" : "passed");
+	return err;
+}
+
+int nvmap_ioctl_set_tag_label(struct file *filp, void __user *arg)
+{
+	struct nvmap_set_tag_label op;
+	struct nvmap_device *dev = nvmap_dev;
+	int err;
+
+	if (copy_from_user(&op, arg, sizeof(op)))
+		return -EFAULT;
+
+	if (op.len > NVMAP_TAG_LABEL_MAXLEN)
+		op.len = NVMAP_TAG_LABEL_MAXLEN;
+
+	if (op.len)
+		err = nvmap_define_tag(dev, op.tag,
+			(const char __user *)op.addr, op.len);
+	else
+		err = nvmap_remove_tag(dev, op.tag);
+
+	return err;
+}
+
+int nvmap_ioctl_get_available_heaps(struct file *filp, void __user *arg)
+{
+	struct nvmap_available_heaps op;
+	int i;
+
+	memset(&op, 0, sizeof(op));
+
+	for (i = 0; i < nvmap_dev->nr_carveouts; i++) {
+		struct nvmap_carveout_node *carveout =
+					nvmap_dev_to_carveout(nvmap_dev, i);
+
+		op.heaps |= nvmap_carveout_heap_bit(carveout);
+	}
+
+	if (copy_to_user(arg, &op, sizeof(op))) {
+		pr_err("copy_to_user failed\n");
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+int nvmap_ioctl_get_heap_size(struct file *filp, void __user *arg)
+{
+	struct nvmap_heap_size op;
+	int i;
+	memset(&op, 0, sizeof(op));
+
+	if (copy_from_user(&op, arg, sizeof(op)))
+		return -EFAULT;
+
+	for (i = 0; i < nvmap_dev->nr_carveouts; i++) {
+		struct nvmap_carveout_node *carveout =
+					nvmap_dev_to_carveout(nvmap_dev, i);
+
+		if (op.heap & nvmap_carveout_heap_bit(carveout)) {
+			op.size = nvmap_carveout_query_heap_size(carveout);
+			if (copy_to_user(arg, &op, sizeof(op)))
+				return -EFAULT;
+			return 0;
+		}
+	}
+	return -ENODEV;
+
+}
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_ioctl.h b/drivers/video/tegra/nvmap/nv2/nvmap_ioctl.h
new file mode 100644
index 000000000000..fac9f7a0590f
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_ioctl.h
@@ -0,0 +1,67 @@
+/*
+ * drivers/video/tegra/nvmnvmap_ioctl.h
+ *
+ * ioctl declarations for nvmap
+ *
+ * Copyright (c) 2010-2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __VIDEO_TEGRA_NVMAP_IOCTL_H
+#define __VIDEO_TEGRA_NVMAP_IOCTL_H
+
+#include <linux/nvmap.h>
+
+int nvmap_ioctl_pinop(struct file *filp, bool is_pin, void __user *arg,
+	bool is32);
+
+int nvmap_ioctl_getid(struct file *filp, void __user *arg);
+
+int nvmap_ioctl_get_ivcid(struct file *filp, void __user *arg);
+
+int nvmap_ioctl_getfd(struct file *filp, void __user *arg);
+
+int nvmap_ioctl_alloc(struct file *filp, unsigned int cmd, void __user *arg);
+
+int nvmap_ioctl_alloc_kind(struct file *filp, void __user *arg);
+
+int nvmap_ioctl_alloc_ivm(struct file *filp, void __user *arg);
+
+int nvmap_ioctl_vpr_floor_size(struct file *filp, void __user *arg);
+
+int nvmap_ioctl_free(struct file *filp, unsigned long arg);
+
+int nvmap_ioctl_create(struct file *filp, unsigned int cmd, void __user *arg);
+
+int nvmap_ioctl_create_from_va(struct file *filp, void __user *arg);
+
+int nvmap_ioctl_create_from_ivc(struct file *filp, void __user *arg);
+
+int nvmap_ioctl_get_ivc_heap(struct file *filp, void __user *arg);
+
+int nvmap_map_into_caller_ptr(struct file *filp, void __user *arg, bool is32);
+
+int nvmap_ioctl_cache_maint(struct file *filp, void __user *arg, int size);
+
+int nvmap_ioctl_rw_handle(struct file *filp, int is_read, void __user *arg,
+	size_t op_size);
+
+int nvmap_ioctl_cache_maint_list(struct file *filp, void __user *arg,
+	bool is_rsrv_op);
+
+int nvmap_ioctl_gup_test(struct file *filp, void __user *arg);
+
+int nvmap_ioctl_set_tag_label(struct file *filp, void __user *arg);
+
+int nvmap_ioctl_get_available_heaps(struct file *filp, void __user *arg);
+
+int nvmap_ioctl_get_heap_size(struct file *filp, void __user *arg);
+#endif	/*  __VIDEO_TEGRA_NVMAP_IOCTL_H */
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_ioctl_cache.c b/drivers/video/tegra/nvmap/nv2/nvmap_ioctl_cache.c
new file mode 100644
index 000000000000..6e02203f8100
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_ioctl_cache.c
@@ -0,0 +1,264 @@
+/*
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"nvmap: %s() " fmt, __func__
+
+#include <linux/dma-mapping.h>
+#include <linux/export.h>
+#include <linux/fs.h>
+#include <linux/io.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/nvmap.h>
+#include <linux/vmalloc.h>
+#include <linux/highmem.h>
+#include <soc/tegra/chip-id.h>
+
+#include <asm/io.h>
+#include <asm/memory.h>
+#include <asm/uaccess.h>
+#include <soc/tegra/common.h>
+#include <trace/events/nvmap.h>
+
+#include "nvmap_ioctl.h"
+#include "nvmap_heap.h"
+
+#include "nvmap_dev.h"
+#include "nvmap_handle.h"
+#include "nvmap_cache.h"
+#include "nvmap_misc.h"
+
+
+/*
+ * sizes[i] == 0  is a special case which causes handle wide operation,
+ */
+static int cache_maint_sanitize_size_offset(struct nvmap_handle **handles,
+				u64 *sizes, u64 *offsets, int nr)
+{
+	int err = 0;
+	int i = 0;
+
+	for (i = 0; i < nr; i++) {
+		if (sizes[i] == 0)
+			sizes[i] = nvmap_handle_size(handles[i]);
+	}
+
+	return err;
+}
+
+static void cache_maint_put_handles(struct nvmap_handle **handles, int nr)
+{
+	int i;
+
+	for (i = 0; i < nr; i++)
+		nvmap_handle_put(handles[i]);
+}
+
+
+static int cache_maint_get_handles(struct nvmap_handle **handles,
+					u32 *handle_ptrs, int nr)
+{
+	int i;
+	int sync_count = 0;
+	int pgalloc_count = 0;
+	int heap_type;
+
+	for (i = 0; i < nr; i++) {
+		handles[i] = nvmap_handle_from_fd(handle_ptrs[i]);
+		handles[i] = nvmap_handle_get(handles[i]);
+		if (!handles[i]) {
+			pr_err("invalid handle_ptr[%d] = %u\n",
+							i, handle_ptrs[i]);
+			cache_maint_put_handles(handles, i);
+			return -EINVAL;
+		}
+
+		heap_type = nvmap_handle_heap_type(handles[i]);
+		if (!(heap_type & nvmap_dev->cpu_access_mask)) {
+			pr_err("heap %x can't be accessed from cpu\n",
+								heap_type);
+			cache_maint_put_handles(handles, i);
+			return -EPERM;
+		}
+	}
+
+	for (i = 0; i < nr; i++) {
+		if (nvmap_handle_userflag(handles[i])
+					& NVMAP_HANDLE_CACHE_SYNC_AT_RESERVE)
+			sync_count++;
+
+		if (nvmap_handle_is_heap(handles[i]))
+			pgalloc_count++;
+	}
+
+	/*
+	 * Either all handles should have NVMAP_HANDLE_CACHE_SYNC_AT_RESERVE
+	 * or none should have it.
+	 */
+	if (!((sync_count == 0) || (sync_count == nr))) {
+		pr_err("incorrect CACHE_SYNC_AT_RESERVE mix of handles\n");
+		cache_maint_put_handles(handles, nr);
+		return -EINVAL;
+	}
+
+	/*
+	 * when NVMAP_HANDLE_CACHE_SYNC_AT_RESERVE is specified mix can cause
+	 * cache WB_INV at unreserve op on iovmm handles increasing overhead.
+	 * So, either all handles should have pages from carveout or from iovmm.
+	 */
+	if (sync_count && !((pgalloc_count == 0) || (pgalloc_count == nr))) {
+		pr_err("all or none of the handles should be from heap\n");
+		cache_maint_put_handles(handles, nr);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int cache_maint_copy_args(struct nvmap_cache_op_list *op,
+				u32 *handle_ptr, u64 *offset_ptr, u64 *size_ptr)
+{
+	int is_32 = !(op->op & NVMAP_ELEM_SIZE_U64);
+	int i = 0;
+
+	if (!op->handles || !op->offsets || !op->sizes) {
+		pr_err("pointers are invalid\n");
+		return -EINVAL;
+	}
+
+	if (is_32) {
+		size_t tmp_size = sizeof(u32) * op->nr;
+		u32 *tmp = nvmap_altalloc(tmp_size);
+		if (!tmp) {
+			pr_err("Failed allocating tmp buffer");
+			return -ENOMEM;
+		}
+
+		if (copy_from_user(tmp, (void *)op->offsets,
+					op->nr * sizeof(u32))) {
+			pr_err("Can't copy from user pointer op.offsets\n");
+			nvmap_altfree(tmp, tmp_size);
+			return -EFAULT;
+		}
+		for (i = 0; i < op->nr; i++) {
+			offset_ptr[i] = tmp[i];
+		}
+
+		if (copy_from_user(tmp, (void *)op->sizes,
+					op->nr * sizeof(u32))) {
+			pr_err("Can't copy from user pointer op.sizes\n");
+			nvmap_altfree(tmp, tmp_size);
+			return -EFAULT;
+		}
+		for (i = 0; i < op->nr; i++) {
+			size_ptr[i] = tmp[i];
+		}
+
+		nvmap_altfree(tmp, tmp_size);
+	} else {
+		if (copy_from_user(offset_ptr, (void *)op->offsets,
+					op->nr * sizeof(u64))) {
+			pr_err("Can't copy from user pointer op.offsets\n");
+			return -EFAULT;
+		}
+
+		if (copy_from_user(size_ptr, (void *)op->sizes,
+					op->nr * sizeof(u64))) {
+			pr_err("Can't copy from user pointer op.sizes\n");
+			return -EFAULT;
+		}
+	}
+
+
+	if (copy_from_user(handle_ptr, (void *)op->handles,
+		op->nr * sizeof(u32))) {
+		pr_err("Can't copy from user pointer op.handles\n");
+		return -EFAULT;
+	}
+
+	return 0;
+}
+
+
+int nvmap_ioctl_cache_maint_list(struct file *filp, void __user *arg,
+				 bool is_reserve_ioctl)
+{
+	struct nvmap_cache_op_list op;
+	struct nvmap_handle **handles;
+	u32 *handle_ptr;
+	u64 *offset_ptr;
+	u64 *size_ptr;
+	int err = 0;
+
+	if (copy_from_user(&op, arg, sizeof(op)))
+		return -EFAULT;
+
+	if (!op.nr || op.nr > UINT_MAX / sizeof(u32))
+		return -EINVAL;
+
+	if (!access_ok(VERIFY_READ, op.handles, op.nr * sizeof(u32)))
+		return -EFAULT;
+
+	handles		= nvmap_altalloc(sizeof(*handles) * op.nr);
+	offset_ptr 	= nvmap_altalloc(sizeof(u64) * op.nr);
+	size_ptr 	= nvmap_altalloc(sizeof(u64) * op.nr);
+	handle_ptr 	= nvmap_altalloc(sizeof(u32) * op.nr);
+
+	if (!handles || !offset_ptr || !size_ptr || !handle_ptr) {
+		if (handles)
+			nvmap_altfree(handles, sizeof(*handles) * op.nr);
+		if (offset_ptr)
+			nvmap_altfree(offset_ptr, sizeof(u64) * op.nr);
+		if (size_ptr)
+			nvmap_altfree(size_ptr, sizeof(u64) * op.nr);
+		if (handle_ptr)
+			nvmap_altfree(handle_ptr, sizeof(u32) * op.nr);
+		return -ENOMEM;
+	}
+
+	err = cache_maint_copy_args(&op, handle_ptr, offset_ptr, size_ptr);
+	if (err)
+		goto free_mem;
+
+	op.op &= ~NVMAP_ELEM_SIZE_U64;
+
+	err = cache_maint_get_handles(handles, handle_ptr, op.nr);
+	if (err)
+		goto free_mem;
+
+	err = cache_maint_sanitize_size_offset(handles, size_ptr,
+							offset_ptr, op.nr);
+	if (err)
+		goto free_handles;
+
+	if (is_reserve_ioctl) {
+		err = nvmap_handles_reserve(handles, offset_ptr, size_ptr,
+							op.op, op.nr);
+	} else {
+		err = nvmap_handles_cache_maint(handles, offset_ptr, size_ptr,
+							op.op, op.nr);
+	}
+
+free_handles:
+	cache_maint_put_handles(handles, op.nr);
+free_mem:
+	nvmap_altfree(handles, sizeof(*handles) * op.nr);
+	nvmap_altfree(offset_ptr, sizeof(u64) * op.nr);
+	nvmap_altfree(size_ptr, sizeof(u64) * op.nr);
+	nvmap_altfree(handle_ptr, sizeof(u32) * op.nr);
+
+	return err;
+}
+
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_misc.h b/drivers/video/tegra/nvmap/nv2/nvmap_misc.h
new file mode 100644
index 000000000000..e5a19f358036
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_misc.h
@@ -0,0 +1,108 @@
+/*
+ * Copyright (c) 2018-2021, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __NVMAP_MISC_H
+#define __NVMAP_MISC_H
+
+#define NVMAP_IVM_INVALID_PEER		(-1)
+
+/* bit 31-29: IVM peer
+ * bit 28-16: offset (aligned to 32K)
+ * bit 15-00: len (aligned to page_size)
+ */
+#define NVMAP_IVM_LENGTH_SHIFT (0)
+#define NVMAP_IVM_LENGTH_WIDTH (16)
+#define NVMAP_IVM_LENGTH_MASK  ((1 << NVMAP_IVM_LENGTH_WIDTH) - 1)
+#define NVMAP_IVM_OFFSET_SHIFT (NVMAP_IVM_LENGTH_SHIFT + NVMAP_IVM_LENGTH_WIDTH)
+#define NVMAP_IVM_OFFSET_WIDTH (14)
+#define NVMAP_IVM_OFFSET_MASK  ((1 << NVMAP_IVM_OFFSET_WIDTH) - 1)
+#define NVMAP_IVM_IVMID_SHIFT  (NVMAP_IVM_OFFSET_SHIFT + NVMAP_IVM_OFFSET_WIDTH)
+#define NVMAP_IVM_IVMID_WIDTH  (3)
+#define NVMAP_IVM_IVMID_MASK   ((1 << NVMAP_IVM_IVMID_WIDTH) - 1)
+#define NVMAP_IVM_ALIGNMENT    (SZ_32K)
+
+void *nvmap_altalloc(size_t len);
+void nvmap_altfree(void *ptr, size_t len);
+
+static inline size_t nvmap_ivmid_to_size(u64 ivm_id)
+{
+	return (ivm_id &
+			((1ULL << NVMAP_IVM_LENGTH_WIDTH) - 1)) << PAGE_SHIFT;
+}
+
+static inline phys_addr_t nvmap_ivmid_to_offset(u64 ivm_id)
+{
+	return ((ivm_id &
+			~((u64)NVMAP_IVM_IVMID_MASK << NVMAP_IVM_IVMID_SHIFT)) >>
+			NVMAP_IVM_LENGTH_WIDTH) << (ffs(NVMAP_IVM_ALIGNMENT) - 1);
+}
+
+static inline int nvmap_ivmid_to_peer(u64 ivm_id)
+{
+	return (ivm_id >> NVMAP_IVM_IVMID_SHIFT);
+
+}
+
+static inline int nvmap_calculate_ivm_id(int vm_id, size_t len,
+						unsigned int offs)
+{
+	int ivm_id = 0;
+
+	BUG_ON(offs & (NVMAP_IVM_ALIGNMENT - 1));
+	BUG_ON((offs >> ffs(NVMAP_IVM_ALIGNMENT)) &
+			~((1 << NVMAP_IVM_OFFSET_WIDTH) - 1));
+	BUG_ON(vm_id & ~(NVMAP_IVM_IVMID_MASK));
+
+	BUG_ON(len & ~(PAGE_MASK));
+
+	ivm_id = ((u64)vm_id << NVMAP_IVM_IVMID_SHIFT);
+	ivm_id |= (((offs >> (ffs(NVMAP_IVM_ALIGNMENT) - 1)) &
+			((1ULL << NVMAP_IVM_OFFSET_WIDTH) - 1)) <<
+				NVMAP_IVM_OFFSET_SHIFT);
+	ivm_id |= (len >> PAGE_SHIFT);
+
+	return ivm_id;
+}
+
+static inline struct page *nvmap_to_page(struct page *page)
+{
+	return (struct page *)((unsigned long)page & ~3UL);
+}
+
+struct page **nvmap_alloc_pages(struct page **pg_pages, u32 nr_pages);
+struct page *nvmap_alloc_pages_exact(gfp_t gfp, size_t size);
+
+int nvmap_get_user_pages(ulong vaddr, int nr_page, struct page **pages);
+
+static inline bool nvmap_page_dirty(struct page *page)
+{
+	return (unsigned long)page & 1UL;
+}
+
+static inline bool nvmap_page_mkdirty(struct page **page)
+{
+	if (nvmap_page_dirty(*page))
+		return false;
+	*page = (struct page *)((unsigned long)*page | 1UL);
+	return true;
+}
+
+static inline bool nvmap_page_mkclean(struct page **page)
+{
+	if (!nvmap_page_dirty(*page))
+		return false;
+	*page = (struct page *)((unsigned long)*page & ~1UL);
+	return true;
+}
+
+#endif /* __NVMAP_MISC_H */
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_page_color.c b/drivers/video/tegra/nvmap/nv2/nvmap_page_color.c
new file mode 100644
index 000000000000..144fe9383f8e
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_page_color.c
@@ -0,0 +1,417 @@
+/*
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#include <linux/moduleparam.h>
+#include <linux/slab.h>
+#include <linux/version.h>
+
+#include <soc/tegra/chip-id.h>
+
+#include <trace/events/nvmap.h>
+
+#include "nvmap_heap_alloc.h"
+#include "nvmap_cache.h"
+#include "nvmap_misc.h"
+#include "nvmap_pp.h"
+#include "nvmap_dev.h"
+
+static uint s_nr_colors = 1;
+module_param_named(nr_colors, s_nr_colors, uint, 0644);
+
+#define NVMAP_MAX_COLORS 16
+
+#define CHANNEL_MASK_0 0x27af5200
+#define CHANNEL_MASK_1 0x563ca400
+#define CHANNEL_MASK_2 0x3f264800
+#define CHANNEL_MASK_3 0xe2443000
+#define BANK_MASK_0 0x5ca78400
+#define BANK_MASK_1 0xe5724800
+#define BANK_MASK_2 0x973bb000
+
+#define BIT_N(a, n) \
+	(((a) >> (n)) & 1)
+
+#define BITS_XOR_9_TO_31(a) \
+	(BIT_N((a), 9) ^ BIT_N((a), 10) ^ BIT_N((a), 11) ^ BIT_N((a), 12)  ^ \
+	BIT_N((a), 13) ^ BIT_N((a), 14) ^ BIT_N((a), 15) ^ BIT_N((a), 16) ^ \
+	BIT_N((a), 17) ^ BIT_N((a), 18) ^ BIT_N((a), 19) ^ BIT_N((a), 20) ^ \
+	BIT_N((a), 21) ^ BIT_N((a), 22) ^ BIT_N((a), 23) ^ BIT_N((a), 24) ^ \
+	BIT_N((a), 25) ^ BIT_N((a), 26) ^ BIT_N((a), 27) ^ BIT_N((a), 28) ^ \
+	BIT_N((a), 29) ^ BIT_N((a), 30) ^ BIT_N((a), 31))
+
+#define BITS_XOR_10_TO_31(a) \
+	(BIT_N((a), 10) ^ BIT_N((a), 11) ^ BIT_N((a), 12) ^ \
+	BIT_N((a), 13) ^ BIT_N((a), 14) ^ BIT_N((a), 15) ^ BIT_N((a), 16) ^ \
+	BIT_N((a), 17) ^ BIT_N((a), 18) ^ BIT_N((a), 19) ^ BIT_N((a), 20) ^ \
+	BIT_N((a), 21) ^ BIT_N((a), 22) ^ BIT_N((a), 23) ^ BIT_N((a), 24) ^ \
+	BIT_N((a), 25) ^ BIT_N((a), 26) ^ BIT_N((a), 27) ^ BIT_N((a), 28) ^ \
+	BIT_N((a), 29) ^ BIT_N((a), 30) ^ BIT_N((a), 31))
+
+struct color_list {
+	u32 *counts;
+	u32 *heads;
+	u32 *list;
+	struct page **pages;
+	u32 page_count;
+	u32 length;
+};
+
+struct nvmap_alloc_state {
+	u32 nr_colors;
+	u32 (*addr_to_color)(uintptr_t phys);
+	u32 tile;
+	u32 output_count;
+	u32 nr_pages;
+	u32 max_color_per_tile;
+	struct color_list *list;
+};
+
+static struct color_list *alloc_color_list(u32 nr_pages, u32 nr_colors)
+{
+	struct color_list *list;
+	u32 *temp = NULL;
+	u32 nr_u32;
+
+	list = kzalloc(sizeof(struct color_list), GFP_KERNEL);
+	if (!list)
+		return NULL;
+
+	list->pages = vmalloc(nr_pages * sizeof(struct page *));
+	if (!list->pages) {
+		kfree(list);
+		return NULL;
+	}
+
+	/* Allocate counts, heads, and list with a single allocation */
+	nr_u32 = nr_pages + 2 * nr_colors;
+	temp = vmalloc(nr_u32 * sizeof(u32));
+	if (!temp)
+		goto fail;
+
+	memset(&temp[0], 0, 2 * nr_colors *  sizeof(u32));
+	list->counts = &temp[0];
+	list->heads = &temp[nr_colors];
+	list->list = &temp[2 * nr_colors];
+
+	list->page_count = nr_pages;
+
+	return list;
+fail:
+	if (list->pages)
+		vfree(list->pages);
+	kfree(list);
+	return NULL;
+}
+
+static void free_color_list(struct color_list *list)
+{
+	vfree(list->pages);
+	vfree(list->counts);	/* Frees counts, heads, and list */
+	kfree(list);
+}
+
+static struct page *list_pop_page(struct color_list *list, u32 color, char *who)
+{
+	u32 i;
+
+	/* Debug check */
+	if ((list->counts[color] == 0) || (list->counts[color] > 1 << 31)) {
+		pr_err("list_pop_page: OVER FREE!\n");
+		pr_err(" called from: %s\n", who);
+		for (i = 0; i < s_nr_colors; i++)
+			pr_err(" color = %d: %d\n", i, list->counts[i]);
+		BUG();
+	}
+	i = list->heads[color];
+	list->heads[color] = list->list[i];
+	list->counts[color]--;
+	return list->pages[i];
+}
+
+
+static u32 addr_to_color_t19x(uintptr_t phys)
+{
+	int color, chan, bank;
+	u32 addr = (u32)phys;
+	u32 xaddr = (u32)(phys >> 4);
+
+
+	chan =  (BITS_XOR_9_TO_31(addr & CHANNEL_MASK_0) << 0);
+	chan |= (BITS_XOR_9_TO_31(addr & CHANNEL_MASK_1) << 1);
+	chan |= (BITS_XOR_9_TO_31(addr & CHANNEL_MASK_2) << 2);
+	chan |= (BITS_XOR_9_TO_31(addr & CHANNEL_MASK_3) << 3);
+
+	bank = (BITS_XOR_10_TO_31(xaddr & BANK_MASK_0) << 0);
+	bank |= (BITS_XOR_10_TO_31(xaddr & BANK_MASK_1) << 1);
+	bank |= (BITS_XOR_10_TO_31(xaddr & BANK_MASK_2) << 2);
+
+	WARN_ON(chan > 15);
+	WARN_ON(bank > 7);
+	/* It is preferable to color pages based on even/odd banks
+	 * as well. To limit the number of colors to 16, bank info
+	 * is not used in page coloring.
+	 */
+	color = chan;
+
+	return color;
+}
+
+static struct color_list *init_color_list(struct nvmap_page_pool *pool,
+					  struct nvmap_alloc_state *state,
+					  u32 nr_pages)
+{
+	struct color_list *list;
+	u32 color, i, page_index = 0;
+	gfp_t gfp = GFP_NVMAP | __GFP_ZERO;
+
+	list = alloc_color_list(nr_pages, state->nr_colors);
+	if (!list)
+		return NULL;
+
+#ifdef CONFIG_NVMAP_PAGE_POOLS
+	/* Allocated page from nvmap page pool if possible */
+	page_index = nvmap_page_pool_alloc_lots(pool, list->pages, nr_pages);
+#endif
+	/* Fall back to general page allocator */
+	for (i = page_index; i < nr_pages; i++) {
+		list->pages[i] = nvmap_alloc_pages_exact(gfp, PAGE_SIZE);
+		if (!list->pages[i])
+			goto fail;
+	}
+	/* Clean the cache for any page that didn't come from the page pool */
+	if (page_index < nr_pages)
+		nvmap_cache_clean_pages(&list->pages[page_index],
+				  nr_pages - page_index);
+
+	/* Create linked list of colors and compute the histogram */
+	for (i = 0; i < nr_pages; i++) {
+		color = state->addr_to_color((uintptr_t)
+					     page_to_phys(list->pages[i]));
+		list->list[i] = list->heads[color];
+		list->heads[color] = i;
+		list->counts[color]++;
+	}
+	return list;
+fail:
+	while (i--)
+		__free_page(list->pages[i]);
+	free_color_list(list);
+	return NULL;
+}
+
+static void smooth_pages(struct color_list *list, u32 nr_extra, u32 nr_colors)
+{
+	u32 i, j, color, max;
+	u32 counts[NVMAP_MAX_COLORS] = {0};
+
+	if (nr_extra == 0)
+		return;
+
+	/* Determine which colors need to be freed */
+	for (i = 0; i < nr_extra; i++) {
+		/* Find the max */
+		max = 0;
+		color = 0;
+		for (j = 0; j < nr_colors; j++) {
+			if (list->counts[j] - counts[j] > max) {
+				color = j;
+				max = list->counts[j] - counts[j];
+			}
+		}
+		counts[color]++;
+	}
+
+	/* Iterate through 0...nr_extra-1 in psuedorandom order */
+	do {
+		/* Pop the max off and free it */
+		for (color = 0; color < nr_colors; color++) {
+			while (counts[color]) {
+				__free_page(list_pop_page(list,
+						color, "smooth_pages"));
+				counts[color]--;
+				nr_extra--;
+			}
+		}
+	} while (nr_extra > 0);
+}
+
+static void add_perfect(struct nvmap_alloc_state *state, u32 nr_pages,
+			struct page **out_pages)
+{
+	u32 i;
+	u32 color;
+	struct page *page;
+	uintptr_t virt_addr;
+
+
+	/* create a perfect tile */
+	for (i = 0;
+	     i < state->nr_colors && state->output_count < nr_pages;
+	     i++) {
+		virt_addr = (i + (state->tile * state->nr_colors)) * PAGE_SIZE;
+		color = state->addr_to_color(virt_addr);
+		page = list_pop_page(state->list, color, "perfect");
+		out_pages[state->output_count++] = page;
+	}
+}
+
+static void add_imperfect(struct nvmap_alloc_state *state, u32 nr_pages,
+			  struct page **out_pages)
+{
+	u32 i, j;
+	u32 max_count;
+	u32 color;
+	struct page *page;
+	uintptr_t virt_addr;
+	u32 counts[NVMAP_MAX_COLORS] = {0};
+
+	/* Determine which colors will go into the tile */
+	for (i = 0; i < state->nr_colors; i++) {
+		max_count = 0;
+		color = 0;
+		for (j = 0; j < state->nr_colors; j++) {
+			u32 left = state->list->counts[j] - counts[j];
+
+			if (left > max_count &&
+			    counts[j] < state->max_color_per_tile) {
+				max_count = left;
+				color = j;
+			}
+		}
+		counts[color]++;
+	}
+
+	/* Arrange the colors into the tile */
+	for (i = 0;
+	     i < state->nr_colors && state->output_count < nr_pages;
+	     i++) {
+		virt_addr = (i + (i * state->nr_colors)) * PAGE_SIZE;
+		color = state->addr_to_color(virt_addr);
+		/* Find a substitute color */
+		if (counts[color] == 0) {
+			/* Find the color used the most in the tile */
+			max_count = 0;
+			for (j = 0; j < state->nr_colors; j++) {
+				if (counts[j] > max_count) {
+					max_count = counts[j];
+					color = j;
+				}
+			}
+		}
+		page = list_pop_page(state->list, color, "imperfect");
+		out_pages[state->output_count++] = page;
+		counts[color]--;
+	}
+}
+
+int nvmap_color_is_enabled(void)
+{
+	return s_nr_colors > 1;
+}
+
+int nvmap_color_init(void)
+{
+	u32 chipid = tegra_hidrev_get_chipid(tegra_read_chipid());
+
+	if (chipid == TEGRA194)
+		s_nr_colors = 16;
+	return 0;
+}
+
+int nvmap_color_alloc(struct nvmap_page_pool *pool, u32 nr_pages,
+			 struct page **out_pages)
+{
+	struct nvmap_alloc_state state;
+	u32 nr_alloc, max_count, min_count;
+	u32 nr_tiles, nr_perfect, nr_imperfect;
+	int dither_state;
+	u32 i;
+
+	state.nr_colors = s_nr_colors;
+	state.addr_to_color = addr_to_color_t19x;
+
+	/* Allocate pages for full 32-page tiles */
+	nr_tiles = (nr_pages + state.nr_colors - 1) / state.nr_colors;
+	/* Overallocate pages by 1/16th */
+	nr_alloc  = state.nr_colors * nr_tiles;
+	nr_alloc += nr_alloc >> 4;
+
+	/* Create lists of each page color */
+	state.list = init_color_list(pool, &state, nr_alloc);
+	if (!state.list)
+		return -ENOMEM;
+
+	/* Smooth out the histogram by freeing over allocated pages */
+	smooth_pages(state.list, nr_alloc - state.nr_colors * nr_tiles,
+		     state.nr_colors);
+
+	max_count = 0;
+	min_count = state.list->counts[0];
+	for (i = 0; i < state.nr_colors; i++) {
+		if (state.list->counts[i] > max_count)
+			max_count = state.list->counts[i];
+		if (state.list->counts[i] < min_count)
+			min_count = state.list->counts[i];
+	}
+
+	/* Compute the number of perfect / imperfect tiles and the maximum
+	 * number of pages with the same color can be in a tile
+	 */
+	if (max_count / nr_tiles >= 3) {
+		/* It is not possible to create perfect tiles with
+		 * max_color_per_tile <= 3
+		 */
+		nr_perfect = 0;
+		state.max_color_per_tile = (max_count + nr_tiles - 1)
+					   / nr_tiles;
+	} else if (nr_tiles * 2 == max_count) {
+		/* All of the tiles can be perfect */
+		nr_perfect = nr_tiles;
+		state.max_color_per_tile = 2;
+	} else {
+		/* Some of the tiles can be perfect */
+		nr_perfect = nr_tiles - (max_count % nr_tiles);
+		state.max_color_per_tile = 3;
+	}
+	/* Check if the number of perfect tiles is bound by the color with the
+	 * minimum count
+	 */
+	if (nr_perfect * 2 > min_count)
+		nr_perfect = min_count / 2;
+
+	nr_imperfect = nr_tiles - nr_perfect;
+
+	/* Output tiles */
+	dither_state = nr_perfect - nr_imperfect;
+	state.output_count = 0;
+	for (state.tile = 0; state.tile < nr_tiles; state.tile++) {
+		if (dither_state > 0) {
+			add_perfect(&state, nr_pages, out_pages);
+			dither_state -= nr_imperfect;
+		} else {
+			add_imperfect(&state, nr_pages, out_pages);
+			dither_state += nr_perfect;
+		}
+	}
+
+	/* Free extra pages created when the buffer does not
+	 * fill the last tile
+	 */
+	for (i = 0; i < state.nr_colors; i++)
+		while (state.list->counts[i] > 0)
+			__free_page(list_pop_page(state.list, i, "free"));
+
+	free_color_list(state.list);
+
+	return 0;
+}
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_page_color.h b/drivers/video/tegra/nvmap/nv2/nvmap_page_color.h
new file mode 100644
index 000000000000..a2ae9e70aee1
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_page_color.h
@@ -0,0 +1,46 @@
+/*
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __NVMAP_PAGE_COLOR_H
+#define __NVMAP_PAGE_COLOR_H
+
+#include "nvmap_structs.h"
+
+#ifdef CONFIG_NVMAP_COLOR_PAGES
+
+int nvmap_color_alloc(struct nvmap_page_pool *pool, u32 nr_pages,
+			 struct page **out_pages);
+int nvmap_color_init(void);
+int nvmap_color_is_enabled(void);
+
+#else
+
+static inline int nvmap_color_alloc(struct nvmap_page_pool *pool, u32 nr_pages,
+			 struct page **out_pages)
+{
+	return -1;
+}
+
+static inline int nvmap_color_init(void)
+{
+	return 0;
+}
+
+static inline int nvmap_color_is_enabled(void)
+{
+	return 0;
+}
+
+#endif
+
+#endif
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_pp.c b/drivers/video/tegra/nvmap/nv2/nvmap_pp.c
new file mode 100644
index 000000000000..b1f2b4d5f9d4
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_pp.c
@@ -0,0 +1,750 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_pp.c
+ *
+ * Manage page pools to speed up page allocation.
+ *
+ * Copyright (c) 2009-2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt) "%s: " fmt, __func__
+
+#include <linux/kernel.h>
+#include <linux/vmalloc.h>
+#include <linux/moduleparam.h>
+#include <linux/nodemask.h>
+#include <linux/shrinker.h>
+#include <linux/kthread.h>
+#include <linux/debugfs.h>
+#include <linux/freezer.h>
+#include <linux/highmem.h>
+#include <linux/version.h>
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+#include <linux/sched/clock.h>
+#include <uapi/linux/sched/types.h>
+#endif
+
+#include <trace/events/nvmap.h>
+
+#include "nvmap_cache.h"
+#include "nvmap_pp.h"
+#include "nvmap_dev.h"
+
+#define NVMAP_TEST_PAGE_POOL_SHRINKER     1
+#define PENDING_PAGES_SIZE                (SZ_1M / PAGE_SIZE)
+
+u64 nvmap_big_page_allocs;
+u64 nvmap_total_page_allocs;
+
+static bool enable_pp = 1;
+static u32 pool_size;
+
+static struct task_struct *background_allocator;
+static DECLARE_WAIT_QUEUE_HEAD(nvmap_bg_wait);
+
+#ifdef CONFIG_NVMAP_PAGE_POOL_DEBUG
+static inline void __pp_dbg_var_add(u64 *dbg_var, u32 nr)
+{
+	*dbg_var += nr;
+}
+#else
+#define __pp_dbg_var_add(dbg_var, nr)
+#endif
+
+#define pp_alloc_add(pool, nr) __pp_dbg_var_add(&(pool)->allocs, nr)
+#define pp_fill_add(pool, nr)  __pp_dbg_var_add(&(pool)->fills, nr)
+#define pp_hit_add(pool, nr)   __pp_dbg_var_add(&(pool)->hits, nr)
+#define pp_miss_add(pool, nr)  __pp_dbg_var_add(&(pool)->misses, nr)
+
+static int __nvmap_page_pool_fill_lots_locked(struct nvmap_page_pool *pool,
+				       struct page **pages, u32 nr);
+
+static inline struct page *get_zero_list_page(struct nvmap_page_pool *pool)
+{
+	struct page *page;
+
+	trace_get_zero_list_page(pool->to_zero);
+
+	if (list_empty(&pool->zero_list))
+		return NULL;
+
+	page = list_first_entry(&pool->zero_list, struct page, lru);
+	list_del(&page->lru);
+
+	pool->to_zero--;
+
+	return page;
+}
+
+static inline struct page *get_page_list_page(struct nvmap_page_pool *pool)
+{
+	struct page *page;
+
+	trace_get_page_list_page(pool->count);
+
+	if (list_empty(&pool->page_list))
+		return NULL;
+
+	page = list_first_entry(&pool->page_list, struct page, lru);
+	list_del(&page->lru);
+
+	pool->count--;
+
+	return page;
+}
+
+static inline struct page *get_page_list_page_bp(struct nvmap_page_pool *pool)
+{
+	struct page *page;
+
+	if (list_empty(&pool->page_list_bp))
+		return NULL;
+
+	page = list_first_entry(&pool->page_list_bp, struct page, lru);
+	list_del(&page->lru);
+
+	pool->count -= pool->pages_per_big_pg;
+	pool->big_page_count -= pool->pages_per_big_pg;
+
+	return page;
+}
+
+static inline bool nvmap_bg_should_run(struct nvmap_page_pool *pool)
+{
+	return !list_empty(&pool->zero_list);
+}
+
+static void nvmap_pp_zero_pages(struct page **pages, int nr)
+{
+	int i;
+
+	for (i = 0; i < nr; i++) {
+		clear_highpage(pages[i]);
+	}
+	nvmap_cache_clean_pages(pages, nr);
+
+	trace_nvmap_pp_zero_pages(nr);
+}
+
+static void nvmap_pp_do_background_zero_pages(struct nvmap_page_pool *pool)
+{
+	int i;
+	struct page *page;
+	int ret;
+	/*
+	 * Statically declared array of pages to be zeroed in a batch,
+	 * local to this thread but too big for the stack.
+	 */
+	static struct page *pending_zero_pages[PENDING_PAGES_SIZE];
+
+	rt_mutex_lock(&pool->lock);
+	for (i = 0; i < PENDING_PAGES_SIZE; i++) {
+		page = get_zero_list_page(pool);
+		if (page == NULL)
+			break;
+		pending_zero_pages[i] = page;
+		pool->under_zero++;
+	}
+	rt_mutex_unlock(&pool->lock);
+
+	nvmap_pp_zero_pages(pending_zero_pages, i);
+
+	rt_mutex_lock(&pool->lock);
+	ret = __nvmap_page_pool_fill_lots_locked(pool, pending_zero_pages, i);
+	pool->under_zero -= i;
+	rt_mutex_unlock(&pool->lock);
+
+	trace_nvmap_pp_do_background_zero_pages(ret, i);
+
+	for (; ret < i; ret++)
+		__free_page(pending_zero_pages[ret]);
+}
+
+/*
+ * This thread fills the page pools with zeroed pages. We avoid releasing the
+ * pages directly back into the page pools since we would then have to zero
+ * them ourselves. Instead it is easier to just reallocate zeroed pages. This
+ * happens in the background so that the overhead of allocating zeroed pages is
+ * not directly seen by userspace. Of course if the page pools are empty user
+ * space will suffer.
+ */
+static int nvmap_background_zero_thread(void *arg)
+{
+	struct nvmap_page_pool *pool = &nvmap_dev->pool;
+	struct sched_param param = { .sched_priority = 0 };
+
+	pr_info("PP zeroing thread starting.\n");
+
+	set_freezable();
+	sched_setscheduler(current, SCHED_IDLE, &param);
+
+	while (!kthread_should_stop()) {
+		while (nvmap_bg_should_run(pool))
+			nvmap_pp_do_background_zero_pages(pool);
+
+		wait_event_freezable(nvmap_bg_wait,
+				nvmap_bg_should_run(pool) ||
+				kthread_should_stop());
+	}
+
+	return 0;
+}
+
+static void nvmap_pgcount(struct page *page, bool incr)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 9, 0)
+	if (incr)
+		atomic_inc(&page->_count);
+	else
+		atomic_dec(&page->_count);
+#else
+	page_ref_add(page, incr ? 1 : -1);
+#endif
+}
+
+/*
+ * Free the passed number of pages from the page pool. This happens regardless
+ * of whether the page pools are enabled. This lets one disable the page pools
+ * and then free all the memory therein.
+ *
+ * FIXME: Pages in pending_zero_pages[] can still be unreleased.
+ */
+static ulong nvmap_page_pool_free_pages_locked(struct nvmap_page_pool *pool,
+						      ulong nr_pages)
+{
+	struct page *page;
+	bool use_page_list = false;
+	bool use_page_list_bp = false;
+
+	pr_debug("req to release pages=%ld\n", nr_pages);
+
+	while (nr_pages) {
+		int i;
+
+		if (use_page_list_bp)
+			page = get_page_list_page_bp(pool);
+		else if (use_page_list)
+			page = get_page_list_page(pool);
+		else
+			page = get_zero_list_page(pool);
+
+		if (!page) {
+			if (!use_page_list) {
+				use_page_list = true;
+				continue;
+			} else if (!use_page_list_bp) {
+				use_page_list_bp = true;
+				continue;
+			}
+			break;
+		}
+
+		if (use_page_list_bp) {
+			for (i = 0; i < pool->pages_per_big_pg; i++)
+				__free_page(nth_page(page, i));
+			pr_debug("released %d pages\n", pool->pages_per_big_pg);
+			if (nr_pages > pool->pages_per_big_pg)
+				nr_pages -= pool->pages_per_big_pg;
+			else
+				nr_pages = 0;
+		} else {
+			__free_page(page);
+			nr_pages--;
+			pr_debug("released 1 page\n");
+		}
+	}
+
+	pr_debug("remaining pages to release=%ld\n", nr_pages);
+	return nr_pages;
+}
+
+/*
+ * Alloc a bunch of pages from the page pool. This will alloc as many as it can
+ * and return the number of pages allocated. Pages are placed into the passed
+ * array in a linear fashion starting from index 0.
+ */
+int nvmap_page_pool_alloc_lots(struct nvmap_page_pool *pool,
+				struct page **pages, u32 nr)
+{
+	u32 ind = 0;
+	u32 non_zero_idx;
+	u32 non_zero_cnt = 0;
+
+	if (!enable_pp || !nr)
+		return 0;
+
+	rt_mutex_lock(&pool->lock);
+
+	while (ind < nr) {
+		struct page *page = NULL;
+
+		if (!non_zero_cnt)
+			page = get_page_list_page(pool);
+
+		if (!page) {
+			page = get_zero_list_page(pool);
+			if (!page)
+				break;
+			if (!non_zero_cnt)
+				non_zero_idx = ind;
+			non_zero_cnt++;
+		}
+
+		pages[ind++] = page;
+		if (IS_ENABLED(CONFIG_NVMAP_PAGE_POOL_DEBUG)) {
+			nvmap_pgcount(page, false);
+			BUG_ON(page_count(page) != 1);
+		}
+	}
+
+	rt_mutex_unlock(&pool->lock);
+
+	/* Zero non-zeroed pages, if any */
+	if (non_zero_cnt)
+		nvmap_pp_zero_pages(&pages[non_zero_idx], non_zero_cnt);
+
+	pp_alloc_add(pool, ind);
+	pp_hit_add(pool, ind);
+	pp_miss_add(pool, nr - ind);
+
+	trace_nvmap_pp_alloc_lots(ind, nr);
+
+	return ind;
+}
+
+int nvmap_page_pool_alloc_lots_bp(struct nvmap_page_pool *pool,
+				struct page **pages, u32 nr)
+{
+	int ind = 0, nr_pages = nr;
+	struct page *page;
+
+	if (!enable_pp || pool->pages_per_big_pg <= 1 ||
+	    nr_pages < pool->pages_per_big_pg)
+		return 0;
+
+	rt_mutex_lock(&pool->lock);
+
+	while (nr_pages - ind >= pool->pages_per_big_pg) {
+		int i;
+
+		page = get_page_list_page_bp(pool);
+		if (!page)
+			break;
+
+		for (i = 0; i < pool->pages_per_big_pg; i++)
+			pages[ind + i] = nth_page(page, i);
+
+		ind += pool->pages_per_big_pg;
+	}
+
+	rt_mutex_unlock(&pool->lock);
+	return ind;
+}
+
+static bool nvmap_is_big_page(struct nvmap_page_pool *pool,
+			      struct page **pages, int idx, int nr)
+{
+	int i;
+	struct page *page = pages[idx];
+
+	if (pool->pages_per_big_pg <= 1)
+		return false;
+
+	if (nr - idx < pool->pages_per_big_pg)
+		return false;
+
+	/* Allow coalescing pages at big page boundary only */
+	if (page_to_phys(page) & (pool->big_pg_sz - 1))
+		return false;
+
+	for (i = 1; i < pool->pages_per_big_pg; i++)
+		if (pages[idx + i] != nth_page(page, i))
+			break;
+
+	return i == pool->pages_per_big_pg ? true: false;
+}
+
+/*
+ * Fill a bunch of pages into the page pool. This will fill as many as it can
+ * and return the number of pages filled. Pages are used from the start of the
+ * passed page pointer array in a linear fashion.
+ *
+ * You must lock the page pool before using this.
+ */
+static int __nvmap_page_pool_fill_lots_locked(struct nvmap_page_pool *pool,
+				       struct page **pages, u32 nr)
+{
+	int real_nr;
+	int ind = 0;
+
+	if (!enable_pp)
+		return 0;
+
+	real_nr = min_t(u32, pool->max - pool->count, nr);
+	BUG_ON(real_nr < 0);
+	if (real_nr == 0)
+		return 0;
+
+	while (real_nr > 0) {
+		if (IS_ENABLED(CONFIG_NVMAP_PAGE_POOL_DEBUG)) {
+			nvmap_pgcount(pages[ind], true);
+			BUG_ON(page_count(pages[ind]) != 2);
+		}
+
+		if (nvmap_is_big_page(pool, pages, ind, nr)) {
+			list_add_tail(&pages[ind]->lru, &pool->page_list_bp);
+			ind += pool->pages_per_big_pg;
+			real_nr -= pool->pages_per_big_pg;
+			pool->big_page_count += pool->pages_per_big_pg;
+		} else {
+			list_add_tail(&pages[ind++]->lru, &pool->page_list);
+			real_nr--;
+		}
+	}
+
+	pool->count += ind;
+	BUG_ON(pool->count > pool->max);
+	pp_fill_add(pool, ind);
+
+	return ind;
+}
+
+int nvmap_page_pool_fill_lots(struct nvmap_page_pool *pool,
+				       struct page **pages, u32 nr)
+{
+	int ret = 0;
+	int i;
+	u32 save_to_zero;
+
+	rt_mutex_lock(&pool->lock);
+
+	save_to_zero = pool->to_zero;
+
+	ret = min(nr, pool->max - pool->count - pool->to_zero - pool->under_zero);
+
+	for (i = 0; i < ret; i++) {
+		/* If page has additonal referecnces, Don't add it into
+		 * page pool. get_user_pages() on mmap'ed nvmap handle can
+		 * hold a refcount on the page. These pages can't be
+		 * reused till the additional refs are dropped.
+		 */
+		if (page_count(pages[i]) > 1) {
+			__free_page(pages[i]);
+		} else {
+			list_add_tail(&pages[i]->lru, &pool->zero_list);
+			pool->to_zero++;
+		}
+	}
+
+	if (pool->to_zero)
+		wake_up_interruptible(&nvmap_bg_wait);
+	ret = i;
+
+	trace_nvmap_pp_fill_zero_lots(save_to_zero, pool->to_zero,
+			ret, nr);
+
+	rt_mutex_unlock(&pool->lock);
+
+	return ret;
+}
+
+ulong nvmap_page_pool_get_unused_pages(void)
+{
+	int total = 0;
+
+	if (!nvmap_dev)
+		return 0;
+
+	total = nvmap_dev->pool.count + nvmap_dev->pool.to_zero;
+
+	return total;
+}
+
+/*
+ * Remove and free to the system all the pages currently in the page
+ * pool. This operation will happen even if the page pools are disabled.
+ */
+int nvmap_page_pool_clear(void)
+{
+	struct nvmap_page_pool *pool = &nvmap_dev->pool;
+
+	rt_mutex_lock(&pool->lock);
+
+	(void)nvmap_page_pool_free_pages_locked(pool, pool->count + pool->to_zero);
+
+	/* For some reason, if an error occured... */
+	if (!list_empty(&pool->page_list) || !list_empty(&pool->zero_list)) {
+		rt_mutex_unlock(&pool->lock);
+		return -ENOMEM;
+	}
+
+	rt_mutex_unlock(&pool->lock);
+
+	return 0;
+}
+
+/*
+ * Resizes the page pool to the passed size. If the passed size is 0 then
+ * all associated resources are released back to the system. This operation
+ * will only occur if the page pools are enabled.
+ */
+static void nvmap_page_pool_resize(struct nvmap_page_pool *pool, u32 size)
+{
+	u32 curr;
+
+	rt_mutex_lock(&pool->lock);
+
+	curr = nvmap_page_pool_get_unused_pages();
+	if (curr > size)
+		(void)nvmap_page_pool_free_pages_locked(pool, curr - size);
+
+	pr_debug("page pool resized to %d from %d pages\n", size, pool->max);
+	pool->max = size;
+
+	rt_mutex_unlock(&pool->lock);
+}
+
+static unsigned long nvmap_page_pool_count_objects(struct shrinker *shrinker,
+						   struct shrink_control *sc)
+{
+	return nvmap_page_pool_get_unused_pages();
+}
+
+static unsigned long nvmap_page_pool_scan_objects(struct shrinker *shrinker,
+						  struct shrink_control *sc)
+{
+	unsigned long remaining;
+
+	pr_debug("sh_pages=%lu", sc->nr_to_scan);
+
+	rt_mutex_lock(&nvmap_dev->pool.lock);
+	remaining = nvmap_page_pool_free_pages_locked(
+			&nvmap_dev->pool, sc->nr_to_scan);
+	rt_mutex_unlock(&nvmap_dev->pool.lock);
+
+	return (remaining == sc->nr_to_scan) ? \
+			   SHRINK_STOP : (sc->nr_to_scan - remaining);
+}
+
+static struct shrinker nvmap_page_pool_shrinker = {
+	.count_objects = nvmap_page_pool_count_objects,
+	.scan_objects = nvmap_page_pool_scan_objects,
+	.seeks = 1,
+};
+
+static void shrink_page_pools(int *total_pages, int *available_pages)
+{
+	struct shrink_control sc;
+
+	if (*total_pages == 0) {
+		sc.gfp_mask = GFP_KERNEL;
+		sc.nr_to_scan = 0;
+		*total_pages = nvmap_page_pool_count_objects(NULL, &sc);
+	}
+	sc.nr_to_scan = *total_pages;
+	nvmap_page_pool_scan_objects(NULL, &sc);
+	*available_pages = nvmap_page_pool_count_objects(NULL, &sc);
+}
+
+#if NVMAP_TEST_PAGE_POOL_SHRINKER
+static int shrink_pp;
+static int shrink_set(const char *arg, const struct kernel_param *kp)
+{
+	int cpu = smp_processor_id();
+	unsigned long long t1, t2;
+	int total_pages, available_pages;
+
+	param_set_int(arg, kp);
+
+	if (shrink_pp) {
+		total_pages = shrink_pp;
+		t1 = cpu_clock(cpu);
+		shrink_page_pools(&total_pages, &available_pages);
+		t2 = cpu_clock(cpu);
+		pr_debug("shrink page pools: time=%lldns, "
+			"total_pages_released=%d, free_pages_available=%d",
+			t2-t1, total_pages, available_pages);
+	}
+	return 0;
+}
+
+static int shrink_get(char *buff, const struct kernel_param *kp)
+{
+	return param_get_int(buff, kp);
+}
+
+static struct kernel_param_ops shrink_ops = {
+	.get = shrink_get,
+	.set = shrink_set,
+};
+
+module_param_cb(shrink_page_pools, &shrink_ops, &shrink_pp, 0644);
+#endif
+
+static int enable_pp_set(const char *arg, const struct kernel_param *kp)
+{
+	int ret;
+
+	ret = param_set_bool(arg, kp);
+	if (ret)
+		return ret;
+
+	if (!enable_pp)
+		nvmap_page_pool_clear();
+
+	return 0;
+}
+
+static int enable_pp_get(char *buff, const struct kernel_param *kp)
+{
+	return param_get_bool(buff, kp);
+}
+
+static struct kernel_param_ops enable_pp_ops = {
+	.get = enable_pp_get,
+	.set = enable_pp_set,
+};
+
+module_param_cb(enable_page_pools, &enable_pp_ops, &enable_pp, 0644);
+
+static int pool_size_set(const char *arg, const struct kernel_param *kp)
+{
+	int ret = param_set_uint(arg, kp);
+
+	if (!ret && (pool_size != nvmap_dev->pool.max))
+		nvmap_page_pool_resize(&nvmap_dev->pool, pool_size);
+
+	return ret;
+}
+
+static int pool_size_get(char *buff, const struct kernel_param *kp)
+{
+	return param_get_int(buff, kp);
+}
+
+static struct kernel_param_ops pool_size_ops = {
+	.get = pool_size_get,
+	.set = pool_size_set,
+};
+
+module_param_cb(pool_size, &pool_size_ops, &pool_size, 0644);
+
+int nvmap_page_pool_debugfs_init(struct dentry *nvmap_root)
+{
+	struct dentry *pp_root;
+
+	if (!nvmap_root)
+		return -ENODEV;
+
+	pp_root = debugfs_create_dir("pagepool", nvmap_root);
+	if (!pp_root)
+		return -ENODEV;
+
+	debugfs_create_u32("page_pool_available_pages",
+			   S_IRUGO, pp_root,
+			   &nvmap_dev->pool.count);
+	debugfs_create_u32("page_pool_pages_to_zero",
+			   S_IRUGO, pp_root,
+			   &nvmap_dev->pool.to_zero);
+	debugfs_create_u32("page_pool_available_big_pages",
+			   S_IRUGO, pp_root,
+			   &nvmap_dev->pool.big_page_count);
+	debugfs_create_u32("page_pool_big_page_size",
+			   S_IRUGO, pp_root,
+			   &nvmap_dev->pool.big_pg_sz);
+	debugfs_create_u64("total_big_page_allocs",
+			   S_IRUGO, pp_root,
+			   &nvmap_big_page_allocs);
+	debugfs_create_u64("total_page_allocs",
+			   S_IRUGO, pp_root,
+			   &nvmap_total_page_allocs);
+
+#ifdef CONFIG_NVMAP_PAGE_POOL_DEBUG
+	debugfs_create_u64("page_pool_allocs",
+			   S_IRUGO, pp_root,
+			   &nvmap_dev->pool.allocs);
+	debugfs_create_u64("page_pool_fills",
+			   S_IRUGO, pp_root,
+			   &nvmap_dev->pool.fills);
+	debugfs_create_u64("page_pool_hits",
+			   S_IRUGO, pp_root,
+			   &nvmap_dev->pool.hits);
+	debugfs_create_u64("page_pool_misses",
+			   S_IRUGO, pp_root,
+			   &nvmap_dev->pool.misses);
+#endif
+
+	return 0;
+}
+
+int nvmap_page_pool_init(struct nvmap_device *dev)
+{
+	struct sysinfo info;
+	struct nvmap_page_pool *pool = &dev->pool;
+
+	memset(pool, 0x0, sizeof(*pool));
+	rt_mutex_init(&pool->lock);
+	INIT_LIST_HEAD(&pool->page_list);
+	INIT_LIST_HEAD(&pool->zero_list);
+	INIT_LIST_HEAD(&pool->page_list_bp);
+
+	pool->big_pg_sz = NVMAP_PP_BIG_PAGE_SIZE;
+	pool->pages_per_big_pg = NVMAP_PP_BIG_PAGE_SIZE >> PAGE_SHIFT;
+
+	si_meminfo(&info);
+	pr_info("Total RAM pages: %lu\n", info.totalram);
+
+	if (!CONFIG_NVMAP_PAGE_POOL_SIZE)
+		/* The ratio is pool pages per 1K ram pages.
+		 * So, the >> 10 */
+		pool->max = (info.totalram * NVMAP_PP_POOL_SIZE) >> 10;
+	else
+		pool->max = CONFIG_NVMAP_PAGE_POOL_SIZE;
+
+	if (pool->max >= info.totalram)
+		goto fail;
+	pool_size = pool->max;
+
+	pr_info("nvmap page pool size: %u pages (%u MB)\n", pool->max,
+		(pool->max * info.mem_unit) >> 20);
+
+	background_allocator = kthread_run(nvmap_background_zero_thread,
+					    NULL, "nvmap-bz");
+	if (IS_ERR(background_allocator))
+		goto fail;
+
+	register_shrinker(&nvmap_page_pool_shrinker);
+
+	return 0;
+fail:
+	nvmap_page_pool_fini(dev);
+	return -ENOMEM;
+}
+
+int nvmap_page_pool_fini(struct nvmap_device *dev)
+{
+	struct nvmap_page_pool *pool = &dev->pool;
+
+	/*
+	 * if background allocator is not initialzed or not
+	 * properly initialized, then shrinker is also not
+	 * registered
+	 */
+	if (!IS_ERR_OR_NULL(background_allocator)) {
+		unregister_shrinker(&nvmap_page_pool_shrinker);
+		kthread_stop(background_allocator);
+	}
+
+	WARN_ON(!list_empty(&pool->page_list));
+
+	return 0;
+}
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_pp.h b/drivers/video/tegra/nvmap/nv2/nvmap_pp.h
new file mode 100644
index 000000000000..131e81054a3d
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_pp.h
@@ -0,0 +1,64 @@
+/*
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __NVMAP_PP_H
+#define __NVMAP_PP_H
+
+#ifdef CONFIG_NVMAP_PAGE_POOLS
+
+struct nvmap_device;
+/*
+ * This is the default ratio defining pool size. It can be thought of as pool
+ * size in either MB per GB or KB per MB. That means the max this number can
+ * be is 1024 (all physical memory - not a very good idea) or 0 (no page pool
+ * at all).
+ */
+#define NVMAP_PP_POOL_SIZE               (128)
+
+#define NVMAP_PP_BIG_PAGE_SIZE           (0x10000)
+
+struct nvmap_page_pool {
+	struct rt_mutex lock;
+	u32 count;      /* Number of pages in the page & dirty list. */
+	u32 max;        /* Max no. of pages in all lists. */
+	u32 to_zero;    /* Number of pages on the zero list */
+	u32 under_zero; /* Number of pages getting zeroed */
+	u32 big_pg_sz;  /* big page size supported(64k, etc.) */
+	u32 big_page_count;   /* Number of zeroed big pages avaialble */
+	u32 pages_per_big_pg; /* Number of pages in big page */
+	struct list_head page_list;
+	struct list_head zero_list;
+	struct list_head page_list_bp;
+
+#ifdef CONFIG_NVMAP_PAGE_POOL_DEBUG
+	u64 allocs;
+	u64 fills;
+	u64 hits;
+	u64 misses;
+#endif
+};
+
+int nvmap_page_pool_init(struct nvmap_device *dev);
+int nvmap_page_pool_fini(struct nvmap_device *dev);
+struct page *nvmap_page_pool_alloc(struct nvmap_page_pool *pool);
+int nvmap_page_pool_alloc_lots(struct nvmap_page_pool *pool,
+					struct page **pages, u32 nr);
+int nvmap_page_pool_alloc_lots_bp(struct nvmap_page_pool *pool,
+					struct page **pages, u32 nr);
+int nvmap_page_pool_fill_lots(struct nvmap_page_pool *pool,
+				       struct page **pages, u32 nr);
+int nvmap_page_pool_clear(void);
+int nvmap_page_pool_debugfs_init(struct dentry *nvmap_root);
+#endif
+
+#endif /* __NVMAP_PP_H */
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_stats.c b/drivers/video/tegra/nvmap/nv2/nvmap_stats.c
new file mode 100644
index 000000000000..08bdc339b21a
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_stats.c
@@ -0,0 +1,107 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_stats.c
+ *
+ * Nvmap Stats keeping
+ *
+ * Copyright (c) 2011-2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#include <linux/debugfs.h>
+#include <linux/atomic.h>
+
+#include "nvmap_stats.h"
+
+struct nvmap_stats nvmap_stats;
+
+static int nvmap_stats_reset(void *data, u64 val)
+{
+	int i;
+
+	if (val) {
+		atomic64_set(&nvmap_stats.collect, 0);
+		for (i = 0; i < NS_NUM; i++) {
+			if (i == NS_TOTAL)
+				continue;
+			atomic64_set(&nvmap_stats.stats[i], 0);
+		}
+	}
+	return 0;
+}
+
+static int nvmap_stats_get(void *data, u64 *val)
+{
+	atomic64_t *ptr = data;
+
+	*val = atomic64_read(ptr);
+	return 0;
+}
+
+static int nvmap_stats_set(void *data, u64 val)
+{
+	atomic64_t *ptr = data;
+
+	atomic64_set(ptr, val);
+	return 0;
+}
+
+DEFINE_SIMPLE_ATTRIBUTE(reset_stats_fops, NULL, nvmap_stats_reset, "%llu\n");
+DEFINE_SIMPLE_ATTRIBUTE(stats_fops, nvmap_stats_get, nvmap_stats_set, "%llu\n");
+
+void nvmap_stats_init(struct dentry *nvmap_debug_root)
+{
+	struct dentry *stats_root;
+
+#define CREATE_DF(x, y) \
+	debugfs_create_file(#x, S_IRUGO, stats_root, &y, &stats_fops);
+
+	stats_root = debugfs_create_dir("stats", nvmap_debug_root);
+	if (!IS_ERR_OR_NULL(stats_root)) {
+		CREATE_DF(alloc, nvmap_stats.stats[NS_ALLOC]);
+		CREATE_DF(release, nvmap_stats.stats[NS_RELEASE]);
+		CREATE_DF(ualloc, nvmap_stats.stats[NS_UALLOC]);
+		CREATE_DF(urelease, nvmap_stats.stats[NS_URELEASE]);
+		CREATE_DF(kalloc, nvmap_stats.stats[NS_KALLOC]);
+		CREATE_DF(krelease, nvmap_stats.stats[NS_KRELEASE]);
+		CREATE_DF(cflush_rq, nvmap_stats.stats[NS_CFLUSH_RQ]);
+		CREATE_DF(cflush_done, nvmap_stats.stats[NS_CFLUSH_DONE]);
+		CREATE_DF(ucflush_rq, nvmap_stats.stats[NS_UCFLUSH_RQ]);
+		CREATE_DF(ucflush_done, nvmap_stats.stats[NS_UCFLUSH_DONE]);
+		CREATE_DF(kcflush_rq, nvmap_stats.stats[NS_KCFLUSH_RQ]);
+		CREATE_DF(kcflush_done, nvmap_stats.stats[NS_KCFLUSH_DONE]);
+		CREATE_DF(total_memory, nvmap_stats.stats[NS_TOTAL]);
+
+		debugfs_create_file("collect", S_IRUGO | S_IWUSR,
+			stats_root, &nvmap_stats.collect, &stats_fops);
+		debugfs_create_file("reset", S_IWUSR,
+			stats_root, NULL, &reset_stats_fops);
+	}
+
+#undef CREATE_DF
+}
+
+void nvmap_stats_inc(enum nvmap_stats_t stat, size_t size)
+{
+	if (atomic64_read(&nvmap_stats.collect) || stat == NS_TOTAL)
+		atomic64_add(size, &nvmap_stats.stats[stat]);
+}
+
+void nvmap_stats_dec(enum nvmap_stats_t stat, size_t size)
+{
+	if (atomic64_read(&nvmap_stats.collect) || stat == NS_TOTAL)
+		atomic64_sub(size, &nvmap_stats.stats[stat]);
+}
+
+u64 nvmap_stats_read(enum nvmap_stats_t stat)
+{
+	return atomic64_read(&nvmap_stats.stats[stat]);
+}
+
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_stats.h b/drivers/video/tegra/nvmap/nv2/nvmap_stats.h
new file mode 100644
index 000000000000..3e2d3e806c87
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_stats.h
@@ -0,0 +1,45 @@
+/*
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __VIDEO_TEGRA_NVMAP_STATS_H
+#define __VIDEO_TEGRA_NVMAP_STATS_H
+
+enum nvmap_stats_t {
+	NS_ALLOC = 0,
+	NS_RELEASE,
+	NS_UALLOC,
+	NS_URELEASE,
+	NS_KALLOC,
+	NS_KRELEASE,
+	NS_CFLUSH_RQ,
+	NS_CFLUSH_DONE,
+	NS_UCFLUSH_RQ,
+	NS_UCFLUSH_DONE,
+	NS_KCFLUSH_RQ,
+	NS_KCFLUSH_DONE,
+	NS_TOTAL,
+	NS_NUM,
+};
+
+struct nvmap_stats {
+	atomic64_t stats[NS_NUM];
+	atomic64_t collect;
+};
+
+extern struct nvmap_stats nvmap_stats;
+
+void nvmap_stats_init(struct dentry *nvmap_debug_root);
+void nvmap_stats_inc(enum nvmap_stats_t, size_t size);
+void nvmap_stats_dec(enum nvmap_stats_t, size_t size);
+u64 nvmap_stats_read(enum nvmap_stats_t);
+#endif /* __VIDEO_TEGRA_NVMAP_STATS_H */
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_structs.h b/drivers/video/tegra/nvmap/nv2/nvmap_structs.h
new file mode 100644
index 000000000000..e5b0c498dc4a
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_structs.h
@@ -0,0 +1,26 @@
+/*
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __NVMAP_STRUCTS_H
+#define __NVMAP_STRUCTS_H
+
+struct nvmap_handle;
+struct nvmap_handle_info;
+struct nvmap_handle_ref;
+
+struct nvmap_client;
+
+struct nvmap_carveout_node;
+struct nvmap_heap;
+
+#endif /* __NVMAP_STRUCTS_H */
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_tag.c b/drivers/video/tegra/nvmap/nv2/nvmap_tag.c
new file mode 100644
index 000000000000..e3154add0ebb
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_tag.c
@@ -0,0 +1,116 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_tag.c
+ *
+ * Allocation tag routines for nvmap
+ *
+ * Copyright (c) 2016-2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"%s: " fmt, __func__
+
+#include <linux/rtmutex.h>
+#include <linux/rbtree.h>
+#include <linux/moduleparam.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+
+#include <trace/events/nvmap.h>
+#include "nvmap_tag.h"
+
+struct nvmap_tag_entry *nvmap_search_tag_entry(struct rb_root *root, u32 tag)
+{
+	struct rb_node *node = root->rb_node;  /* top of the tree */
+	struct nvmap_tag_entry *entry;
+
+	while (node) {
+		entry = rb_entry(node, struct nvmap_tag_entry, node);
+
+		if (entry->tag > tag)
+			node = node->rb_left;
+		else if (entry->tag < tag)
+			node = node->rb_right;
+		else
+			return entry;  /* Found it */
+	}
+	return NULL;
+}
+
+
+static void nvmap_insert_tag_entry(struct rb_root *root,
+		struct nvmap_tag_entry *new)
+{
+	struct rb_node **link = &root->rb_node;
+	struct rb_node *parent = NULL;
+	struct nvmap_tag_entry *entry;
+	u32 tag = new->tag;
+
+	/* Go to the bottom of the tree */
+	while (*link) {
+	    parent = *link;
+	    entry = rb_entry(parent, struct nvmap_tag_entry, node);
+
+	    if (entry->tag > tag)
+		link = &parent->rb_left;
+	    else
+		link = &parent->rb_right;
+	}
+
+	/* Put the new node there */
+	rb_link_node(&new->node, parent, link);
+	rb_insert_color(&new->node, root);
+}
+
+
+int nvmap_define_tag(struct nvmap_device *dev, u32 tag,
+		const char __user *name, u32 len)
+{
+	struct nvmap_tag_entry *new;
+	struct nvmap_tag_entry *old;
+
+	new = kzalloc(sizeof(struct nvmap_tag_entry) + len + 1, GFP_KERNEL);
+	if (!new)
+		return -ENOMEM;
+
+	if (copy_from_user(new + 1, name, len)) {
+		kfree(new);
+		return -EFAULT;
+	}
+
+	new->tag = tag;
+
+	mutex_lock(&dev->tags_lock);
+	old = nvmap_search_tag_entry(&dev->tags, tag);
+	if (old) {
+		rb_replace_node(&old->node, &new->node, &dev->tags);
+		kfree(old);
+	} else {
+		nvmap_insert_tag_entry(&dev->tags, new);
+	}
+	mutex_unlock(&dev->tags_lock);
+
+	return 0;
+}
+
+int nvmap_remove_tag(struct nvmap_device *dev, u32 tag)
+{
+	struct nvmap_tag_entry *old;
+
+	mutex_lock(&dev->tags_lock);
+	old = nvmap_search_tag_entry(&dev->tags, tag);
+	if (old){
+		rb_erase(&old->node, &dev->tags);
+		kfree(old);
+	}
+	mutex_unlock(&dev->tags_lock);
+
+	return 0;
+}
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_tag.h b/drivers/video/tegra/nvmap/nv2/nvmap_tag.h
new file mode 100644
index 000000000000..963b28d35e3c
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_tag.h
@@ -0,0 +1,82 @@
+/*
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __NVMAP_TAG_H
+#define __NVMAP_TAG_H
+
+#include "nvmap_dev.h"
+#include "nvmap_client.h"
+#include "nvmap_handle_ref.h"
+
+// TODO We need to fix these args
+/*
+#define NVMAP_TP_ARGS_H(handle)					      	      \
+	handle,								      \
+	atomic_read(&handle->share_count),				      \
+	handle->heap_type == NVMAP_HEAP_IOVMM ? 0 : 			      \
+			(handle->carveout ? handle->carveout->base : 0),      \
+	handle->size,							      \
+	(handle->userflags & 0xFFFF),                                         \
+	(handle->userflags >> 16),					      \
+	__nvmap_tag_name(nvmap_dev, handle->userflags >> 16)
+	*/
+
+#define NVMAP_TP_ARGS_H(handle)					      	      \
+	NULL,								      \
+	0, \
+	0, \
+	0, \
+	0, \
+	0, \
+	__nvmap_tag_name(nvmap_dev, 0)
+
+#define NVMAP_TAG_LABEL_MAXLEN	(63 - sizeof(struct nvmap_tag_entry))
+
+#define NVMAP_TP_ARGS_CHR(client, handle, ref)			      	      \
+	client,                                                               \
+	client ? nvmap_client_pid((struct nvmap_client *)client) : 0,         \
+	(ref) ? nvmap_handle_ref_count(ref) : 1,    \
+	NVMAP_TP_ARGS_H(handle)
+
+#define NVMAP_TAG_TRACE(x, ...) 			\
+do {                                                    \
+	if (x##_enabled()) {                            \
+		mutex_lock(&nvmap_dev->tags_lock);      \
+		x(__VA_ARGS__);                         \
+		mutex_unlock(&nvmap_dev->tags_lock);    \
+	}                                               \
+} while (0)
+
+struct nvmap_tag_entry {
+	struct rb_node node;
+	atomic_t ref;		/* reference count (i.e., # of duplications) */
+	u32 tag;
+};
+
+struct nvmap_tag_entry *nvmap_search_tag_entry(struct rb_root *root, u32 tag);
+
+int nvmap_define_tag(struct nvmap_device *dev, u32 tag,
+	const char __user *name, u32 len);
+
+int nvmap_remove_tag(struct nvmap_device *dev, u32 tag);
+
+/* must hold tag_lock */
+static inline char *__nvmap_tag_name(struct nvmap_device *dev, u32 tag)
+{
+	struct nvmap_tag_entry *entry;
+
+	entry = nvmap_search_tag_entry(&dev->tags, tag);
+	return entry ? (char *)(entry + 1) : "";
+}
+#endif /* __NVMAP_TAG_H */
+
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_vma.c b/drivers/video/tegra/nvmap/nv2/nvmap_vma.c
new file mode 100644
index 000000000000..dcd58bc87a76
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_vma.c
@@ -0,0 +1,162 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_vma.c
+ *
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"nvmap: %s() " fmt, __func__
+
+#include <linux/kernel.h>
+#include <linux/mm.h>
+#include <linux/mm_types.h>
+#include <linux/list.h>
+#include <linux/version.h>
+
+#include "nvmap_vma.h"
+#include "nvmap_handle.h"
+
+extern struct vm_operations_struct nvmap_vma_ops;
+
+int nvmap_vma_is_nvmap(struct vm_area_struct *vma)
+{
+	return vma->vm_ops == &nvmap_vma_ops;
+}
+
+int nvmap_vma_belongs_to_handle(struct vm_area_struct *vma,
+					struct nvmap_handle *h)
+{
+	struct nvmap_vma_priv *priv;
+
+	priv = (struct nvmap_vma_priv *) vma->vm_private_data;
+
+	return (priv->handle == h);
+}
+
+static void nvmap_zap_page_range(struct vm_area_struct *vma,
+		unsigned long start, unsigned long size)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	zap_page_range(vma, start, size);
+#else
+	zap_page_range(vma, start, size, NULL);
+#endif
+}
+
+void nvmap_vma_zap(struct list_head *vmas, u64 offset, u64 size)
+{
+	struct nvmap_vma_list *vma_list;
+	struct vm_area_struct *vma;
+
+	list_for_each_entry(vma_list, vmas, list) {
+		struct nvmap_vma_priv *priv;
+		size_t vm_size = size;
+
+		vma = vma_list->vma;
+		priv = vma->vm_private_data;
+		if ((offset + size) > (vma->vm_end - vma->vm_start))
+			vm_size = vma->vm_end - vma->vm_start - offset;
+
+		if (priv->offs || vma->vm_pgoff) {
+			/* vma mapping starts in the middle of handle memory.
+			 * zapping needs special care. zap entire range for now.
+			 * FIXME: optimze zapping.
+			 */
+			nvmap_zap_page_range(vma, vma->vm_start,
+					vma->vm_end - vma->vm_start);
+		} else {
+			nvmap_zap_page_range(vma, vma->vm_start + offset,
+						vm_size);
+		}
+	}
+}
+
+static int nvmap_vma_list_prot_none(struct nvmap_vma_list *vma_list,
+				struct vm_area_struct *vma,
+				struct vm_area_struct *prev,
+				size_t vm_size,
+				int handle_is_dirty)
+{
+	int err = 0;
+
+	vma->vm_flags = vma_list->save_vm_flags;
+	(void)vma_set_page_prot(vma);
+
+	if (!handle_is_dirty)
+		return 0;
+
+	err = mprotect_fixup(vma, &prev, vma->vm_start,
+			vma->vm_start + vm_size, VM_NONE);
+	if (err)
+		return err;
+
+	vma->vm_flags = vma_list->save_vm_flags;
+	(void)vma_set_page_prot(vma);
+
+	return err;
+}
+
+static int nvmap_vma_list_prot_restore(struct nvmap_vma_list *vma_list,
+				struct vm_area_struct *vma,
+				struct vm_area_struct *prev,
+				size_t vm_size)
+{
+	int err = 0;
+
+	vma->vm_flags = VM_NONE;
+	(void)vma_set_page_prot(vma);
+
+	err = mprotect_fixup(vma, &prev, vma->vm_start,
+			vma->vm_start + vm_size,
+			vma_list->save_vm_flags);
+	return err;
+}
+
+int nvmap_vma_list_prot(struct nvmap_vma_list *vma_list, u64 offset,
+					u64 size, int handle_is_dirty, int op)
+{
+	struct vm_area_struct *vma = vma_list->vma;
+	struct nvmap_vma_priv *priv = vma->vm_private_data;
+	struct vm_area_struct *prev = vma->vm_prev;
+	size_t vm_size;
+	int err = 0;
+
+	vm_size = size;
+
+	if ((offset + size) > (vma->vm_end - vma->vm_start))
+		vm_size = vma->vm_end - vma->vm_start - offset;
+
+	if ((priv->offs || vma->vm_pgoff) ||
+			(size > (vma->vm_end - vma->vm_start)))
+		vm_size = vma->vm_end - vma->vm_start;
+
+	if (vma->vm_mm != current->mm)
+		down_write(&vma->vm_mm->mmap_sem);
+
+	switch (op) {
+		case NVMAP_HANDLE_PROT_NONE:
+			err = nvmap_vma_list_prot_none(vma_list, vma,
+							prev, vm_size,
+							handle_is_dirty);
+			break;
+		case NVMAP_HANDLE_PROT_RESTORE:
+			err = nvmap_vma_list_prot_restore(vma_list, vma,
+							prev, vm_size);
+			break;
+		default:
+			BUG();
+	};
+
+	if (vma->vm_mm != current->mm)
+		up_write(&vma->vm_mm->mmap_sem);
+
+	return err;
+}
diff --git a/drivers/video/tegra/nvmap/nv2/nvmap_vma.h b/drivers/video/tegra/nvmap/nv2/nvmap_vma.h
new file mode 100644
index 000000000000..c03e5e9725c6
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nv2/nvmap_vma.h
@@ -0,0 +1,43 @@
+/*
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __NVMAP_VMA_H
+#define __NVMAP_VMA_H
+
+#include "nvmap_structs.h"
+
+// TODO pretty sure list and priv can be merged into one structure
+struct nvmap_vma_list {
+	struct list_head list;
+	struct vm_area_struct *vma;
+	unsigned long save_vm_flags;
+	pid_t pid;
+	atomic_t ref;
+};
+
+struct nvmap_vma_priv {
+	struct nvmap_handle *handle;
+	size_t		offs;
+	atomic_t	count;	/* number of processes cloning the VMA */
+};
+
+int nvmap_vma_is_nvmap(struct vm_area_struct *vma);
+
+int nvmap_vma_belongs_to_handle(struct vm_area_struct *vma,
+					struct nvmap_handle *h);
+void nvmap_vma_zap(struct list_head *vmas, u64 offset, u64 size);
+
+int nvmap_vma_list_prot(struct nvmap_vma_list *vma_list, u64 offset,
+					u64 size, int handle_is_dirty, int op);
+
+#endif /* __NVMAP_VMA_H */
diff --git a/drivers/video/tegra/nvmap/nvmap.c b/drivers/video/tegra/nvmap/nvmap.c
new file mode 100644
index 000000000000..1168b4fda907
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nvmap.c
@@ -0,0 +1,315 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap.c
+ *
+ * Memory manager for Tegra GPU
+ *
+ * Copyright (c) 2009-2021, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"nvmap: %s() " fmt, __func__
+
+#include <linux/err.h>
+#include <linux/highmem.h>
+#include <linux/io.h>
+#include <linux/rbtree.h>
+#include <linux/vmalloc.h>
+#include <linux/wait.h>
+#include <linux/slab.h>
+#include <linux/export.h>
+
+#include <asm/pgtable.h>
+#include <asm/tlbflush.h>
+
+#include <linux/nvmap.h>
+#include <trace/events/nvmap.h>
+
+#include "nvmap_priv.h"
+
+static phys_addr_t handle_phys(struct nvmap_handle *h)
+{
+	if (h->heap_pgalloc)
+		BUG();
+	return h->carveout->base;
+}
+
+void *__nvmap_kmap(struct nvmap_handle *h, unsigned int pagenum)
+{
+	phys_addr_t paddr;
+	unsigned long kaddr;
+	pgprot_t prot;
+	struct vm_struct *area = NULL;
+
+	if (!virt_addr_valid(h))
+		return NULL;
+
+	h = nvmap_handle_get(h);
+	if (!h)
+		return NULL;
+	/*
+	 * If the handle is RO and virtual mapping is requested in
+	 * kernel address space, return error.
+	 */
+	if (h->from_va && h->is_ro)
+		goto put_handle;
+
+	if (!h->alloc)
+		goto put_handle;
+
+	if (!(h->heap_type & nvmap_dev->cpu_access_mask))
+		goto put_handle;
+
+	nvmap_kmaps_inc(h);
+	if (pagenum >= h->size >> PAGE_SHIFT)
+		goto out;
+
+	if (h->vaddr) {
+		kaddr = (unsigned long)h->vaddr + pagenum * PAGE_SIZE;
+	} else {
+		prot = nvmap_pgprot(h, PG_PROT_KERNEL);
+		area = get_vm_area(PAGE_SIZE, 0);
+		if (!area)
+			goto out;
+		kaddr = (ulong)area->addr;
+
+		if (h->heap_pgalloc)
+			paddr = page_to_phys(nvmap_to_page(
+						h->pgalloc.pages[pagenum]));
+		else
+			paddr = h->carveout->base + pagenum * PAGE_SIZE;
+
+		ioremap_page_range(kaddr, kaddr + PAGE_SIZE, paddr, prot);
+	}
+	return (void *)kaddr;
+out:
+	nvmap_kmaps_dec(h);
+put_handle:
+	nvmap_handle_put(h);
+	return NULL;
+}
+
+void __nvmap_kunmap(struct nvmap_handle *h, unsigned int pagenum,
+		  void *addr)
+{
+	phys_addr_t paddr;
+	struct vm_struct *area = NULL;
+
+	if (!h || !h->alloc ||
+	    WARN_ON(!virt_addr_valid(h)) ||
+	    WARN_ON(!addr) ||
+	    !(h->heap_type & nvmap_dev->cpu_access_mask))
+		return;
+
+	if (WARN_ON(pagenum >= h->size >> PAGE_SHIFT))
+		return;
+
+	if (h->vaddr && (h->vaddr == (addr - pagenum * PAGE_SIZE)))
+		goto out;
+
+	if (h->heap_pgalloc)
+		paddr = page_to_phys(nvmap_to_page(h->pgalloc.pages[pagenum]));
+	else
+		paddr = h->carveout->base + pagenum * PAGE_SIZE;
+
+	if (h->flags != NVMAP_HANDLE_UNCACHEABLE &&
+	    h->flags != NVMAP_HANDLE_WRITE_COMBINE) {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 9, 0)
+		__dma_flush_range(addr, addr + PAGE_SIZE);
+#else
+		dcache_clean_inval_poc((unsigned long)addr, PAGE_SIZE);
+#endif
+		outer_flush_range(paddr, paddr + PAGE_SIZE); /* FIXME */
+	}
+
+	area = find_vm_area(addr);
+	if (area)
+		free_vm_area(area);
+	else
+		WARN(1, "Invalid address passed");
+out:
+	nvmap_kmaps_dec(h);
+	nvmap_handle_put(h);
+}
+
+void *__nvmap_mmap(struct nvmap_handle *h)
+{
+	pgprot_t prot;
+	void *vaddr;
+	unsigned long adj_size;
+	struct vm_struct *v;
+	struct page **pages;
+
+	if (!virt_addr_valid(h))
+		return NULL;
+
+	h = nvmap_handle_get(h);
+	if (!h)
+		return NULL;
+	/*
+	 * If the handle is RO and virtual mapping is requested in
+	 * kernel address space, return error.
+	 */
+	if (h->from_va && h->is_ro)
+		goto put_handle;
+
+
+	if (!h->alloc)
+		goto put_handle;
+
+	if (!(h->heap_type & nvmap_dev->cpu_access_mask))
+		goto put_handle;
+
+	if (h->vaddr)
+		return h->vaddr;
+
+	nvmap_kmaps_inc(h);
+	prot = nvmap_pgprot(h, PG_PROT_KERNEL);
+
+	if (h->heap_pgalloc) {
+		pages = nvmap_pages(h->pgalloc.pages, h->size >> PAGE_SHIFT);
+		if (!pages)
+			goto out;
+
+		vaddr = vm_map_ram(pages, h->size >> PAGE_SHIFT, -1);
+		nvmap_altfree(pages, (h->size >> PAGE_SHIFT) * sizeof(*pages));
+		if (!vaddr && !h->vaddr)
+			goto out;
+
+		if (vaddr && atomic_long_cmpxchg((atomic_long_t *)&h->vaddr, 0, (long)vaddr)) {
+			nvmap_kmaps_dec(h);
+			vm_unmap_ram(vaddr, h->size >> PAGE_SHIFT);
+		}
+		return h->vaddr;
+	}
+
+	/* carveout - explicitly map the pfns into a vmalloc area */
+	adj_size = h->carveout->base & ~PAGE_MASK;
+	adj_size += h->size;
+	adj_size = PAGE_ALIGN(adj_size);
+
+	v = get_vm_area(adj_size, 0);
+	if (!v)
+		goto out;
+
+	vaddr = v->addr + (h->carveout->base & ~PAGE_MASK);
+	ioremap_page_range((ulong)v->addr, (ulong)v->addr + adj_size,
+		h->carveout->base & PAGE_MASK, prot);
+
+	if (vaddr && atomic_long_cmpxchg((atomic_long_t *)&h->vaddr, 0, (long)vaddr)) {
+		struct vm_struct *vm;
+
+		vaddr -= (h->carveout->base & ~PAGE_MASK);
+		vm = find_vm_area(vaddr);
+		BUG_ON(!vm);
+		free_vm_area(vm);
+		nvmap_kmaps_dec(h);
+	}
+
+	/* leave the handle ref count incremented by 1, so that
+	 * the handle will not be freed while the kernel mapping exists.
+	 * nvmap_handle_put will be called by unmapping this address */
+	return h->vaddr;
+out:
+	nvmap_kmaps_dec(h);
+put_handle:
+	nvmap_handle_put(h);
+	return NULL;
+}
+
+void __nvmap_munmap(struct nvmap_handle *h, void *addr)
+{
+	if (!h || !h->alloc ||
+	    WARN_ON(!virt_addr_valid(h)) ||
+	    WARN_ON(!addr) ||
+	    !(h->heap_type & nvmap_dev->cpu_access_mask))
+		return;
+
+	nvmap_handle_put(h);
+}
+
+void nvmap_handle_put(struct nvmap_handle *h)
+{
+	int cnt;
+
+	if (WARN_ON(!virt_addr_valid(h)))
+		return;
+	cnt = atomic_dec_return(&h->ref);
+
+	if (WARN_ON(cnt < 0)) {
+		pr_err("%s: %s put to negative references\n",
+			__func__, current->comm);
+	} else if (cnt == 0)
+		_nvmap_handle_free(h);
+}
+
+struct sg_table *__nvmap_sg_table(struct nvmap_client *client,
+		struct nvmap_handle *h)
+{
+	struct sg_table *sgt = NULL;
+	int err, npages;
+	struct page **pages;
+
+	if (!virt_addr_valid(h))
+		return ERR_PTR(-EINVAL);
+
+	h = nvmap_handle_get(h);
+	if (!h)
+		return ERR_PTR(-EINVAL);
+
+	if (!h->alloc) {
+		err = -EINVAL;
+		goto put_handle;
+	}
+
+	npages = PAGE_ALIGN(h->size) >> PAGE_SHIFT;
+	sgt = kzalloc(sizeof(*sgt), GFP_KERNEL);
+	if (!sgt) {
+		err = -ENOMEM;
+		goto err;
+	}
+
+	if (!h->heap_pgalloc) {
+		phys_addr_t paddr = handle_phys(h);
+		struct page *page = phys_to_page(paddr);
+
+		err = sg_alloc_table(sgt, 1, GFP_KERNEL);
+		if (err)
+			goto err;
+
+		sg_set_page(sgt->sgl, page, h->size, offset_in_page(paddr));
+	} else {
+		pages = nvmap_pages(h->pgalloc.pages, npages);
+		if (!pages) {
+			err = -ENOMEM;
+			goto err;
+		}
+		err = sg_alloc_table_from_pages(sgt, pages,
+				npages, 0, h->size, GFP_KERNEL);
+		nvmap_altfree(pages, npages * sizeof(*pages));
+		if (err)
+			goto err;
+	}
+	nvmap_handle_put(h);
+	return sgt;
+
+err:
+	kfree(sgt);
+put_handle:
+	nvmap_handle_put(h);
+	return ERR_PTR(err);
+}
+
+void __nvmap_free_sg_table(struct nvmap_client *client,
+		struct nvmap_handle *h, struct sg_table *sgt)
+{
+	sg_free_table(sgt);
+	kfree(sgt);
+}
diff --git a/drivers/video/tegra/nvmap/nvmap_alloc.c b/drivers/video/tegra/nvmap/nvmap_alloc.c
new file mode 100644
index 000000000000..7cac44f30281
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nvmap_alloc.c
@@ -0,0 +1,1047 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_alloc.c
+ *
+ * Handle allocation and freeing routines for nvmap
+ *
+ * Copyright (c) 2011-2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"%s: " fmt, __func__
+
+#include <linux/moduleparam.h>
+#include <linux/random.h>
+#include <soc/tegra/fuse.h>
+#include <trace/events/nvmap.h>
+
+#include "nvmap_priv.h"
+
+bool nvmap_convert_carveout_to_iovmm;
+bool nvmap_convert_iovmm_to_carveout;
+
+u32 nvmap_max_handle_count;
+u64 nvmap_big_page_allocs;
+u64 nvmap_total_page_allocs;
+
+/* handles may be arbitrarily large (16+MiB), and any handle allocated from
+ * the kernel (i.e., not a carveout handle) includes its array of pages. to
+ * preserve kmalloc space, if the array of pages exceeds PAGELIST_VMALLOC_MIN,
+ * the array is allocated using vmalloc. */
+#define PAGELIST_VMALLOC_MIN	(PAGE_SIZE)
+
+void *nvmap_altalloc(size_t len)
+{
+	if (len > PAGELIST_VMALLOC_MIN)
+		return vmalloc(len);
+	else
+		return kmalloc(len, GFP_KERNEL);
+}
+
+void nvmap_altfree(void *ptr, size_t len)
+{
+	if (!ptr)
+		return;
+
+	if (len > PAGELIST_VMALLOC_MIN)
+		vfree(ptr);
+	else
+		kfree(ptr);
+}
+
+static struct page *nvmap_alloc_pages_exact(gfp_t gfp, size_t size)
+{
+	struct page *page, *p, *e;
+	unsigned int order;
+
+	order = get_order(size);
+	page = alloc_pages(gfp, order);
+
+	if (!page)
+		return NULL;
+
+	split_page(page, order);
+	e = nth_page(page, (1 << order));
+	for (p = nth_page(page, (size >> PAGE_SHIFT)); p < e; p++)
+		__free_page(p);
+
+	return page;
+}
+
+static uint s_nr_colors = 1;
+module_param_named(nr_colors, s_nr_colors, uint, 0644);
+
+#define NVMAP_MAX_COLORS 16
+
+struct color_list {
+	u32 *counts;
+	u32 *heads;
+	u32 *list;
+	struct page **pages;
+	u32 page_count;
+	u32 length;
+};
+
+static struct color_list *alloc_color_list(u32 nr_pages, u32 nr_colors)
+{
+	struct color_list *list;
+	u32 *temp = NULL;
+	u32 nr_u32;
+
+	list = kzalloc(sizeof(struct color_list), GFP_KERNEL);
+	if (!list)
+		return NULL;
+
+	list->pages = vmalloc(nr_pages * sizeof(struct page *));
+	if (!list->pages) {
+		kfree(list);
+		return NULL;
+	}
+
+	/* Allocate counts, heads, and list with a single allocation */
+	nr_u32 = nr_pages + 2 * nr_colors;
+	temp = vmalloc(nr_u32 * sizeof(u32));
+	if (!temp)
+		goto fail;
+
+	memset(&temp[0], 0, 2 * nr_colors *  sizeof(u32));
+	list->counts = &temp[0];
+	list->heads = &temp[nr_colors];
+	list->list = &temp[2 * nr_colors];
+
+	list->page_count = nr_pages;
+
+	return list;
+fail:
+	if (list->pages)
+		vfree(list->pages);
+	kfree(list);
+	return NULL;
+}
+
+static void free_color_list(struct color_list *list)
+{
+	vfree(list->pages);
+	vfree(list->counts);	/* Frees counts, heads, and list */
+	kfree(list);
+}
+
+static struct page *list_pop_page(struct color_list *list, u32 color, char *who)
+{
+	u32 i;
+
+	/* Debug check */
+	if ((list->counts[color] == 0) || (list->counts[color] > 1 << 31)) {
+		pr_err("list_pop_page: OVER FREE!\n");
+		pr_err(" called from: %s\n", who);
+		for (i = 0; i < s_nr_colors; i++)
+			pr_err(" color = %d: %d\n", i, list->counts[i]);
+		BUG();
+	}
+	i = list->heads[color];
+	list->heads[color] = list->list[i];
+	list->counts[color]--;
+	return list->pages[i];
+}
+
+struct nvmap_alloc_state {
+	u32 nr_colors;
+	u32 (*addr_to_color)(uintptr_t phys);
+	u32 tile;
+	u32 output_count;
+	u32 nr_pages;
+	u32 max_color_per_tile;
+	struct color_list *list;
+};
+
+#define CHANNEL_MASK_0 0x27af5200
+#define CHANNEL_MASK_1 0x563ca400
+#define CHANNEL_MASK_2 0x3f264800
+#define CHANNEL_MASK_3 0xe2443000
+#define BANK_MASK_0 0x5ca78400
+#define BANK_MASK_1 0xe5724800
+#define BANK_MASK_2 0x973bb000
+
+#define BIT_N(a, n) \
+	(((a) >> (n)) & 1)
+
+#define BITS_XOR_9_TO_31(a) \
+	(BIT_N((a), 9) ^ BIT_N((a), 10) ^ BIT_N((a), 11) ^ BIT_N((a), 12)  ^ \
+	BIT_N((a), 13) ^ BIT_N((a), 14) ^ BIT_N((a), 15) ^ BIT_N((a), 16) ^ \
+	BIT_N((a), 17) ^ BIT_N((a), 18) ^ BIT_N((a), 19) ^ BIT_N((a), 20) ^ \
+	BIT_N((a), 21) ^ BIT_N((a), 22) ^ BIT_N((a), 23) ^ BIT_N((a), 24) ^ \
+	BIT_N((a), 25) ^ BIT_N((a), 26) ^ BIT_N((a), 27) ^ BIT_N((a), 28) ^ \
+	BIT_N((a), 29) ^ BIT_N((a), 30) ^ BIT_N((a), 31))
+
+#define BITS_XOR_10_TO_31(a) \
+	(BIT_N((a), 10) ^ BIT_N((a), 11) ^ BIT_N((a), 12) ^ \
+	BIT_N((a), 13) ^ BIT_N((a), 14) ^ BIT_N((a), 15) ^ BIT_N((a), 16) ^ \
+	BIT_N((a), 17) ^ BIT_N((a), 18) ^ BIT_N((a), 19) ^ BIT_N((a), 20) ^ \
+	BIT_N((a), 21) ^ BIT_N((a), 22) ^ BIT_N((a), 23) ^ BIT_N((a), 24) ^ \
+	BIT_N((a), 25) ^ BIT_N((a), 26) ^ BIT_N((a), 27) ^ BIT_N((a), 28) ^ \
+	BIT_N((a), 29) ^ BIT_N((a), 30) ^ BIT_N((a), 31))
+
+static u32 addr_to_color_t19x(uintptr_t phys)
+{
+	int color, chan, bank;
+	u32 addr = (u32)phys;
+	u32 xaddr = (u32)(phys >> 4);
+
+
+	chan =  (BITS_XOR_9_TO_31(addr & CHANNEL_MASK_0) << 0);
+	chan |= (BITS_XOR_9_TO_31(addr & CHANNEL_MASK_1) << 1);
+	chan |= (BITS_XOR_9_TO_31(addr & CHANNEL_MASK_2) << 2);
+	chan |= (BITS_XOR_9_TO_31(addr & CHANNEL_MASK_3) << 3);
+
+	bank = (BITS_XOR_10_TO_31(xaddr & BANK_MASK_0) << 0);
+	bank |= (BITS_XOR_10_TO_31(xaddr & BANK_MASK_1) << 1);
+	bank |= (BITS_XOR_10_TO_31(xaddr & BANK_MASK_2) << 2);
+
+	WARN_ON(chan > 15);
+	WARN_ON(bank > 7);
+	/* It is preferable to color pages based on even/odd banks
+	 * as well. To limit the number of colors to 16, bank info
+	 * is not used in page coloring.
+	 */
+	color = chan;
+
+	return color;
+}
+
+static struct color_list *init_color_list(struct nvmap_page_pool *pool,
+					  struct nvmap_alloc_state *state,
+					  u32 nr_pages)
+{
+	struct color_list *list;
+	u32 color, i, page_index = 0;
+	gfp_t gfp = GFP_NVMAP | __GFP_ZERO;
+
+	list = alloc_color_list(nr_pages, state->nr_colors);
+	if (!list)
+		return NULL;
+
+#ifdef CONFIG_NVMAP_PAGE_POOLS
+	/* Allocated page from nvmap page pool if possible */
+	page_index = nvmap_page_pool_alloc_lots(pool, list->pages, nr_pages);
+#endif
+	/* Fall back to general page allocator */
+	for (i = page_index; i < nr_pages; i++) {
+		list->pages[i] = nvmap_alloc_pages_exact(gfp, PAGE_SIZE);
+		if (!list->pages[i])
+			goto fail;
+	}
+	/* Clean the cache for any page that didn't come from the page pool */
+	if (page_index < nr_pages)
+		nvmap_clean_cache(&list->pages[page_index],
+				  nr_pages - page_index);
+
+	/* Create linked list of colors and compute the histogram */
+	for (i = 0; i < nr_pages; i++) {
+		color = state->addr_to_color((uintptr_t)
+					     page_to_phys(list->pages[i]));
+		list->list[i] = list->heads[color];
+		list->heads[color] = i;
+		list->counts[color]++;
+	}
+	return list;
+fail:
+	while (i--)
+		__free_page(list->pages[i]);
+	free_color_list(list);
+	return NULL;
+}
+
+static void smooth_pages(struct color_list *list, u32 nr_extra, u32 nr_colors)
+{
+	u32 i, j, color, max;
+	u32 counts[NVMAP_MAX_COLORS] = {0};
+
+	if (nr_extra == 0)
+		return;
+
+	/* Determine which colors need to be freed */
+	for (i = 0; i < nr_extra; i++) {
+		/* Find the max */
+		max = 0;
+		color = 0;
+		for (j = 0; j < nr_colors; j++) {
+			if (list->counts[j] - counts[j] > max) {
+				color = j;
+				max = list->counts[j] - counts[j];
+			}
+		}
+		counts[color]++;
+	}
+
+	/* Iterate through 0...nr_extra-1 in psuedorandom order */
+	do {
+		/* Pop the max off and free it */
+		for (color = 0; color < nr_colors; color++) {
+			while (counts[color]) {
+				__free_page(list_pop_page(list,
+						color, "smooth_pages"));
+				counts[color]--;
+				nr_extra--;
+			}
+		}
+	} while (nr_extra > 0);
+}
+
+static void add_perfect(struct nvmap_alloc_state *state, u32 nr_pages,
+			struct page **out_pages)
+{
+	u32 i;
+	u32 color;
+	struct page *page;
+	uintptr_t virt_addr;
+
+
+	/* create a perfect tile */
+	for (i = 0;
+	     i < state->nr_colors && state->output_count < nr_pages;
+	     i++) {
+		virt_addr = (i + (state->tile * state->nr_colors)) * PAGE_SIZE;
+		color = state->addr_to_color(virt_addr);
+		page = list_pop_page(state->list, color, "perfect");
+		out_pages[state->output_count++] = page;
+	}
+}
+
+static void add_imperfect(struct nvmap_alloc_state *state, u32 nr_pages,
+			  struct page **out_pages)
+{
+	u32 i, j;
+	u32 max_count;
+	u32 color;
+	struct page *page;
+	uintptr_t virt_addr;
+	u32 counts[NVMAP_MAX_COLORS] = {0};
+
+	/* Determine which colors will go into the tile */
+	for (i = 0; i < state->nr_colors; i++) {
+		max_count = 0;
+		color = 0;
+		for (j = 0; j < state->nr_colors; j++) {
+			u32 left = state->list->counts[j] - counts[j];
+
+			if (left > max_count &&
+			    counts[j] < state->max_color_per_tile) {
+				max_count = left;
+				color = j;
+			}
+		}
+		counts[color]++;
+	}
+
+	/* Arrange the colors into the tile */
+	for (i = 0;
+	     i < state->nr_colors && state->output_count < nr_pages;
+	     i++) {
+		virt_addr = (i + (i * state->nr_colors)) * PAGE_SIZE;
+		color = state->addr_to_color(virt_addr);
+		/* Find a substitute color */
+		if (counts[color] == 0) {
+			/* Find the color used the most in the tile */
+			max_count = 0;
+			for (j = 0; j < state->nr_colors; j++) {
+				if (counts[j] > max_count) {
+					max_count = counts[j];
+					color = j;
+				}
+			}
+		}
+		page = list_pop_page(state->list, color, "imperfect");
+		out_pages[state->output_count++] = page;
+		counts[color]--;
+	}
+}
+
+static int alloc_colored(struct nvmap_page_pool *pool, u32 nr_pages,
+			 struct page **out_pages, u32 chipid)
+{
+	struct nvmap_alloc_state state;
+	u32 nr_alloc, max_count, min_count;
+	u32 nr_tiles, nr_perfect, nr_imperfect;
+	int dither_state;
+	u32 i;
+
+	state.nr_colors = s_nr_colors;
+	state.addr_to_color = addr_to_color_t19x;
+
+	/* Allocate pages for full 32-page tiles */
+	nr_tiles = (nr_pages + state.nr_colors - 1) / state.nr_colors;
+	/* Overallocate pages by 1/16th */
+	nr_alloc  = state.nr_colors * nr_tiles;
+	nr_alloc += nr_alloc >> 4;
+
+	/* Create lists of each page color */
+	state.list = init_color_list(pool, &state, nr_alloc);
+	if (!state.list)
+		return -ENOMEM;
+
+	/* Smooth out the histogram by freeing over allocated pages */
+	smooth_pages(state.list, nr_alloc - state.nr_colors * nr_tiles,
+		     state.nr_colors);
+
+	max_count = 0;
+	min_count = state.list->counts[0];
+	for (i = 0; i < state.nr_colors; i++) {
+		if (state.list->counts[i] > max_count)
+			max_count = state.list->counts[i];
+		if (state.list->counts[i] < min_count)
+			min_count = state.list->counts[i];
+	}
+
+	/* Compute the number of perfect / imperfect tiles and the maximum
+	 * number of pages with the same color can be in a tile
+	 */
+	if (max_count / nr_tiles >= 3) {
+		/* It is not possible to create perfect tiles with
+		 * max_color_per_tile <= 3
+		 */
+		nr_perfect = 0;
+		state.max_color_per_tile = (max_count + nr_tiles - 1)
+					   / nr_tiles;
+	} else if (nr_tiles * 2 == max_count) {
+		/* All of the tiles can be perfect */
+		nr_perfect = nr_tiles;
+		state.max_color_per_tile = 2;
+	} else {
+		/* Some of the tiles can be perfect */
+		nr_perfect = nr_tiles - (max_count % nr_tiles);
+		state.max_color_per_tile = 3;
+	}
+	/* Check if the number of perfect tiles is bound by the color with the
+	 * minimum count
+	 */
+	if (nr_perfect * 2 > min_count)
+		nr_perfect = min_count / 2;
+
+	nr_imperfect = nr_tiles - nr_perfect;
+
+	/* Output tiles */
+	dither_state = nr_perfect - nr_imperfect;
+	state.output_count = 0;
+	for (state.tile = 0; state.tile < nr_tiles; state.tile++) {
+		if (dither_state > 0) {
+			add_perfect(&state, nr_pages, out_pages);
+			dither_state -= nr_imperfect;
+		} else {
+			add_imperfect(&state, nr_pages, out_pages);
+			dither_state += nr_perfect;
+		}
+	}
+
+	/* Free extra pages created when the buffer does not
+	 * fill the last tile
+	 */
+	for (i = 0; i < state.nr_colors; i++)
+		while (state.list->counts[i] > 0)
+			__free_page(list_pop_page(state.list, i, "free"));
+
+	free_color_list(state.list);
+
+	return 0;
+}
+
+static int handle_page_alloc(struct nvmap_client *client,
+			     struct nvmap_handle *h, bool contiguous)
+{
+	size_t size = h->size;
+	size_t nr_page = size >> PAGE_SHIFT;
+	int i = 0, page_index = 0;
+	struct page **pages;
+	gfp_t gfp = GFP_NVMAP | __GFP_ZERO;
+	int pages_per_big_pg = NVMAP_PP_BIG_PAGE_SIZE >> PAGE_SHIFT;
+	static u32 chipid;
+
+	if (!chipid) {
+#ifdef CONFIG_NVMAP_COLOR_PAGES
+		chipid = tegra_hidrev_get_chipid(tegra_read_chipid());
+		if (chipid == TEGRA194)
+			s_nr_colors = 16;
+#endif
+	}
+
+	pages = nvmap_altalloc(nr_page * sizeof(*pages));
+	if (!pages)
+		return -ENOMEM;
+
+	if (contiguous) {
+		struct page *page;
+		page = nvmap_alloc_pages_exact(gfp, size);
+		if (!page)
+			goto fail;
+
+		for (i = 0; i < nr_page; i++)
+			pages[i] = nth_page(page, i);
+
+	} else {
+#ifdef CONFIG_NVMAP_PAGE_POOLS
+		/* Get as many big pages from the pool as possible. */
+		page_index = nvmap_page_pool_alloc_lots_bp(&nvmap_dev->pool, pages,
+								 nr_page);
+		pages_per_big_pg = nvmap_dev->pool.pages_per_big_pg;
+#endif
+		/* Try to allocate big pages from page allocator */
+		for (i = page_index;
+		     i < nr_page && pages_per_big_pg > 1 && (nr_page - i) >= pages_per_big_pg;
+		     i += pages_per_big_pg, page_index += pages_per_big_pg) {
+			struct page *page;
+			int idx;
+			/*
+			 * set the gfp not to trigger direct/kswapd reclaims and
+			 * not to use emergency reserves.
+			 */
+			gfp_t gfp_no_reclaim = (gfp | __GFP_NOMEMALLOC) & ~__GFP_RECLAIM;
+
+			page = nvmap_alloc_pages_exact(gfp_no_reclaim,
+					pages_per_big_pg << PAGE_SHIFT);
+			if (!page)
+				break;
+
+			for (idx = 0; idx < pages_per_big_pg; idx++)
+				pages[i + idx] = nth_page(page, idx);
+			nvmap_clean_cache(&pages[i], pages_per_big_pg);
+		}
+		nvmap_big_page_allocs += page_index;
+
+		if (s_nr_colors <= 1) {
+#ifdef CONFIG_NVMAP_PAGE_POOLS
+			/* Get as many 4K pages from the pool as possible. */
+			page_index += nvmap_page_pool_alloc_lots(
+				      &nvmap_dev->pool, &pages[page_index],
+				      nr_page - page_index);
+#endif
+
+			for (i = page_index; i < nr_page; i++) {
+				pages[i] = nvmap_alloc_pages_exact(gfp,
+								   PAGE_SIZE);
+				if (!pages[i])
+					goto fail;
+			}
+		} else if (page_index < nr_page) {
+			if (alloc_colored(&nvmap_dev->pool,
+			     nr_page - page_index, &pages[page_index], chipid))
+				goto fail;
+			page_index = nr_page;
+		}
+		nvmap_total_page_allocs += nr_page;
+	}
+
+	/*
+	 * Make sure any data in the caches is cleaned out before
+	 * passing these pages to userspace. Many nvmap clients assume that
+	 * the buffers are clean as soon as they are allocated. nvmap
+	 * clients can pass the buffer to hardware as it is without any
+	 * explicit cache maintenance.
+	 */
+	if (page_index < nr_page)
+		nvmap_clean_cache(&pages[page_index], nr_page - page_index);
+
+	h->pgalloc.pages = pages;
+	h->pgalloc.contig = contiguous;
+	atomic_set(&h->pgalloc.ndirty, 0);
+	return 0;
+
+fail:
+	while (i--)
+		__free_page(pages[i]);
+	nvmap_altfree(pages, nr_page * sizeof(*pages));
+	wmb();
+	return -ENOMEM;
+}
+
+static struct device *nvmap_heap_pgalloc_dev(unsigned long type)
+{
+	int ret = -EINVAL;
+	struct device *dma_dev;
+
+	ret = 0;
+
+	if (ret || (type != NVMAP_HEAP_CARVEOUT_VPR))
+		return ERR_PTR(-EINVAL);
+
+	dma_dev = dma_dev_from_handle(type);
+	if (IS_ERR(dma_dev))
+		return dma_dev;
+
+	ret = dma_set_resizable_heap_floor_size(dma_dev, 0);
+	if (ret)
+		return ERR_PTR(ret);
+	return dma_dev;
+}
+
+static int nvmap_heap_pgalloc(struct nvmap_client *client,
+			struct nvmap_handle *h, unsigned long type)
+{
+	size_t size = h->size;
+	struct page **pages;
+	struct device *dma_dev;
+	DEFINE_DMA_ATTRS(attrs);
+	dma_addr_t pa;
+
+	dma_dev = nvmap_heap_pgalloc_dev(type);
+	if (IS_ERR(dma_dev))
+		return PTR_ERR(dma_dev);
+
+	dma_set_attr(DMA_ATTR_ALLOC_EXACT_SIZE, __DMA_ATTR(attrs));
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0)
+	dma_set_attr(DMA_ATTR_ALLOC_SINGLE_PAGES, __DMA_ATTR(attrs));
+#endif
+
+	pages = dma_alloc_attrs(dma_dev, size, &pa,
+			GFP_KERNEL, __DMA_ATTR(attrs));
+	if (dma_mapping_error(dma_dev, pa))
+		return -ENOMEM;
+
+	h->pgalloc.pages = pages;
+	h->pgalloc.contig = 0;
+	atomic_set(&h->pgalloc.ndirty, 0);
+	return 0;
+}
+
+static int nvmap_heap_pgfree(struct nvmap_handle *h)
+{
+	size_t size = h->size;
+	struct device *dma_dev;
+	DEFINE_DMA_ATTRS(attrs);
+	dma_addr_t pa = ~(dma_addr_t)0;
+
+	dma_dev = nvmap_heap_pgalloc_dev(h->heap_type);
+	if (IS_ERR(dma_dev))
+		return PTR_ERR(dma_dev);
+
+	dma_set_attr(DMA_ATTR_ALLOC_EXACT_SIZE, __DMA_ATTR(attrs));
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0)
+	dma_set_attr(DMA_ATTR_ALLOC_SINGLE_PAGES, __DMA_ATTR(attrs));
+#endif
+
+	dma_free_attrs(dma_dev, size, h->pgalloc.pages, pa,
+		       __DMA_ATTR(attrs));
+
+	h->pgalloc.pages = NULL;
+	return 0;
+}
+static void alloc_handle(struct nvmap_client *client,
+			 struct nvmap_handle *h, unsigned int type)
+{
+	unsigned int carveout_mask = NVMAP_HEAP_CARVEOUT_MASK;
+	unsigned int iovmm_mask = NVMAP_HEAP_IOVMM;
+	int ret;
+
+	BUG_ON(type & (type - 1));
+
+	if (nvmap_convert_carveout_to_iovmm) {
+		carveout_mask &= ~NVMAP_HEAP_CARVEOUT_GENERIC;
+		iovmm_mask |= NVMAP_HEAP_CARVEOUT_GENERIC;
+	} else if (nvmap_convert_iovmm_to_carveout) {
+		if (type & NVMAP_HEAP_IOVMM) {
+			type &= ~NVMAP_HEAP_IOVMM;
+			type |= NVMAP_HEAP_CARVEOUT_GENERIC;
+		}
+	}
+
+	if (type & carveout_mask) {
+		struct nvmap_heap_block *b;
+
+		b = nvmap_carveout_alloc(client, h, type, NULL);
+		if (b) {
+			h->heap_type = type;
+			h->heap_pgalloc = false;
+			/* barrier to ensure all handle alloc data
+			 * is visible before alloc is seen by other
+			 * processors.
+			 */
+			mb();
+			h->alloc = true;
+			return;
+		}
+
+		/* The following is a fallback for (non-IVM) VPR in case
+		 * allocation of a contiguous block failed.
+		 */
+		ret = nvmap_heap_pgalloc(client, h, type);
+		if (ret)
+			return;
+		h->heap_type = NVMAP_HEAP_CARVEOUT_VPR;
+		h->heap_pgalloc = true;
+		mb();
+		h->alloc = true;
+	} else if (type & iovmm_mask) {
+		ret = handle_page_alloc(client, h,
+			h->userflags & NVMAP_HANDLE_PHYS_CONTIG);
+		if (ret)
+			return;
+		h->heap_type = NVMAP_HEAP_IOVMM;
+		h->heap_pgalloc = true;
+		mb();
+		h->alloc = true;
+	}
+}
+
+static int alloc_handle_from_va(struct nvmap_client *client,
+				 struct nvmap_handle *h,
+				 ulong vaddr,
+				 u32 flags)
+{
+	size_t nr_page = h->size >> PAGE_SHIFT;
+	struct page **pages;
+	int ret = 0;
+
+	pages = nvmap_altalloc(nr_page * sizeof(*pages));
+	if (IS_ERR_OR_NULL(pages))
+		return PTR_ERR(pages);
+
+	ret = nvmap_get_user_pages(vaddr & PAGE_MASK, nr_page, pages, true,
+				(flags & NVMAP_HANDLE_RO) ? 0 : FOLL_WRITE);
+	if (ret) {
+		nvmap_altfree(pages, nr_page * sizeof(*pages));
+		return ret;
+	}
+
+	if (flags & NVMAP_HANDLE_RO)
+		h->is_ro = true;
+
+	nvmap_clean_cache(&pages[0], nr_page);
+	h->pgalloc.pages = pages;
+	atomic_set(&h->pgalloc.ndirty, 0);
+	h->heap_type = NVMAP_HEAP_IOVMM;
+	h->heap_pgalloc = true;
+	h->from_va = true;
+	mb();
+	h->alloc = true;
+	return ret;
+}
+
+/* small allocations will try to allocate from generic OS memory before
+ * any of the limited heaps, to increase the effective memory for graphics
+ * allocations, and to reduce fragmentation of the graphics heaps with
+ * sub-page splinters */
+static const unsigned int heap_policy_small[] = {
+	NVMAP_HEAP_CARVEOUT_VPR,
+	NVMAP_HEAP_CARVEOUT_IRAM,
+	NVMAP_HEAP_CARVEOUT_MASK,
+	NVMAP_HEAP_IOVMM,
+	0,
+};
+
+static const unsigned int heap_policy_large[] = {
+	NVMAP_HEAP_CARVEOUT_VPR,
+	NVMAP_HEAP_CARVEOUT_IRAM,
+	NVMAP_HEAP_IOVMM,
+	NVMAP_HEAP_CARVEOUT_MASK,
+	0,
+};
+
+static const unsigned int heap_policy_excl[] = {
+	NVMAP_HEAP_CARVEOUT_IVM,
+	NVMAP_HEAP_CARVEOUT_IVM_VPR,
+	NVMAP_HEAP_CARVEOUT_VIDMEM,
+	0,
+};
+
+int nvmap_alloc_handle(struct nvmap_client *client,
+		       struct nvmap_handle *h, unsigned int heap_mask,
+		       size_t align,
+		       u8 kind,
+		       unsigned int flags,
+		       int peer)
+{
+	const unsigned int *alloc_policy;
+	size_t nr_page;
+	int err = -ENOMEM;
+	int tag, i;
+	bool alloc_from_excl = false;
+
+	h = nvmap_handle_get(h);
+
+	if (!h)
+		return -EINVAL;
+
+	if (h->alloc) {
+		nvmap_handle_put(h);
+		return -EEXIST;
+	}
+
+	nvmap_stats_inc(NS_TOTAL, h->size);
+	nvmap_stats_inc(NS_ALLOC, h->size);
+	trace_nvmap_alloc_handle(client, h,
+		h->size, heap_mask, align, flags,
+		nvmap_stats_read(NS_TOTAL),
+		nvmap_stats_read(NS_ALLOC));
+	h->userflags = flags;
+	nr_page = ((h->size + PAGE_SIZE - 1) >> PAGE_SHIFT);
+	/* Force mapping to uncached for VPR memory. */
+	if (heap_mask & (NVMAP_HEAP_CARVEOUT_VPR | NVMAP_HEAP_CARVEOUT_IVM_VPR |
+		~nvmap_dev->cpu_access_mask))
+		h->flags = NVMAP_HANDLE_UNCACHEABLE;
+	else
+		h->flags = flags & NVMAP_HANDLE_CACHE_FLAG;
+	h->align = max_t(size_t, align, L1_CACHE_BYTES);
+	h->peer = peer;
+	tag = flags >> 16;
+
+	if (!tag && client && !client->tag_warned) {
+		char task_comm[TASK_COMM_LEN];
+		client->tag_warned = 1;
+		get_task_comm(task_comm, client->task);
+		pr_err("PID %d: %s: WARNING: "
+			"All NvMap Allocations must have a tag "
+			"to identify the subsystem allocating memory."
+			"Please pass the tag to the API call"
+			" NvRmMemHanldeAllocAttr() or relevant. \n",
+			client->task->pid, task_comm);
+	}
+
+	/*
+	 * If user specifies one of the exclusive carveouts, allocation
+	 * from no other heap should be allowed.
+	 */
+	for (i = 0; i < ARRAY_SIZE(heap_policy_excl); i++) {
+		if (!(heap_mask & heap_policy_excl[i]))
+			continue;
+
+		if (heap_mask & ~(heap_policy_excl[i])) {
+			pr_err("%s alloc mixes exclusive heap %d and other heaps\n",
+			       current->group_leader->comm, heap_policy_excl[i]);
+			err = -EINVAL;
+			goto out;
+		}
+		alloc_from_excl = true;
+	}
+
+	if (!heap_mask) {
+		err = -EINVAL;
+		goto out;
+	}
+
+	alloc_policy = alloc_from_excl ? heap_policy_excl :
+			(nr_page == 1) ? heap_policy_small : heap_policy_large;
+
+	while (!h->alloc && *alloc_policy) {
+		unsigned int heap_type;
+
+		heap_type = *alloc_policy++;
+		heap_type &= heap_mask;
+
+		if (!heap_type)
+			continue;
+
+		heap_mask &= ~heap_type;
+
+		while (heap_type && !h->alloc) {
+			unsigned int heap;
+
+			/* iterate possible heaps MSB-to-LSB, since higher-
+			 * priority carveouts will have higher usage masks */
+			heap = 1 << __fls(heap_type);
+			alloc_handle(client, h, heap);
+			heap_type &= ~heap;
+		}
+	}
+
+out:
+	if (h->alloc) {
+		if (client->kernel_client)
+			nvmap_stats_inc(NS_KALLOC, h->size);
+		else
+			nvmap_stats_inc(NS_UALLOC, h->size);
+		NVMAP_TAG_TRACE(trace_nvmap_alloc_handle_done,
+			NVMAP_TP_ARGS_CHR(client, h, NULL));
+		err = 0;
+	} else {
+		nvmap_stats_dec(NS_TOTAL, h->size);
+		nvmap_stats_dec(NS_ALLOC, h->size);
+	}
+	nvmap_handle_put(h);
+	return err;
+}
+
+int nvmap_alloc_handle_from_va(struct nvmap_client *client,
+			       struct nvmap_handle *h,
+			       ulong addr,
+			       unsigned int flags)
+{
+	int err = -ENOMEM;
+	int tag;
+
+	h = nvmap_handle_get(h);
+	if (!h)
+		return -EINVAL;
+
+	if (h->alloc) {
+		nvmap_handle_put(h);
+		return -EEXIST;
+	}
+
+	h->userflags = flags;
+	h->flags = (flags & NVMAP_HANDLE_CACHE_FLAG);
+	h->align = PAGE_SIZE;
+	tag = flags >> 16;
+
+	if (!tag && client && !client->tag_warned) {
+		char task_comm[TASK_COMM_LEN];
+		client->tag_warned = 1;
+		get_task_comm(task_comm, client->task);
+		pr_err("PID %d: %s: WARNING: "
+			"All NvMap Allocations must have a tag "
+			"to identify the subsystem allocating memory."
+			"Please pass the tag to the API call"
+			" NvRmMemHanldeAllocAttr() or relevant. \n",
+			client->task->pid, task_comm);
+	}
+
+	err = alloc_handle_from_va(client, h, addr, flags);
+	if (err) {
+		pr_err("alloc_handle_from_va failed %d", err);
+		nvmap_handle_put(h);
+		return -EINVAL;
+	}
+
+	if (h->alloc) {
+		NVMAP_TAG_TRACE(trace_nvmap_alloc_handle_done,
+			NVMAP_TP_ARGS_CHR(client, h, NULL));
+		err = 0;
+	}
+	nvmap_handle_put(h);
+	return err;
+}
+
+void _nvmap_handle_free(struct nvmap_handle *h)
+{
+	unsigned int i, nr_page, page_index = 0;
+	struct nvmap_handle_dmabuf_priv *curr, *next;
+
+	list_for_each_entry_safe(curr, next, &h->dmabuf_priv, list) {
+		curr->priv_release(curr->priv);
+		list_del(&curr->list);
+		kzfree(curr);
+	}
+
+	if (nvmap_handle_remove(nvmap_dev, h) != 0)
+		return;
+
+	if (!h->alloc)
+		goto out;
+
+	nvmap_stats_inc(NS_RELEASE, h->size);
+	nvmap_stats_dec(NS_TOTAL, h->size);
+	if (!h->heap_pgalloc) {
+		if (h->vaddr) {
+			struct vm_struct *vm;
+			void *addr = h->vaddr;
+
+			addr -= (h->carveout->base & ~PAGE_MASK);
+			vm = find_vm_area(addr);
+			BUG_ON(!vm);
+			free_vm_area(vm);
+		}
+
+		nvmap_heap_free(h->carveout);
+		nvmap_kmaps_dec(h);
+		h->vaddr = NULL;
+		goto out;
+	} else {
+		int ret = nvmap_heap_pgfree(h);
+		if (!ret)
+			goto out;
+	}
+
+	nr_page = DIV_ROUND_UP(h->size, PAGE_SIZE);
+
+	BUG_ON(h->size & ~PAGE_MASK);
+	BUG_ON(!h->pgalloc.pages);
+
+	if (h->vaddr) {
+		nvmap_kmaps_dec(h);
+
+		vm_unmap_ram(h->vaddr, h->size >> PAGE_SHIFT);
+		h->vaddr = NULL;
+	}
+
+	for (i = 0; i < nr_page; i++)
+		h->pgalloc.pages[i] = nvmap_to_page(h->pgalloc.pages[i]);
+
+#ifdef CONFIG_NVMAP_PAGE_POOLS
+	if (!h->from_va)
+		page_index = nvmap_page_pool_fill_lots(&nvmap_dev->pool,
+					h->pgalloc.pages, nr_page);
+#endif
+
+	for (i = page_index; i < nr_page; i++) {
+		if (h->from_va)
+			put_page(h->pgalloc.pages[i]);
+		else
+			__free_page(h->pgalloc.pages[i]);
+	}
+
+	nvmap_altfree(h->pgalloc.pages, nr_page * sizeof(struct page *));
+
+out:
+	NVMAP_TAG_TRACE(trace_nvmap_destroy_handle,
+		NULL, get_current()->pid, 0, NVMAP_TP_ARGS_H(h));
+	kfree(h);
+}
+
+void nvmap_free_handle(struct nvmap_client *client,
+		       struct nvmap_handle *handle)
+{
+	struct nvmap_handle_ref *ref;
+	struct nvmap_handle *h;
+
+	nvmap_ref_lock(client);
+
+	ref = __nvmap_validate_locked(client, handle);
+	if (!ref) {
+		nvmap_ref_unlock(client);
+		return;
+	}
+
+	BUG_ON(!ref->handle);
+	h = ref->handle;
+
+	if (atomic_dec_return(&ref->dupes)) {
+		NVMAP_TAG_TRACE(trace_nvmap_free_handle,
+			NVMAP_TP_ARGS_CHR(client, h, ref));
+		nvmap_ref_unlock(client);
+		goto out;
+	}
+
+	smp_rmb();
+	rb_erase(&ref->node, &client->handle_refs);
+	client->handle_count--;
+	atomic_dec(&ref->handle->share_count);
+
+	nvmap_ref_unlock(client);
+
+	if (h->owner == client)
+		h->owner = NULL;
+
+	dma_buf_put(ref->handle->dmabuf);
+	NVMAP_TAG_TRACE(trace_nvmap_free_handle,
+		NVMAP_TP_ARGS_CHR(client, h, ref));
+	kfree(ref);
+
+out:
+	BUG_ON(!atomic_read(&h->ref));
+	nvmap_handle_put(h);
+}
+EXPORT_SYMBOL(nvmap_free_handle);
+
+void nvmap_free_handle_fd(struct nvmap_client *client,
+			       int fd)
+{
+	struct nvmap_handle *handle = nvmap_handle_get_from_fd(fd);
+	if (handle) {
+		nvmap_free_handle(client, handle);
+		nvmap_handle_put(handle);
+	}
+}
diff --git a/drivers/video/tegra/nvmap/nvmap_cache.c b/drivers/video/tegra/nvmap/nvmap_cache.c
new file mode 100644
index 000000000000..a26615a949b6
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nvmap_cache.c
@@ -0,0 +1,667 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_cache.c
+ *
+ * Copyright (c) 2011-2021, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"nvmap: %s() " fmt, __func__
+
+#include <linux/highmem.h>
+#include <linux/io.h>
+#include <linux/debugfs.h>
+#include <linux/of.h>
+#include <soc/tegra/fuse.h>
+
+#include <trace/events/nvmap.h>
+
+#include "nvmap_priv.h"
+
+#ifndef CONFIG_NVMAP_CACHE_MAINT_BY_SET_WAYS
+/* This is basically the L2 cache size but may be tuned as per requirement */
+size_t cache_maint_inner_threshold = SIZE_MAX;
+int nvmap_cache_maint_by_set_ways;
+#else
+int nvmap_cache_maint_by_set_ways = 1;
+size_t cache_maint_inner_threshold = 8 * SZ_2M;
+#endif
+
+static struct static_key nvmap_disable_vaddr_for_cache_maint;
+void (*nvmap_get_cacheability)(struct nvmap_handle *h,
+		bool *inner, bool *outer);
+
+inline static void nvmap_flush_dcache_all(void *dummy)
+{
+#if defined(CONFIG_DENVER_CPU)
+	u64 id_afr0;
+	u64 midr;
+
+	asm volatile ("mrs %0, MIDR_EL1" : "=r"(midr));
+	/* check if current core is a Denver processor */
+	if ((midr & 0xFF8FFFF0) == 0x4e0f0000) {
+		asm volatile ("mrs %0, ID_AFR0_EL1" : "=r"(id_afr0));
+		/* check if complete cache flush through msr is supported */
+		if (likely((id_afr0 & 0xf00) == 0x100)) {
+			asm volatile ("msr s3_0_c15_c13_0, %0" : : "r" (0));
+			asm volatile ("dsb sy");
+			return;
+		}
+	}
+#endif
+	tegra_flush_dcache_all(NULL);
+}
+
+static void nvmap_inner_flush_cache_all(void)
+{
+	nvmap_flush_dcache_all(NULL);
+}
+void (*inner_flush_cache_all)(void) = nvmap_inner_flush_cache_all;
+
+extern void __clean_dcache_louis(void *);
+static void nvmap_inner_clean_cache_all(void)
+{
+#ifdef CONFIG_ARCH_TEGRA_210_SOC
+	on_each_cpu(__clean_dcache_louis, NULL, 1);
+#endif
+	tegra_clean_dcache_all(NULL);
+}
+void (*inner_clean_cache_all)(void) = nvmap_inner_clean_cache_all;
+
+static void nvmap_handle_get_cacheability(struct nvmap_handle *h,
+		bool *inner, bool *outer)
+{
+	*inner = h->flags == NVMAP_HANDLE_CACHEABLE ||
+		 h->flags == NVMAP_HANDLE_INNER_CACHEABLE;
+	*outer = h->flags == NVMAP_HANDLE_CACHEABLE;
+}
+
+static void nvmap_cache_of_setup(struct nvmap_chip_cache_op *op)
+{
+	op->inner_clean_cache_all = nvmap_inner_clean_cache_all;
+	op->inner_flush_cache_all = nvmap_inner_flush_cache_all;
+	op->nvmap_get_cacheability = nvmap_handle_get_cacheability;
+	op->name = kstrdup("set/ways", GFP_KERNEL);
+	BUG_ON(!op->name);
+}
+NVMAP_CACHE_OF_DECLARE("nvidia,carveouts", nvmap_cache_of_setup);
+
+void nvmap_select_cache_ops(struct device *dev)
+{
+	struct nvmap_chip_cache_op op;
+	bool match_found = false;
+	const struct of_device_id *matches = &__nvmapcache_of_table;
+
+	memset(&op, 0, sizeof(op));
+
+	for (; matches; matches++) {
+		if (of_device_is_compatible(dev->of_node,
+					    matches->compatible)) {
+			const nvmap_setup_chip_cache_fn init_fn = matches->data;
+			init_fn(&op);
+			match_found = true;
+			break;
+		}
+	}
+
+	if (WARN_ON(match_found == false)) {
+		pr_err("%s: no cache ops found\n",__func__);
+		return;
+	}
+	inner_flush_cache_all = op.inner_flush_cache_all;
+	inner_clean_cache_all = op.inner_clean_cache_all;
+	nvmap_get_cacheability = op.nvmap_get_cacheability;
+	pr_info("nvmap cache ops set to %s\n", op.name);
+	kfree(op.name);
+
+	if (inner_clean_cache_all && (op.flags & CALL_CLEAN_CACHE_ON_INIT)) {
+		pr_info("calling cache operation %pF\n",
+					inner_clean_cache_all);
+		inner_clean_cache_all();
+	}
+
+	if (inner_flush_cache_all && (op.flags & CALL_FLUSH_CACHE_ON_INIT)) {
+		pr_info("calling cache operation %pF\n",
+					inner_flush_cache_all);
+		inner_flush_cache_all();
+	}
+}
+
+/*
+ * FIXME:
+ *
+ *   __clean_dcache_page() is only available on ARM64 (well, we haven't
+ *   implemented it on ARMv7).
+ */
+void nvmap_clean_cache_page(struct page *page)
+{
+	__clean_dcache_page(page);
+}
+
+void nvmap_clean_cache(struct page **pages, int numpages)
+{
+	int i;
+
+	/* Not technically a flush but that's what nvmap knows about. */
+	nvmap_stats_inc(NS_CFLUSH_DONE, numpages << PAGE_SHIFT);
+	trace_nvmap_cache_flush(numpages << PAGE_SHIFT,
+		nvmap_stats_read(NS_ALLOC),
+		nvmap_stats_read(NS_CFLUSH_RQ),
+		nvmap_stats_read(NS_CFLUSH_DONE));
+
+	for (i = 0; i < numpages; i++)
+		nvmap_clean_cache_page(pages[i]);
+}
+
+__weak void nvmap_override_cache_ops(void)
+{
+	nvmap_select_cache_ops(nvmap_dev->dev_user.parent);
+}
+
+void inner_cache_maint(unsigned int op, void *vaddr, size_t size)
+{
+	if (op == NVMAP_CACHE_OP_WB_INV)
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 9, 0)
+		__dma_flush_range(vaddr, vaddr + size);
+#else
+		dcache_clean_inval_poc((unsigned long)vaddr, size);
+#endif
+	else if (op == NVMAP_CACHE_OP_INV)
+		__dma_map_area(vaddr, size, DMA_FROM_DEVICE);
+	else
+		__dma_map_area(vaddr, size, DMA_TO_DEVICE);
+}
+
+static void heap_page_cache_maint(
+	struct nvmap_handle *h, unsigned long start, unsigned long end,
+	unsigned int op, bool inner, bool outer, bool clean_only_dirty)
+{
+	if (h->userflags & NVMAP_HANDLE_CACHE_SYNC) {
+		/*
+		 * zap user VA->PA mappings so that any access to the pages
+		 * will result in a fault and can be marked dirty
+		 */
+		nvmap_handle_mkclean(h, start, end-start);
+		nvmap_zap_handle(h, start, end - start);
+	}
+
+	if (static_key_false(&nvmap_disable_vaddr_for_cache_maint))
+		goto per_page_cache_maint;
+
+	if (inner) {
+		if (!h->vaddr) {
+			if (__nvmap_mmap(h))
+				__nvmap_munmap(h, h->vaddr);
+			else
+				goto per_page_cache_maint;
+		}
+		/* Fast inner cache maintenance using single mapping */
+		inner_cache_maint(op, h->vaddr + start, end - start);
+		if (!outer)
+			return;
+		/* Skip per-page inner maintenance in loop below */
+		inner = false;
+
+	}
+per_page_cache_maint:
+
+	while (start < end) {
+		struct page *page;
+		phys_addr_t paddr;
+		unsigned long next;
+		unsigned long off;
+		size_t size;
+		int ret;
+
+		page = nvmap_to_page(h->pgalloc.pages[start >> PAGE_SHIFT]);
+		next = min(((start + PAGE_SIZE) & PAGE_MASK), end);
+		off = start & ~PAGE_MASK;
+		size = next - start;
+		paddr = page_to_phys(page) + off;
+
+		ret = nvmap_cache_maint_phys_range(op, paddr, paddr + size,
+				inner, outer);
+		WARN_ON(ret != 0);
+		start = next;
+	}
+}
+
+static inline bool can_fast_cache_maint(unsigned long start,
+			unsigned long end, unsigned int op)
+{
+	if (!nvmap_cache_maint_by_set_ways)
+		return false;
+
+	if ((op == NVMAP_CACHE_OP_INV) ||
+		((end - start) < cache_maint_inner_threshold))
+		return false;
+	return true;
+}
+
+static bool fast_cache_maint(struct nvmap_handle *h,
+	unsigned long start,
+	unsigned long end, unsigned int op,
+	bool clean_only_dirty)
+{
+	if (!can_fast_cache_maint(start, end, op))
+		return false;
+
+	if (h->userflags & NVMAP_HANDLE_CACHE_SYNC) {
+		nvmap_handle_mkclean(h, 0, h->size);
+		nvmap_zap_handle(h, 0, h->size);
+	}
+
+	if (op == NVMAP_CACHE_OP_WB_INV)
+		inner_flush_cache_all();
+	else if (op == NVMAP_CACHE_OP_WB)
+		inner_clean_cache_all();
+
+	return true;
+}
+
+struct cache_maint_op {
+	phys_addr_t start;
+	phys_addr_t end;
+	unsigned int op;
+	struct nvmap_handle *h;
+	bool inner;
+	bool outer;
+	bool clean_only_dirty;
+};
+
+int nvmap_cache_maint_phys_range(unsigned int op, phys_addr_t pstart,
+		phys_addr_t pend, int inner, int outer)
+{
+	unsigned long kaddr;
+	struct vm_struct *area = NULL;
+	phys_addr_t loop;
+
+	if (!inner)
+		goto do_outer;
+
+	if (can_fast_cache_maint((unsigned long)pstart,
+				 (unsigned long)pend, op)) {
+		if (op == NVMAP_CACHE_OP_WB_INV)
+			inner_flush_cache_all();
+		else if (op == NVMAP_CACHE_OP_WB)
+			inner_clean_cache_all();
+		goto do_outer;
+	}
+
+	area = get_vm_area(PAGE_SIZE, 0);
+	if (!area)
+		return -ENOMEM;
+	kaddr = (ulong)area->addr;
+
+	loop = pstart;
+	while (loop < pend) {
+		phys_addr_t next = (loop + PAGE_SIZE) & PAGE_MASK;
+		void *base = (void *)kaddr + (loop & ~PAGE_MASK);
+
+		next = min(next, pend);
+		ioremap_page_range(kaddr, kaddr + PAGE_SIZE,
+			loop, PG_PROT_KERNEL);
+		inner_cache_maint(op, base, next - loop);
+		loop = next;
+		unmap_kernel_range(kaddr, PAGE_SIZE);
+	}
+
+	free_vm_area(area);
+do_outer:
+	return 0;
+}
+
+static int do_cache_maint(struct cache_maint_op *cache_work)
+{
+	phys_addr_t pstart = cache_work->start;
+	phys_addr_t pend = cache_work->end;
+	int err = 0;
+	struct nvmap_handle *h = cache_work->h;
+	unsigned int op = cache_work->op;
+
+	if (!h || !h->alloc)
+		return -EFAULT;
+
+	wmb();
+	if (h->flags == NVMAP_HANDLE_UNCACHEABLE ||
+	    h->flags == NVMAP_HANDLE_WRITE_COMBINE || pstart == pend)
+		goto out;
+
+	trace_nvmap_cache_maint(h->owner, h, pstart, pend, op, pend - pstart);
+	if (pstart > h->size || pend > h->size) {
+		pr_warn("cache maintenance outside handle\n");
+		err = -EINVAL;
+		goto out;
+	}
+
+	if (fast_cache_maint(h, pstart, pend, op, cache_work->clean_only_dirty))
+		goto out;
+
+	if (h->heap_pgalloc) {
+		heap_page_cache_maint(h, pstart, pend, op, true,
+			(h->flags == NVMAP_HANDLE_INNER_CACHEABLE) ?
+			false : true, cache_work->clean_only_dirty);
+		goto out;
+	}
+
+	pstart += h->carveout->base;
+	pend += h->carveout->base;
+
+	err = nvmap_cache_maint_phys_range(op, pstart, pend, true,
+			h->flags != NVMAP_HANDLE_INNER_CACHEABLE);
+
+out:
+	if (!err) {
+		if (can_fast_cache_maint(pstart, pend, op))
+			nvmap_stats_inc(NS_CFLUSH_DONE,
+					cache_maint_inner_threshold);
+		else
+			nvmap_stats_inc(NS_CFLUSH_DONE, pend - pstart);
+	}
+
+	trace_nvmap_cache_flush(pend - pstart,
+		nvmap_stats_read(NS_ALLOC),
+		nvmap_stats_read(NS_CFLUSH_RQ),
+		nvmap_stats_read(NS_CFLUSH_DONE));
+
+	return 0;
+}
+
+int __nvmap_do_cache_maint(struct nvmap_client *client,
+			struct nvmap_handle *h,
+			unsigned long start, unsigned long end,
+			unsigned int op, bool clean_only_dirty)
+{
+	int err;
+	struct cache_maint_op cache_op;
+
+	h = nvmap_handle_get(h);
+	if (!h)
+		return -EFAULT;
+
+	if ((start >= h->size) || (end > h->size)) {
+		pr_debug("%s start: %ld end: %ld h->size: %zu\n", __func__,
+				start, end, h->size);
+		nvmap_handle_put(h);
+		return -EFAULT;
+	}
+
+	if (!(h->heap_type & nvmap_dev->cpu_access_mask)) {
+		pr_debug("%s heap_type %u access_mask 0x%x\n", __func__,
+				h->heap_type, nvmap_dev->cpu_access_mask);
+		nvmap_handle_put(h);
+		return -EPERM;
+	}
+
+	nvmap_kmaps_inc(h);
+	if (op == NVMAP_CACHE_OP_INV)
+		op = NVMAP_CACHE_OP_WB_INV;
+
+	/* clean only dirty is applicable only for Write Back operation */
+	if (op != NVMAP_CACHE_OP_WB)
+		clean_only_dirty = false;
+
+	cache_op.h = h;
+	cache_op.start = start ? start : 0;
+	cache_op.end = end ? end : h->size;
+	cache_op.op = op;
+	nvmap_get_cacheability(h, &cache_op.inner, &cache_op.outer);
+	cache_op.clean_only_dirty = clean_only_dirty;
+
+	nvmap_stats_inc(NS_CFLUSH_RQ, end - start);
+	err = do_cache_maint(&cache_op);
+	nvmap_kmaps_dec(h);
+	nvmap_handle_put(h);
+	return err;
+}
+
+int __nvmap_cache_maint(struct nvmap_client *client,
+			       struct nvmap_cache_op_64 *op)
+{
+	struct vm_area_struct *vma;
+	struct nvmap_vma_priv *priv;
+	struct nvmap_handle *handle;
+	unsigned long start;
+	unsigned long end;
+	int err = 0;
+
+	if (!op->addr || op->op < NVMAP_CACHE_OP_WB ||
+	    op->op > NVMAP_CACHE_OP_WB_INV)
+		return -EINVAL;
+
+	handle = nvmap_handle_get_from_fd(op->handle);
+	if (!handle)
+		return -EINVAL;
+
+	down_read(&current->mm->mmap_sem);
+
+	vma = find_vma(current->active_mm, (unsigned long)op->addr);
+	if (!vma || !is_nvmap_vma(vma) ||
+	    (ulong)op->addr < vma->vm_start ||
+	    (ulong)op->addr >= vma->vm_end ||
+	    op->len > vma->vm_end - (ulong)op->addr) {
+		err = -EADDRNOTAVAIL;
+		goto out;
+	}
+
+	priv = (struct nvmap_vma_priv *)vma->vm_private_data;
+
+	if (priv->handle != handle) {
+		err = -EFAULT;
+		goto out;
+	}
+
+	start = (unsigned long)op->addr - vma->vm_start +
+		(vma->vm_pgoff << PAGE_SHIFT);
+	end = start + op->len;
+
+	err = __nvmap_do_cache_maint(client, priv->handle, start, end, op->op,
+				     false);
+out:
+	up_read(&current->mm->mmap_sem);
+	nvmap_handle_put(handle);
+	return err;
+}
+
+/*
+ * Perform cache op on the list of memory regions within passed handles.
+ * A memory region within handle[i] is identified by offsets[i], sizes[i]
+ *
+ * sizes[i] == 0  is a special case which causes handle wide operation,
+ * this is done by replacing offsets[i] = 0, sizes[i] = handles[i]->size.
+ * So, the input arrays sizes, offsets  are not guaranteed to be read-only
+ *
+ * This will optimze the op if it can.
+ * In the case that all the handles together are larger than the inner cache
+ * maint threshold it is possible to just do an entire inner cache flush.
+ *
+ * NOTE: this omits outer cache operations which is fine for ARM64
+ */
+static int __nvmap_do_cache_maint_list(struct nvmap_handle **handles,
+				u64 *offsets, u64 *sizes, int op, u32 nr_ops,
+				bool is_32)
+{
+	u32 i;
+	u64 total = 0;
+	u64 thresh = ~0;
+
+	WARN(!IS_ENABLED(CONFIG_ARM64),
+		"cache list operation may not function properly");
+
+	if (nvmap_cache_maint_by_set_ways)
+		thresh = cache_maint_inner_threshold;
+
+	for (i = 0; i < nr_ops; i++) {
+		bool inner, outer;
+		u32 *sizes_32 = (u32 *)sizes;
+		u64 size = is_32 ? sizes_32[i] : sizes[i];
+
+		nvmap_get_cacheability(handles[i], &inner, &outer);
+
+		if (!inner && !outer)
+			continue;
+
+		if ((op == NVMAP_CACHE_OP_WB) && nvmap_handle_track_dirty(handles[i]))
+			total += atomic_read(&handles[i]->pgalloc.ndirty);
+		else
+			total += size ? size : handles[i]->size;
+	}
+
+	if (!total)
+		return 0;
+
+	/* Full flush in the case the passed list is bigger than our
+	 * threshold. */
+	if (total >= thresh) {
+		for (i = 0; i < nr_ops; i++) {
+			if (handles[i]->userflags &
+			    NVMAP_HANDLE_CACHE_SYNC) {
+				nvmap_handle_mkclean(handles[i], 0,
+						     handles[i]->size);
+				nvmap_zap_handle(handles[i], 0,
+						 handles[i]->size);
+			}
+		}
+
+		if (op == NVMAP_CACHE_OP_WB)
+			inner_clean_cache_all();
+		else
+			inner_flush_cache_all();
+		nvmap_stats_inc(NS_CFLUSH_RQ, total);
+		nvmap_stats_inc(NS_CFLUSH_DONE, thresh);
+		trace_nvmap_cache_flush(total,
+					nvmap_stats_read(NS_ALLOC),
+					nvmap_stats_read(NS_CFLUSH_RQ),
+					nvmap_stats_read(NS_CFLUSH_DONE));
+	} else {
+		for (i = 0; i < nr_ops; i++) {
+			u32 *offs_32 = (u32 *)offsets, *sizes_32 = (u32 *)sizes;
+			u64 size = is_32 ? sizes_32[i] : sizes[i];
+			u64 offset = is_32 ? offs_32[i] : offsets[i];
+			int err;
+
+			size = size ?: handles[i]->size;
+			offset = offset ?: 0;
+			err = __nvmap_do_cache_maint(handles[i]->owner,
+						     handles[i], offset,
+						     offset + size,
+						     op, false);
+			if (err) {
+				pr_err("cache maint per handle failed [%d]\n",
+						err);
+				return err;
+			}
+		}
+	}
+
+	return 0;
+}
+
+inline int nvmap_do_cache_maint_list(struct nvmap_handle **handles,
+				u64 *offsets, u64 *sizes, int op, u32 nr_ops,
+				bool is_32)
+{
+	int ret = 0;
+
+	switch (tegra_get_chip_id()) {
+	case TEGRA194:
+		/*
+		 * As io-coherency is enabled by default from T194 onwards,
+		 * Don't do cache maint from CPU side. The HW, SCF will do.
+		 */
+		break;
+	default:
+		ret = __nvmap_do_cache_maint_list(handles,
+					offsets, sizes, op, nr_ops, is_32);
+		break;
+	}
+
+	return ret;
+}
+
+static int cache_inner_threshold_show(struct seq_file *m, void *v)
+{
+	if (nvmap_cache_maint_by_set_ways)
+		seq_printf(m, "%zuB\n", cache_maint_inner_threshold);
+	else
+		seq_printf(m, "%zuB\n", SIZE_MAX);
+	return 0;
+}
+
+static int cache_inner_threshold_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, cache_inner_threshold_show, inode->i_private);
+}
+
+static ssize_t cache_inner_threshold_write(struct file *file,
+					const char __user *buffer,
+					size_t count, loff_t *pos)
+{
+	int ret;
+	struct seq_file *p = file->private_data;
+	char str[] = "0123456789abcdef";
+
+	count = min_t(size_t, strlen(str), count);
+	if (copy_from_user(str, buffer, count))
+		return -EINVAL;
+
+	if (!nvmap_cache_maint_by_set_ways)
+		return -EINVAL;
+
+	mutex_lock(&p->lock);
+	ret = sscanf(str, "%16zu", &cache_maint_inner_threshold);
+	mutex_unlock(&p->lock);
+	if (ret != 1)
+		return -EINVAL;
+
+	pr_debug("nvmap:cache_maint_inner_threshold is now :%zuB\n",
+			cache_maint_inner_threshold);
+	return count;
+}
+
+static const struct file_operations cache_inner_threshold_fops = {
+	.open		= cache_inner_threshold_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+	.write		= cache_inner_threshold_write,
+};
+
+int nvmap_cache_debugfs_init(struct dentry *nvmap_root)
+{
+	struct dentry *cache_root;
+
+	if (!nvmap_root)
+		return -ENODEV;
+
+	cache_root = debugfs_create_dir("cache", nvmap_root);
+	if (!cache_root)
+		return -ENODEV;
+
+	if (nvmap_cache_maint_by_set_ways) {
+		debugfs_create_x32("nvmap_cache_maint_by_set_ways",
+				   S_IRUSR | S_IWUSR,
+				   cache_root,
+				   &nvmap_cache_maint_by_set_ways);
+
+	debugfs_create_file("cache_maint_inner_threshold",
+			    S_IRUSR | S_IWUSR,
+			    cache_root,
+			    NULL,
+			    &cache_inner_threshold_fops);
+	}
+
+	debugfs_create_atomic_t("nvmap_disable_vaddr_for_cache_maint",
+				S_IRUSR | S_IWUSR,
+				cache_root,
+				&nvmap_disable_vaddr_for_cache_maint.enabled);
+
+	return 0;
+}
diff --git a/drivers/video/tegra/nvmap/nvmap_cache_nvmap_t18x.c b/drivers/video/tegra/nvmap/nvmap_cache_nvmap_t18x.c
new file mode 100644
index 000000000000..1be5ef07b7e4
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nvmap_cache_nvmap_t18x.c
@@ -0,0 +1,73 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_cache.c
+ *
+ * Copyright (c) 2015-2020, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"nvmap: %s() " fmt, __func__
+
+#include "nvmap_priv.h"
+#include <linux/tegra-mce.h>
+#include <soc/tegra/chip-id.h>
+
+static void nvmap_roc_flush_cache(void)
+{
+	int ret;
+
+	if (!tegra_platform_is_silicon() && !tegra_platform_is_fpga()) {
+		pr_info_once("ROC flush supported on only FPGA and silicon\n");
+		pr_info_once("Fall back to flush by VA\n");
+		nvmap_cache_maint_by_set_ways = 0;
+		return;
+	}
+
+	ret = tegra_flush_dcache_all(NULL);
+	if (ret) {
+		pr_info_once("ROC flush failed with %u\n", ret);
+		pr_info_once("Fall back to flush by VA\n");
+		nvmap_cache_maint_by_set_ways = 0;
+	}
+}
+
+static void nvmap_roc_clean_cache(void)
+{
+	int ret;
+
+	if (!tegra_platform_is_silicon() && !tegra_platform_is_fpga()) {
+		pr_info_once("ROC flush supported on only FPGA and silicon\n");
+		return;
+	}
+
+	ret = tegra_clean_dcache_all(NULL);
+	if (ret) {
+		pr_info_once("ROC clean failed with %u\n", ret);
+		pr_info_once("Fall back to clean by VA\n");
+		nvmap_cache_maint_by_set_ways = 0;
+	}
+}
+
+static void nvmap_handle_get_cacheability(struct nvmap_handle *h,
+		bool *inner, bool *outer)
+{
+	*inner = h->flags == NVMAP_HANDLE_CACHEABLE ||
+		 h->flags == NVMAP_HANDLE_INNER_CACHEABLE;
+	*outer = h->flags == NVMAP_HANDLE_CACHEABLE;
+}
+
+static void nvmap_setup_t18x_cache_ops(struct nvmap_chip_cache_op *op)
+{
+	op->inner_flush_cache_all = nvmap_roc_flush_cache;
+	op->inner_clean_cache_all = nvmap_roc_clean_cache;
+	op->nvmap_get_cacheability = nvmap_handle_get_cacheability;
+	op->name = kstrdup("roc", GFP_KERNEL);
+}
+NVMAP_CACHE_OF_DECLARE("nvidia,carveouts-t18x", nvmap_setup_t18x_cache_ops);
diff --git a/drivers/video/tegra/nvmap/nvmap_cache_t19x.c b/drivers/video/tegra/nvmap/nvmap_cache_t19x.c
new file mode 100644
index 000000000000..97532775207c
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nvmap_cache_t19x.c
@@ -0,0 +1,63 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_cache_t19x.c
+ *
+ * Copyright (c) 2016-2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"nvmap: %s() " fmt, __func__
+
+#include <linux/miscdevice.h>
+#include <linux/nvmap_t19x.h>
+
+#include "nvmap_priv.h"
+
+struct static_key nvmap_updated_cache_config;
+
+static void nvmap_handle_get_cacheability(struct nvmap_handle *h,
+		bool *inner, bool *outer)
+{
+	struct nvmap_handle_t19x *handle_t19x = NULL;
+	struct device *dev = nvmap_dev->dev_user.parent;
+
+	handle_t19x = dma_buf_get_drvdata(h->dmabuf, dev);
+	if (!IS_ERR_OR_NULL(handle_t19x) && atomic_read(&handle_t19x->nc_pin)) {
+		*inner = *outer = false;
+		return;
+	}
+
+	*inner = h->flags == NVMAP_HANDLE_CACHEABLE ||
+		 h->flags == NVMAP_HANDLE_INNER_CACHEABLE;
+	*outer = h->flags == NVMAP_HANDLE_CACHEABLE;
+}
+
+static void nvmap_t19x_flush_cache(void)
+{
+	void *unused = NULL;
+
+	tegra_flush_dcache_all(unused);
+}
+
+static void nvmap_t19x_clean_cache(void)
+{
+	void *unused = NULL;
+
+	tegra_clean_dcache_all(unused);
+}
+
+static void nvmap_setup_t19x_cache_ops(struct nvmap_chip_cache_op *op)
+{
+	op->inner_flush_cache_all = nvmap_t19x_flush_cache;
+	op->inner_clean_cache_all = nvmap_t19x_clean_cache;
+	op->nvmap_get_cacheability = nvmap_handle_get_cacheability;
+	op->name = kstrdup("scf", GFP_KERNEL);
+}
+NVMAP_CACHE_OF_DECLARE("nvidia,carveouts-t19x", nvmap_setup_t19x_cache_ops);
diff --git a/drivers/video/tegra/nvmap/nvmap_carveout.c b/drivers/video/tegra/nvmap/nvmap_carveout.c
new file mode 100644
index 000000000000..f9389c0891a8
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nvmap_carveout.c
@@ -0,0 +1,152 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_dev.c
+ *
+ * Interface with nvmap carveouts
+ *
+ * Copyright (c) 2011-2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#include <linux/debugfs.h>
+
+#include "nvmap_priv.h"
+
+extern struct nvmap_device *nvmap_dev;
+
+extern const struct file_operations debug_clients_fops;
+extern const struct file_operations debug_allocations_fops;
+extern const struct file_operations debug_all_allocations_fops;
+extern const struct file_operations debug_orphan_handles_fops;
+extern const struct file_operations debug_maps_fops;
+
+int nvmap_create_carveout(const struct nvmap_platform_carveout *co)
+{
+	int i, err = 0;
+	struct nvmap_carveout_node *node;
+
+	if (!nvmap_dev->heaps) {
+		nvmap_dev->nr_carveouts = 0;
+		nvmap_dev->nr_heaps = nvmap_dev->plat->nr_carveouts + 1;
+		nvmap_dev->heaps = kzalloc(sizeof(struct nvmap_carveout_node) *
+				     nvmap_dev->nr_heaps, GFP_KERNEL);
+		if (!nvmap_dev->heaps) {
+			err = -ENOMEM;
+			pr_err("couldn't allocate carveout memory\n");
+			goto out;
+		}
+	} else if (nvmap_dev->nr_carveouts >= nvmap_dev->nr_heaps) {
+		node = krealloc(nvmap_dev->heaps,
+				sizeof(*node) * (nvmap_dev->nr_carveouts + 1),
+				GFP_KERNEL);
+		if (!node) {
+			err = -ENOMEM;
+			pr_err("nvmap heap array resize failed\n");
+			goto out;
+		}
+		nvmap_dev->heaps = node;
+		nvmap_dev->nr_heaps = nvmap_dev->nr_carveouts + 1;
+	}
+
+	for (i = 0; i < nvmap_dev->nr_heaps; i++)
+		if ((co->usage_mask != NVMAP_HEAP_CARVEOUT_IVM) &&
+		    (nvmap_dev->heaps[i].heap_bit & co->usage_mask)) {
+			pr_err("carveout %s already exists\n", co->name);
+			return -EEXIST;
+		}
+
+	node = &nvmap_dev->heaps[nvmap_dev->nr_carveouts];
+
+	node->base = round_up(co->base, PAGE_SIZE);
+	node->size = round_down(co->size -
+				(node->base - co->base), PAGE_SIZE);
+	if (!co->size)
+		goto out;
+
+	node->carveout = nvmap_heap_create(
+			nvmap_dev->dev_user.this_device, co,
+			node->base, node->size, node);
+
+	if (!node->carveout) {
+		err = -ENOMEM;
+		pr_err("couldn't create %s\n", co->name);
+		goto out;
+	}
+	node->index = nvmap_dev->nr_carveouts;
+	nvmap_dev->nr_carveouts++;
+	node->heap_bit = co->usage_mask;
+
+	if (!IS_ERR_OR_NULL(nvmap_dev->debug_root)) {
+		struct dentry *heap_root =
+			debugfs_create_dir(co->name, nvmap_dev->debug_root);
+		if (!IS_ERR_OR_NULL(heap_root)) {
+			debugfs_create_file("clients", S_IRUGO,
+				heap_root,
+				(void *)(uintptr_t)node->heap_bit,
+				&debug_clients_fops);
+			debugfs_create_file("allocations", S_IRUGO,
+				heap_root,
+				(void *)(uintptr_t)node->heap_bit,
+				&debug_allocations_fops);
+			debugfs_create_file("all_allocations", S_IRUGO,
+				heap_root,
+				(void *)(uintptr_t)node->heap_bit,
+				&debug_all_allocations_fops);
+			debugfs_create_file("orphan_handles", S_IRUGO,
+				heap_root,
+				(void *)(uintptr_t)node->heap_bit,
+				&debug_orphan_handles_fops);
+			debugfs_create_file("maps", S_IRUGO,
+				heap_root,
+				(void *)(uintptr_t)node->heap_bit,
+				&debug_maps_fops);
+			nvmap_heap_debugfs_init(heap_root,
+						node->carveout);
+		}
+	}
+out:
+	return err;
+}
+
+static
+struct nvmap_heap_block *do_nvmap_carveout_alloc(struct nvmap_client *client,
+					      struct nvmap_handle *handle,
+					      unsigned long type,
+					      phys_addr_t *start)
+{
+	struct nvmap_carveout_node *co_heap;
+	struct nvmap_device *dev = nvmap_dev;
+	int i;
+
+	for (i = 0; i < dev->nr_carveouts; i++) {
+		struct nvmap_heap_block *block;
+		co_heap = &dev->heaps[i];
+
+		if (!(co_heap->heap_bit & type))
+			continue;
+
+		if (type &
+			(NVMAP_HEAP_CARVEOUT_IVM | NVMAP_HEAP_CARVEOUT_IVM_VPR))
+			handle->size = ALIGN(handle->size, NVMAP_IVM_ALIGNMENT);
+
+		block = nvmap_heap_alloc(co_heap->carveout, handle, start);
+		if (block)
+			return block;
+	}
+	return NULL;
+}
+
+struct nvmap_heap_block *nvmap_carveout_alloc(struct nvmap_client *client,
+					      struct nvmap_handle *handle,
+					      unsigned long type,
+					      phys_addr_t *start)
+{
+	return do_nvmap_carveout_alloc(client, handle, type, start);
+}
diff --git a/drivers/video/tegra/nvmap/nvmap_dev.c b/drivers/video/tegra/nvmap/nvmap_dev.c
new file mode 100644
index 000000000000..eb579146485f
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nvmap_dev.c
@@ -0,0 +1,1378 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_dev.c
+ *
+ * User-space interface to nvmap
+ *
+ * Copyright (c) 2011-2021, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#include <linux/backing-dev.h>
+#include <linux/bitmap.h>
+#include <linux/debugfs.h>
+#include <linux/delay.h>
+#include <linux/io.h>
+#include <linux/kernel.h>
+#include <linux/device.h>
+#include <linux/oom.h>
+#include <linux/platform_device.h>
+#include <linux/seq_file.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/uaccess.h>
+#include <linux/vmalloc.h>
+#include <linux/nvmap.h>
+#include <linux/module.h>
+#include <linux/resource.h>
+#include <linux/security.h>
+#include <linux/stat.h>
+#include <linux/kthread.h>
+#include <linux/highmem.h>
+#include <linux/lzo.h>
+#include <linux/swap.h>
+#include <linux/swapops.h>
+#include <linux/of.h>
+#include <linux/iommu.h>
+#include <linux/version.h>
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+#include <linux/sched/mm.h>
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0)
+#include <linux/backing-dev.h>
+#endif
+
+#include <asm/cputype.h>
+
+#define CREATE_TRACE_POINTS
+#include <trace/events/nvmap.h>
+
+#include "nvmap_priv.h"
+#include "nvmap_ioctl.h"
+
+#define NVMAP_CARVEOUT_KILLER_RETRY_TIME 100 /* msecs */
+
+struct nvmap_device *nvmap_dev;
+EXPORT_SYMBOL(nvmap_dev);
+ulong nvmap_init_time;
+
+static struct device_dma_parameters nvmap_dma_parameters = {
+	.max_segment_size = UINT_MAX,
+};
+
+static int nvmap_open(struct inode *inode, struct file *filp);
+static int nvmap_release(struct inode *inode, struct file *filp);
+static long nvmap_ioctl(struct file *filp, unsigned int cmd, unsigned long arg);
+static int nvmap_map(struct file *filp, struct vm_area_struct *vma);
+#if !defined(CONFIG_MMU) && (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+static unsigned nvmap_mmap_capabilities(struct file *filp);
+#endif
+
+static const struct file_operations nvmap_user_fops = {
+	.owner		= THIS_MODULE,
+	.open		= nvmap_open,
+	.release	= nvmap_release,
+	.unlocked_ioctl	= nvmap_ioctl,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl = nvmap_ioctl,
+#endif
+	.mmap		= nvmap_map,
+#if !defined(CONFIG_MMU) && (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0))
+	.mmap_capabilities = nvmap_mmap_capabilities,
+#endif
+};
+
+static const struct file_operations debug_handles_by_pid_fops;
+
+struct nvmap_pid_data {
+	struct rb_node node;
+	pid_t pid;
+	struct kref refcount;
+	struct dentry *handles_file;
+};
+
+static void nvmap_pid_release_locked(struct kref *kref)
+{
+	struct nvmap_pid_data *p = container_of(kref, struct nvmap_pid_data,
+			refcount);
+	debugfs_remove(p->handles_file);
+	rb_erase(&p->node, &nvmap_dev->pids);
+	kfree(p);
+}
+
+static void nvmap_pid_get_locked(struct nvmap_device *dev, pid_t pid)
+{
+	struct rb_root *root = &dev->pids;
+	struct rb_node **new = &(root->rb_node), *parent = NULL;
+	struct nvmap_pid_data *p;
+	char name[16];
+
+	while (*new) {
+		p = container_of(*new, struct nvmap_pid_data, node);
+		parent = *new;
+
+		if (p->pid > pid) {
+			new = &((*new)->rb_left);
+		} else if (p->pid < pid) {
+			new = &((*new)->rb_right);
+		} else {
+			kref_get(&p->refcount);
+			return;
+		}
+	}
+
+	p = kzalloc(sizeof(*p), GFP_KERNEL);
+	if (!p)
+		return;
+
+	snprintf(name, sizeof(name), "%d", pid);
+	p->pid = pid;
+	kref_init(&p->refcount);
+	p->handles_file = debugfs_create_file(name, S_IRUGO,
+			dev->handles_by_pid, p,
+			&debug_handles_by_pid_fops);
+
+	if (IS_ERR_OR_NULL(p->handles_file)) {
+		kfree(p);
+	} else {
+		rb_link_node(&p->node, parent, new);
+		rb_insert_color(&p->node, root);
+	}
+}
+
+static struct nvmap_pid_data *nvmap_pid_find_locked(struct nvmap_device *dev,
+		pid_t pid)
+{
+	struct rb_node *node = dev->pids.rb_node;
+
+	while (node) {
+		struct nvmap_pid_data *p = container_of(node,
+				struct nvmap_pid_data, node);
+
+		if (p->pid > pid)
+			node = node->rb_left;
+		else if (p->pid < pid)
+			node = node->rb_right;
+		else
+			return p;
+	}
+	return NULL;
+}
+
+static void nvmap_pid_put_locked(struct nvmap_device *dev, pid_t pid)
+{
+	struct nvmap_pid_data *p = nvmap_pid_find_locked(dev, pid);
+	if (p)
+		kref_put(&p->refcount, nvmap_pid_release_locked);
+}
+
+struct nvmap_client *__nvmap_create_client(struct nvmap_device *dev,
+					   const char *name)
+{
+	struct nvmap_client *client;
+	struct task_struct *task;
+
+	if (WARN_ON(!dev))
+		return NULL;
+
+	client = kzalloc(sizeof(*client), GFP_KERNEL);
+	if (!client)
+		return NULL;
+
+	client->name = name;
+	client->kernel_client = true;
+	client->handle_refs = RB_ROOT;
+
+	get_task_struct(current->group_leader);
+	task_lock(current->group_leader);
+	/* don't bother to store task struct for kernel threads,
+	   they can't be killed anyway */
+	if (current->flags & PF_KTHREAD) {
+		put_task_struct(current->group_leader);
+		task = NULL;
+	} else {
+		task = current->group_leader;
+	}
+	task_unlock(current->group_leader);
+	client->task = task;
+
+	mutex_init(&client->ref_lock);
+	atomic_set(&client->count, 1);
+
+	mutex_lock(&dev->clients_lock);
+	list_add(&client->list, &dev->clients);
+	if (!IS_ERR_OR_NULL(dev->handles_by_pid)) {
+		pid_t pid = nvmap_client_pid(client);
+		nvmap_pid_get_locked(dev, pid);
+	}
+	mutex_unlock(&dev->clients_lock);
+	return client;
+}
+
+static void destroy_client(struct nvmap_client *client)
+{
+	struct rb_node *n;
+
+	if (!client)
+		return;
+
+	mutex_lock(&nvmap_dev->clients_lock);
+	if (!IS_ERR_OR_NULL(nvmap_dev->handles_by_pid)) {
+		pid_t pid = nvmap_client_pid(client);
+		nvmap_pid_put_locked(nvmap_dev, pid);
+	}
+	list_del(&client->list);
+	mutex_unlock(&nvmap_dev->clients_lock);
+
+	while ((n = rb_first(&client->handle_refs))) {
+		struct nvmap_handle_ref *ref;
+		int dupes;
+
+		ref = rb_entry(n, struct nvmap_handle_ref, node);
+		smp_rmb();
+		if (ref->handle->owner == client)
+			ref->handle->owner = NULL;
+
+		dma_buf_put(ref->handle->dmabuf);
+		rb_erase(&ref->node, &client->handle_refs);
+		atomic_dec(&ref->handle->share_count);
+
+		dupes = atomic_read(&ref->dupes);
+		while (dupes--)
+			nvmap_handle_put(ref->handle);
+
+		kfree(ref);
+	}
+
+	if (client->task)
+		put_task_struct(client->task);
+
+	kfree(client);
+}
+
+static int nvmap_open(struct inode *inode, struct file *filp)
+{
+	struct miscdevice *miscdev = filp->private_data;
+	struct nvmap_device *dev = dev_get_drvdata(miscdev->parent);
+	struct nvmap_client *priv;
+	int ret;
+	__attribute__((unused)) struct rlimit old_rlim, new_rlim;
+
+	ret = nonseekable_open(inode, filp);
+	if (unlikely(ret))
+		return ret;
+
+	BUG_ON(dev != nvmap_dev);
+	priv = __nvmap_create_client(dev, "user");
+	if (!priv)
+		return -ENOMEM;
+	trace_nvmap_open(priv, priv->name);
+
+	priv->kernel_client = false;
+
+	filp->private_data = priv;
+	return 0;
+}
+
+static int nvmap_release(struct inode *inode, struct file *filp)
+{
+	struct nvmap_client *priv = filp->private_data;
+
+	if(!priv)
+		return 0;
+
+	trace_nvmap_release(priv, priv->name);
+
+	if (!atomic_dec_return(&priv->count))
+		destroy_client(priv);
+
+	return 0;
+}
+
+static int nvmap_map(struct file *filp, struct vm_area_struct *vma)
+{
+	char task_comm[TASK_COMM_LEN];
+
+	get_task_comm(task_comm, current);
+	pr_err("error: mmap not supported on nvmap file, pid=%d, %s\n",
+		task_tgid_nr(current), task_comm);
+	return -EPERM;
+}
+
+static long nvmap_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
+{
+	int err = 0;
+	void __user *uarg = (void __user *)arg;
+
+	if (_IOC_TYPE(cmd) != NVMAP_IOC_MAGIC)
+		return -ENOTTY;
+
+	if (_IOC_NR(cmd) > NVMAP_IOC_MAXNR)
+		return -ENOTTY;
+
+	if (_IOC_DIR(cmd) & _IOC_READ)
+		err = !access_ok(VERIFY_WRITE, uarg, _IOC_SIZE(cmd));
+	if (!err && (_IOC_DIR(cmd) & _IOC_WRITE))
+		err = !access_ok(VERIFY_READ, uarg, _IOC_SIZE(cmd));
+
+	if (err)
+		return -EFAULT;
+
+	err = -ENOTTY;
+
+	switch (cmd) {
+	case NVMAP_IOC_CREATE:
+	case NVMAP_IOC_CREATE_64:
+	case NVMAP_IOC_FROM_FD:
+		err = nvmap_ioctl_create(filp, cmd, uarg);
+		break;
+
+	case NVMAP_IOC_FROM_VA:
+		err = nvmap_ioctl_create_from_va(filp, uarg);
+		break;
+
+	case NVMAP_IOC_GET_FD:
+		err = nvmap_ioctl_getfd(filp, uarg);
+		break;
+
+	case NVMAP_IOC_GET_IVM_HEAPS:
+		err = nvmap_ioctl_get_ivc_heap(filp, uarg);
+		break;
+
+	case NVMAP_IOC_FROM_IVC_ID:
+		err = nvmap_ioctl_create_from_ivc(filp, uarg);
+		break;
+
+	case NVMAP_IOC_GET_IVC_ID:
+		err = nvmap_ioctl_get_ivcid(filp, uarg);
+		break;
+
+	case NVMAP_IOC_ALLOC:
+		err = nvmap_ioctl_alloc(filp, uarg);
+		break;
+
+	case NVMAP_IOC_ALLOC_IVM:
+		err = nvmap_ioctl_alloc_ivm(filp, uarg);
+		break;
+
+	case NVMAP_IOC_VPR_FLOOR_SIZE:
+		err = nvmap_ioctl_vpr_floor_size(filp, uarg);
+		break;
+
+	case NVMAP_IOC_FREE:
+		err = nvmap_ioctl_free(filp, arg);
+		break;
+
+#ifdef CONFIG_COMPAT
+	case NVMAP_IOC_WRITE_32:
+	case NVMAP_IOC_READ_32:
+		err = nvmap_ioctl_rw_handle(filp, cmd == NVMAP_IOC_READ_32,
+			uarg, sizeof(struct nvmap_rw_handle_32));
+		break;
+#endif
+
+	case NVMAP_IOC_WRITE:
+	case NVMAP_IOC_READ:
+		err = nvmap_ioctl_rw_handle(filp, cmd == NVMAP_IOC_READ, uarg,
+			sizeof(struct nvmap_rw_handle));
+		break;
+
+	case NVMAP_IOC_WRITE_64:
+	case NVMAP_IOC_READ_64:
+		err = nvmap_ioctl_rw_handle(filp, cmd == NVMAP_IOC_READ_64,
+			uarg, sizeof(struct nvmap_rw_handle_64));
+		break;
+
+#ifdef CONFIG_COMPAT
+	case NVMAP_IOC_CACHE_32:
+		err = nvmap_ioctl_cache_maint(filp, uarg,
+			sizeof(struct nvmap_cache_op_32));
+		break;
+#endif
+
+	case NVMAP_IOC_CACHE:
+		err = nvmap_ioctl_cache_maint(filp, uarg,
+			sizeof(struct nvmap_cache_op));
+		break;
+
+	case NVMAP_IOC_CACHE_64:
+		err = nvmap_ioctl_cache_maint(filp, uarg,
+			sizeof(struct nvmap_cache_op_64));
+		break;
+
+	case NVMAP_IOC_CACHE_LIST:
+	case NVMAP_IOC_RESERVE:
+		err = nvmap_ioctl_cache_maint_list(filp, uarg,
+						   cmd == NVMAP_IOC_RESERVE);
+		break;
+
+	case NVMAP_IOC_GUP_TEST:
+		err = nvmap_ioctl_gup_test(filp, uarg);
+		break;
+
+	/* Depreacted IOCTL's */
+	case NVMAP_IOC_ALLOC_KIND:
+		pr_warn("NVMAP_IOC_ALLOC_KIND is deprecated. Use NVMAP_IOC_ALLOC.\n");
+		break;
+
+#ifdef CONFIG_COMPAT
+	case NVMAP_IOC_MMAP_32:
+#endif
+	case NVMAP_IOC_MMAP:
+		pr_warn("NVMAP_IOC_MMAP is deprecated. Use mmap().\n");
+		break;
+
+#ifdef CONFIG_COMPAT
+	case NVMAP_IOC_UNPIN_MULT_32:
+	case NVMAP_IOC_PIN_MULT_32:
+		pr_warn("NVMAP_IOC_[UN]PIN_MULT is deprecated. "
+			"User space must never pin NvMap handles to "
+			"allow multiple IOVA spaces.\n");
+		break;
+#endif
+
+	case NVMAP_IOC_UNPIN_MULT:
+	case NVMAP_IOC_PIN_MULT:
+		pr_warn("NVMAP_IOC_[UN]PIN_MULT/ is deprecated. "
+			"User space must never pin NvMap handles to "
+			"allow multiple IOVA spaces.\n");
+		break;
+
+	case NVMAP_IOC_FROM_ID:
+	case NVMAP_IOC_GET_ID:
+		pr_warn("NVMAP_IOC_GET_ID/FROM_ID pair is deprecated. "
+			"Use the pair NVMAP_IOC_GET_FD/FROM_FD.\n");
+		break;
+
+	case NVMAP_IOC_SHARE:
+		pr_warn("NVMAP_IOC_SHARE is deprecated. Use NVMAP_IOC_GET_FD.\n");
+		break;
+
+	case NVMAP_IOC_SET_TAG_LABEL:
+		err = nvmap_ioctl_set_tag_label(filp, uarg);
+		break;
+
+	case NVMAP_IOC_GET_AVAILABLE_HEAPS:
+		err = nvmap_ioctl_get_available_heaps(filp, uarg);
+		break;
+
+	case NVMAP_IOC_GET_HEAP_SIZE:
+		err = nvmap_ioctl_get_heap_size(filp, uarg);
+		break;
+
+	case NVMAP_IOC_QUERY_HEAP_PARAMS:
+		err = nvmap_ioctl_query_heap_params(filp, uarg);
+		break;
+
+	case NVMAP_IOC_PARAMETERS:
+		err = nvmap_ioctl_query_handle_parameters(filp, uarg);
+		break;
+
+	default:
+		pr_warn("Unknown NVMAP_IOC = 0x%x\n", cmd);
+	}
+	return err;
+}
+
+#define DEBUGFS_OPEN_FOPS_STATIC(name) \
+static int nvmap_debug_##name##_open(struct inode *inode, \
+					    struct file *file) \
+{ \
+	return single_open(file, nvmap_debug_##name##_show, \
+			    inode->i_private); \
+} \
+\
+static const struct file_operations debug_##name##_fops = { \
+	.open = nvmap_debug_##name##_open, \
+	.read = seq_read, \
+	.llseek = seq_lseek, \
+	.release = single_release, \
+}
+
+#define DEBUGFS_OPEN_FOPS(name) \
+static int nvmap_debug_##name##_open(struct inode *inode, \
+					    struct file *file) \
+{ \
+	return single_open(file, nvmap_debug_##name##_show, \
+			    inode->i_private); \
+} \
+\
+const struct file_operations debug_##name##_fops = { \
+	.open = nvmap_debug_##name##_open, \
+	.read = seq_read, \
+	.llseek = seq_lseek, \
+	.release = single_release, \
+}
+
+#define K(x) (x >> 10)
+
+static void client_stringify(struct nvmap_client *client, struct seq_file *s)
+{
+	char task_comm[TASK_COMM_LEN];
+	if (!client->task) {
+		seq_printf(s, "%-18s %18s %8u", client->name, "kernel", 0);
+		return;
+	}
+	get_task_comm(task_comm, client->task);
+	seq_printf(s, "%-18s %18s %8u", client->name, task_comm,
+		   client->task->pid);
+}
+
+static void allocations_stringify(struct nvmap_client *client,
+				  struct seq_file *s, u32 heap_type)
+{
+	struct rb_node *n;
+	unsigned int pin_count = 0;
+	struct nvmap_device *dev = nvmap_dev;
+
+	nvmap_ref_lock(client);
+	mutex_lock(&dev->tags_lock);
+	n = rb_first(&client->handle_refs);
+	for (; n != NULL; n = rb_next(n)) {
+		struct nvmap_handle_ref *ref =
+			rb_entry(n, struct nvmap_handle_ref, node);
+		struct nvmap_handle *handle = ref->handle;
+		if (handle->alloc && handle->heap_type == heap_type) {
+			phys_addr_t base = heap_type == NVMAP_HEAP_IOVMM ? 0 :
+					   handle->heap_pgalloc ? 0 :
+					   (handle->carveout->base);
+			size_t size = K(handle->size);
+			int i = 0;
+
+next_page:
+			if ((heap_type == NVMAP_HEAP_CARVEOUT_VPR) && handle->heap_pgalloc) {
+				base = page_to_phys(handle->pgalloc.pages[i++]);
+				size = K(PAGE_SIZE);
+			}
+
+			seq_printf(s,
+				"%-18s %-18s %8llx %10zuK %8x %6u %6u %6u %6u %6u %6u %8pK %s\n",
+				"", "",
+				(unsigned long long)base, size,
+				handle->userflags,
+				atomic_read(&handle->ref),
+				atomic_read(&ref->dupes),
+				pin_count,
+				atomic_read(&handle->kmap_count),
+				atomic_read(&handle->umap_count),
+				atomic_read(&handle->share_count),
+				handle,
+				__nvmap_tag_name(dev, handle->userflags >> 16));
+
+			if ((heap_type == NVMAP_HEAP_CARVEOUT_VPR) && handle->heap_pgalloc) {
+				i++;
+				if (i < (handle->size >> PAGE_SHIFT))
+					goto next_page;
+			}
+		}
+	}
+	mutex_unlock(&dev->tags_lock);
+	nvmap_ref_unlock(client);
+}
+
+bool nvmap_memory_available(size_t size)
+{
+	struct sysinfo i;
+
+	si_meminfo(&i);
+	if (size >> PAGE_SHIFT >= i.totalram) {
+		pr_debug("Requested allocation size is more than system memory");
+		return false;
+	}
+	return true;
+}
+
+/* compute the total amount of handle physical memory that is mapped
+ * into client's virtual address space. Remember that vmas list is
+ * sorted in ascending order of handle offsets.
+ * NOTE: This function should be called while holding handle's lock mutex.
+ */
+static void nvmap_get_client_handle_mss(struct nvmap_client *client,
+				struct nvmap_handle *handle, u64 *total)
+{
+	struct nvmap_vma_list *vma_list = NULL;
+	struct vm_area_struct *vma = NULL;
+	u64 end_offset = 0, vma_start_offset, vma_size;
+	int64_t overlap_size;
+
+	*total = 0;
+	list_for_each_entry(vma_list, &handle->vmas, list) {
+
+		if (client->task->pid == vma_list->pid) {
+			vma = vma_list->vma;
+			vma_size = vma->vm_end - vma->vm_start;
+
+			vma_start_offset = vma->vm_pgoff << PAGE_SHIFT;
+			if (end_offset < vma_start_offset + vma_size) {
+				*total += vma_size;
+
+				overlap_size = end_offset - vma_start_offset;
+				if (overlap_size > 0)
+					*total -= overlap_size;
+				end_offset = vma_start_offset + vma_size;
+			}
+		}
+	}
+}
+
+static void maps_stringify(struct nvmap_client *client,
+				struct seq_file *s, u32 heap_type)
+{
+	struct rb_node *n;
+	struct nvmap_vma_list *vma_list = NULL;
+	struct vm_area_struct *vma = NULL;
+	u64 total_mapped_size, vma_size;
+
+	nvmap_ref_lock(client);
+	n = rb_first(&client->handle_refs);
+	for (; n != NULL; n = rb_next(n)) {
+		struct nvmap_handle_ref *ref =
+			rb_entry(n, struct nvmap_handle_ref, node);
+		struct nvmap_handle *handle = ref->handle;
+		if (handle->alloc && handle->heap_type == heap_type) {
+			phys_addr_t base = heap_type == NVMAP_HEAP_IOVMM ? 0 :
+					   handle->heap_pgalloc ? 0 :
+					   (handle->carveout->base);
+			size_t size = K(handle->size);
+			int i = 0;
+
+next_page:
+			if ((heap_type == NVMAP_HEAP_CARVEOUT_VPR) && handle->heap_pgalloc) {
+				base = page_to_phys(handle->pgalloc.pages[i++]);
+				size = K(PAGE_SIZE);
+			}
+
+			seq_printf(s,
+				"%-18s %-18s %8llx %10zuK %8x %6u %16pK "
+				"%12s %12s ",
+				"", "",
+				(unsigned long long)base, K(handle->size),
+				handle->userflags,
+				atomic_read(&handle->share_count),
+				handle, "", "");
+
+			if ((heap_type == NVMAP_HEAP_CARVEOUT_VPR) && handle->heap_pgalloc) {
+				i++;
+				if (i < (handle->size >> PAGE_SHIFT))
+					goto next_page;
+			}
+
+			mutex_lock(&handle->lock);
+			nvmap_get_client_handle_mss(client, handle,
+							&total_mapped_size);
+			seq_printf(s, "%6lluK\n", K(total_mapped_size));
+
+			list_for_each_entry(vma_list, &handle->vmas, list) {
+
+				if (vma_list->pid == client->task->pid) {
+					vma = vma_list->vma;
+					vma_size = vma->vm_end - vma->vm_start;
+					seq_printf(s,
+					  "%-18s %-18s %8s %11s %8s %6s %16s "
+					  "%-12lx-%12lx %6lluK\n",
+					  "", "", "", "", "", "", "",
+					  vma->vm_start, vma->vm_end,
+					  K(vma_size));
+				}
+			}
+			mutex_unlock(&handle->lock);
+		}
+	}
+	nvmap_ref_unlock(client);
+}
+
+static void nvmap_get_client_mss(struct nvmap_client *client,
+				 u64 *total, u32 heap_type)
+{
+	struct rb_node *n;
+
+	*total = 0;
+	nvmap_ref_lock(client);
+	n = rb_first(&client->handle_refs);
+	for (; n != NULL; n = rb_next(n)) {
+		struct nvmap_handle_ref *ref =
+			rb_entry(n, struct nvmap_handle_ref, node);
+		struct nvmap_handle *handle = ref->handle;
+		if (handle->alloc && handle->heap_type == heap_type)
+			*total += handle->size /
+				  atomic_read(&handle->share_count);
+	}
+	nvmap_ref_unlock(client);
+}
+
+#define PSS_SHIFT 12
+static void nvmap_get_total_mss(u64 *pss, u64 *total, u32 heap_type)
+{
+	int i;
+	struct rb_node *n;
+	struct nvmap_device *dev = nvmap_dev;
+
+	*total = 0;
+	if (pss)
+		*pss = 0;
+	if (!dev)
+		return;
+	spin_lock(&dev->handle_lock);
+	n = rb_first(&dev->handles);
+	for (; n != NULL; n = rb_next(n)) {
+		struct nvmap_handle *h =
+			rb_entry(n, struct nvmap_handle, node);
+
+		if (!h || !h->alloc || h->heap_type != heap_type)
+			continue;
+
+		*total += h->size;
+		if (!pss)
+			continue;
+
+		for (i = 0; i < h->size >> PAGE_SHIFT; i++) {
+			struct page *page = nvmap_to_page(h->pgalloc.pages[i]);
+
+			if (page_mapcount(page) > 0)
+				*pss += PAGE_SIZE;
+		}
+	}
+	spin_unlock(&dev->handle_lock);
+}
+
+static int nvmap_debug_allocations_show(struct seq_file *s, void *unused)
+{
+	u64 total;
+	struct nvmap_client *client;
+	u32 heap_type = (u32)(uintptr_t)s->private;
+
+	mutex_lock(&nvmap_dev->clients_lock);
+	seq_printf(s, "%-18s %18s %8s %11s\n",
+		"CLIENT", "PROCESS", "PID", "SIZE");
+	seq_printf(s, "%-18s %18s %8s %11s %8s %6s %6s %6s %6s %6s %6s %8s\n",
+			"", "", "BASE", "SIZE", "FLAGS", "REFS",
+			"DUPES", "PINS", "KMAPS", "UMAPS", "SHARE", "UID");
+	list_for_each_entry(client, &nvmap_dev->clients, list) {
+		u64 client_total;
+		client_stringify(client, s);
+		nvmap_get_client_mss(client, &client_total, heap_type);
+		seq_printf(s, " %10lluK\n", K(client_total));
+		allocations_stringify(client, s, heap_type);
+		seq_printf(s, "\n");
+	}
+	mutex_unlock(&nvmap_dev->clients_lock);
+	nvmap_get_total_mss(NULL, &total, heap_type);
+	seq_printf(s, "%-18s %-18s %8s %10lluK\n", "total", "", "", K(total));
+	return 0;
+}
+
+DEBUGFS_OPEN_FOPS(allocations);
+
+static int nvmap_debug_all_allocations_show(struct seq_file *s, void *unused)
+{
+	u32 heap_type = (u32)(uintptr_t)s->private;
+	struct rb_node *n;
+
+
+	spin_lock(&nvmap_dev->handle_lock);
+	seq_printf(s, "%8s %11s %9s %6s %6s %6s %6s %8s\n",
+			"BASE", "SIZE", "USERFLAGS", "REFS",
+			"KMAPS", "UMAPS", "SHARE", "UID");
+
+	/* for each handle */
+	n = rb_first(&nvmap_dev->handles);
+	for (; n != NULL; n = rb_next(n)) {
+		struct nvmap_handle *handle =
+			rb_entry(n, struct nvmap_handle, node);
+		int i = 0;
+
+		if (handle->alloc && handle->heap_type == heap_type) {
+			phys_addr_t base = heap_type == NVMAP_HEAP_IOVMM ? 0 :
+					   handle->heap_pgalloc ? 0 :
+					   (handle->carveout->base);
+			size_t size = K(handle->size);
+
+next_page:
+			if ((heap_type == NVMAP_HEAP_CARVEOUT_VPR) && handle->heap_pgalloc) {
+				base = page_to_phys(handle->pgalloc.pages[i++]);
+				size = K(PAGE_SIZE);
+			}
+
+			seq_printf(s,
+				"%8llx %10zuK %9x %6u %6u %6u %6u %8p\n",
+				(unsigned long long)base, K(handle->size),
+				handle->userflags,
+				atomic_read(&handle->ref),
+				atomic_read(&handle->kmap_count),
+				atomic_read(&handle->umap_count),
+				atomic_read(&handle->share_count),
+				handle);
+
+			if ((heap_type == NVMAP_HEAP_CARVEOUT_VPR) && handle->heap_pgalloc) {
+				i++;
+				if (i < (handle->size >> PAGE_SHIFT))
+					goto next_page;
+			}
+		}
+	}
+
+	spin_unlock(&nvmap_dev->handle_lock);
+
+	return 0;
+}
+
+DEBUGFS_OPEN_FOPS(all_allocations);
+
+static int nvmap_debug_orphan_handles_show(struct seq_file *s, void *unused)
+{
+	u32 heap_type = (u32)(uintptr_t)s->private;
+	struct rb_node *n;
+
+
+	spin_lock(&nvmap_dev->handle_lock);
+	seq_printf(s, "%8s %11s %9s %6s %6s %6s %8s\n",
+			"BASE", "SIZE", "USERFLAGS", "REFS",
+			"KMAPS", "UMAPS", "UID");
+
+	/* for each handle */
+	n = rb_first(&nvmap_dev->handles);
+	for (; n != NULL; n = rb_next(n)) {
+		struct nvmap_handle *handle =
+			rb_entry(n, struct nvmap_handle, node);
+		int i = 0;
+
+		if (handle->alloc && handle->heap_type == heap_type &&
+			!atomic_read(&handle->share_count)) {
+			phys_addr_t base = heap_type == NVMAP_HEAP_IOVMM ? 0 :
+					   handle->heap_pgalloc ? 0 :
+					   (handle->carveout->base);
+			size_t size = K(handle->size);
+
+next_page:
+			if ((heap_type == NVMAP_HEAP_CARVEOUT_VPR) && handle->heap_pgalloc) {
+				base = page_to_phys(handle->pgalloc.pages[i++]);
+				size = K(PAGE_SIZE);
+			}
+
+			seq_printf(s,
+				"%8llx %10zuK %9x %6u %6u %6u %8p\n",
+				(unsigned long long)base, K(handle->size),
+				handle->userflags,
+				atomic_read(&handle->ref),
+				atomic_read(&handle->kmap_count),
+				atomic_read(&handle->umap_count),
+				handle);
+
+			if ((heap_type == NVMAP_HEAP_CARVEOUT_VPR) && handle->heap_pgalloc) {
+				i++;
+				if (i < (handle->size >> PAGE_SHIFT))
+					goto next_page;
+			}
+		}
+	}
+
+	spin_unlock(&nvmap_dev->handle_lock);
+
+	return 0;
+}
+
+DEBUGFS_OPEN_FOPS(orphan_handles);
+
+static int nvmap_debug_maps_show(struct seq_file *s, void *unused)
+{
+	u64 total;
+	struct nvmap_client *client;
+	u32 heap_type = (u32)(uintptr_t)s->private;
+
+	mutex_lock(&nvmap_dev->clients_lock);
+	seq_printf(s, "%-18s %18s %8s %11s\n",
+		"CLIENT", "PROCESS", "PID", "SIZE");
+	seq_printf(s, "%-18s %18s %8s %11s %8s %6s %9s %21s %18s\n",
+		"", "", "BASE", "SIZE", "FLAGS", "SHARE", "UID",
+		"MAPS", "MAPSIZE");
+
+	list_for_each_entry(client, &nvmap_dev->clients, list) {
+		u64 client_total;
+		client_stringify(client, s);
+		nvmap_get_client_mss(client, &client_total, heap_type);
+		seq_printf(s, " %10lluK\n", K(client_total));
+		maps_stringify(client, s, heap_type);
+		seq_printf(s, "\n");
+	}
+	mutex_unlock(&nvmap_dev->clients_lock);
+
+	nvmap_get_total_mss(NULL, &total, heap_type);
+	seq_printf(s, "%-18s %-18s %8s %10lluK\n", "total", "", "", K(total));
+	return 0;
+}
+
+DEBUGFS_OPEN_FOPS(maps);
+
+static int nvmap_debug_clients_show(struct seq_file *s, void *unused)
+{
+	u64 total;
+	struct nvmap_client *client;
+	ulong heap_type = (ulong)s->private;
+
+	mutex_lock(&nvmap_dev->clients_lock);
+	seq_printf(s, "%-18s %18s %8s %11s\n",
+		"CLIENT", "PROCESS", "PID", "SIZE");
+	list_for_each_entry(client, &nvmap_dev->clients, list) {
+		u64 client_total;
+		client_stringify(client, s);
+		nvmap_get_client_mss(client, &client_total, heap_type);
+		seq_printf(s, " %10lluK\n", K(client_total));
+	}
+	mutex_unlock(&nvmap_dev->clients_lock);
+	nvmap_get_total_mss(NULL, &total, heap_type);
+	seq_printf(s, "%-18s %18s %8s %10lluK\n", "total", "", "", K(total));
+	return 0;
+}
+
+DEBUGFS_OPEN_FOPS(clients);
+
+static int nvmap_debug_handles_by_pid_show_client(struct seq_file *s,
+		struct nvmap_client *client)
+{
+	struct rb_node *n;
+	int ret = 0;
+
+	nvmap_ref_lock(client);
+	n = rb_first(&client->handle_refs);
+	for (; n != NULL; n = rb_next(n)) {
+		struct nvmap_handle_ref *ref = rb_entry(n,
+				struct nvmap_handle_ref, node);
+		struct nvmap_handle *handle = ref->handle;
+		struct nvmap_debugfs_handles_entry entry;
+		u64 total_mapped_size;
+		int i = 0;
+
+		if (!handle->alloc)
+			continue;
+
+		mutex_lock(&handle->lock);
+		nvmap_get_client_handle_mss(client, handle, &total_mapped_size);
+		mutex_unlock(&handle->lock);
+
+		entry.base = handle->heap_type == NVMAP_HEAP_IOVMM ? 0 :
+			     handle->heap_pgalloc ? 0 :
+			     (handle->carveout->base);
+		entry.size = handle->size;
+		entry.flags = handle->userflags;
+		entry.share_count = atomic_read(&handle->share_count);
+		entry.mapped_size = total_mapped_size;
+
+next_page:
+		if ((handle->heap_type == NVMAP_HEAP_CARVEOUT_VPR) && handle->heap_pgalloc) {
+			entry.base = page_to_phys(handle->pgalloc.pages[i++]);
+			entry.size = K(PAGE_SIZE);
+		}
+
+		ret = seq_write(s, &entry, sizeof(entry));
+		if (ret < 0)
+			break;
+
+		if ((handle->heap_type == NVMAP_HEAP_CARVEOUT_VPR) && handle->heap_pgalloc) {
+			i++;
+			if (i < (handle->size >> PAGE_SHIFT))
+				goto next_page;
+		}
+	}
+	nvmap_ref_unlock(client);
+
+	return ret;
+}
+
+static int nvmap_debug_handles_by_pid_show(struct seq_file *s, void *unused)
+{
+	struct nvmap_pid_data *p = s->private;
+	struct nvmap_client *client;
+	struct nvmap_debugfs_handles_header header;
+	int ret;
+
+	header.version = 1;
+	ret = seq_write(s, &header, sizeof(header));
+	if (ret < 0)
+		return ret;
+
+	mutex_lock(&nvmap_dev->clients_lock);
+
+	list_for_each_entry(client, &nvmap_dev->clients, list) {
+		if (nvmap_client_pid(client) != p->pid)
+			continue;
+
+		ret = nvmap_debug_handles_by_pid_show_client(s, client);
+		if (ret < 0)
+			break;
+	}
+
+	mutex_unlock(&nvmap_dev->clients_lock);
+	return ret;
+}
+
+DEBUGFS_OPEN_FOPS_STATIC(handles_by_pid);
+
+#define PRINT_MEM_STATS_NOTE(x) \
+do { \
+	seq_printf(s, "Note: total memory is precise account of pages " \
+		"allocated by NvMap.\nIt doesn't match with all clients " \
+		"\"%s\" accumulated as shared memory \nis accounted in " \
+		"full in each clients \"%s\" that shared memory.\n", #x, #x); \
+} while (0)
+
+static int nvmap_debug_lru_allocations_show(struct seq_file *s, void *unused)
+{
+	struct nvmap_handle *h;
+	int total_handles = 0, migratable_handles = 0;
+	size_t total_size = 0, migratable_size = 0;
+
+	seq_printf(s, "%-18s %18s %8s %11s %8s %6s %6s %6s %6s %6s %8s\n",
+			"", "", "", "", "", "",
+			"", "PINS", "KMAPS", "UMAPS", "UID");
+	spin_lock(&nvmap_dev->lru_lock);
+	list_for_each_entry(h, &nvmap_dev->lru_handles, lru) {
+		total_handles++;
+		total_size += h->size;
+		if (!atomic_read(&h->pin) && !atomic_read(&h->kmap_count)) {
+			migratable_handles++;
+			migratable_size += h->size;
+		}
+		seq_printf(s, "%-18s %18s %8s %10zuK %8s %6s %6s %6u %6u "
+			"%6u %8p\n", "", "", "", K(h->size), "", "",
+			"", atomic_read(&h->pin),
+			    atomic_read(&h->kmap_count),
+			    atomic_read(&h->umap_count),
+			    h);
+	}
+	seq_printf(s, "total_handles = %d, migratable_handles = %d,"
+		"total_size=%zuK, migratable_size=%zuK\n",
+		total_handles, migratable_handles,
+		K(total_size), K(migratable_size));
+	spin_unlock(&nvmap_dev->lru_lock);
+	PRINT_MEM_STATS_NOTE(SIZE);
+	return 0;
+}
+
+DEBUGFS_OPEN_FOPS(lru_allocations);
+
+struct procrank_stats {
+	struct vm_area_struct *vma;
+	u64 pss;
+};
+
+static int procrank_pte_entry(pte_t *pte, unsigned long addr, unsigned long end,
+		struct mm_walk *walk)
+{
+	struct procrank_stats *mss = walk->private;
+	struct vm_area_struct *vma = mss->vma;
+	struct page *page = NULL;
+	int mapcount;
+
+	if (pte_present(*pte))
+		page = vm_normal_page(vma, addr, *pte);
+	else if (is_swap_pte(*pte)) {
+		swp_entry_t swpent = pte_to_swp_entry(*pte);
+
+		if (is_migration_entry(swpent))
+			page = migration_entry_to_page(swpent);
+	}
+
+	if (!page)
+		return 0;
+
+	mapcount = page_mapcount(page);
+	if (mapcount >= 2)
+		mss->pss += (PAGE_SIZE << PSS_SHIFT) / mapcount;
+	else
+		mss->pss += (PAGE_SIZE << PSS_SHIFT);
+
+	return 0;
+}
+
+#ifndef PTRACE_MODE_READ_FSCREDS
+#define PTRACE_MODE_READ_FSCREDS PTRACE_MODE_READ
+#endif
+
+static void nvmap_iovmm_get_client_mss(struct nvmap_client *client, u64 *pss,
+				   u64 *total)
+{
+	struct rb_node *n;
+	struct nvmap_vma_list *tmp;
+	struct procrank_stats mss;
+	struct mm_walk procrank_walk = {
+		.pte_entry = procrank_pte_entry,
+		.private = &mss,
+	};
+	struct mm_struct *mm;
+
+	memset(&mss, 0, sizeof(mss));
+	*pss = *total = 0;
+
+	mm = mm_access(client->task,
+			PTRACE_MODE_READ_FSCREDS);
+	if (!mm || IS_ERR(mm)) return;
+
+	down_read(&mm->mmap_sem);
+	procrank_walk.mm = mm;
+
+	nvmap_ref_lock(client);
+	n = rb_first(&client->handle_refs);
+	for (; n != NULL; n = rb_next(n)) {
+		struct nvmap_handle_ref *ref =
+			rb_entry(n, struct nvmap_handle_ref, node);
+		struct nvmap_handle *h = ref->handle;
+
+		if (!h || !h->alloc || !h->heap_pgalloc)
+			continue;
+
+		mutex_lock(&h->lock);
+		list_for_each_entry(tmp, &h->vmas, list) {
+			if (client->task->pid == tmp->pid) {
+				mss.vma = tmp->vma;
+				walk_page_range(tmp->vma->vm_start,
+						tmp->vma->vm_end,
+						&procrank_walk);
+			}
+		}
+		mutex_unlock(&h->lock);
+		*total += h->size / atomic_read(&h->share_count);
+	}
+
+	up_read(&mm->mmap_sem);
+	mmput(mm);
+	*pss = (mss.pss >> PSS_SHIFT);
+	nvmap_ref_unlock(client);
+}
+
+static int nvmap_debug_iovmm_procrank_show(struct seq_file *s, void *unused)
+{
+	u64 pss, total;
+	struct nvmap_client *client;
+	struct nvmap_device *dev = s->private;
+	u64 total_memory, total_pss;
+
+	mutex_lock(&dev->clients_lock);
+	seq_printf(s, "%-18s %18s %8s %11s %11s\n",
+		"CLIENT", "PROCESS", "PID", "PSS", "SIZE");
+	list_for_each_entry(client, &dev->clients, list) {
+		client_stringify(client, s);
+		nvmap_iovmm_get_client_mss(client, &pss, &total);
+		seq_printf(s, " %10lluK %10lluK\n", K(pss), K(total));
+	}
+	mutex_unlock(&dev->clients_lock);
+
+	nvmap_get_total_mss(&total_pss, &total_memory, NVMAP_HEAP_IOVMM);
+	seq_printf(s, "%-18s %18s %8s %10lluK %10lluK\n",
+		"total", "", "", K(total_pss), K(total_memory));
+	return 0;
+}
+
+DEBUGFS_OPEN_FOPS(iovmm_procrank);
+
+ulong nvmap_iovmm_get_used_pages(void)
+{
+	u64 total;
+
+	nvmap_get_total_mss(NULL, &total, NVMAP_HEAP_IOVMM);
+	return total >> PAGE_SHIFT;
+}
+
+static void nvmap_iovmm_debugfs_init(void)
+{
+	if (!IS_ERR_OR_NULL(nvmap_dev->debug_root)) {
+		struct dentry *iovmm_root =
+			debugfs_create_dir("iovmm", nvmap_dev->debug_root);
+		if (!IS_ERR_OR_NULL(iovmm_root)) {
+			debugfs_create_file("clients", S_IRUGO, iovmm_root,
+				(void *)(uintptr_t)NVMAP_HEAP_IOVMM,
+				&debug_clients_fops);
+			debugfs_create_file("allocations", S_IRUGO, iovmm_root,
+				(void *)(uintptr_t)NVMAP_HEAP_IOVMM,
+				&debug_allocations_fops);
+			debugfs_create_file("all_allocations", S_IRUGO,
+				iovmm_root, (void *)(uintptr_t)NVMAP_HEAP_IOVMM,
+				&debug_all_allocations_fops);
+			debugfs_create_file("orphan_handles", S_IRUGO,
+				iovmm_root, (void *)(uintptr_t)NVMAP_HEAP_IOVMM,
+				&debug_orphan_handles_fops);
+			debugfs_create_file("maps", S_IRUGO, iovmm_root,
+				(void *)(uintptr_t)NVMAP_HEAP_IOVMM,
+				&debug_maps_fops);
+			debugfs_create_file("procrank", S_IRUGO, iovmm_root,
+				nvmap_dev, &debug_iovmm_procrank_fops);
+		}
+	}
+}
+
+int __init nvmap_probe(struct platform_device *pdev)
+{
+	struct nvmap_platform_data *plat;
+	struct nvmap_device *dev;
+	struct dentry *nvmap_debug_root;
+	unsigned int i;
+	int e;
+	int generic_carveout_present = 0;
+	ulong start_time = sched_clock();
+
+	if (WARN_ON(nvmap_dev != NULL)) {
+		dev_err(&pdev->dev, "only one nvmap device may be present\n");
+		e = -ENODEV;
+		goto finish;
+	}
+
+	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
+	if (!dev) {
+		dev_err(&pdev->dev, "out of memory for device\n");
+		e = -ENOMEM;
+		goto finish;
+	}
+
+	nvmap_init(pdev);
+	plat = pdev->dev.platform_data;
+	if (!plat) {
+		dev_err(&pdev->dev, "no platform data?\n");
+		e = -ENODEV;
+		goto free_dev;
+	}
+
+	nvmap_dev = dev;
+	nvmap_dev->plat = plat;
+	/*
+	 * dma_parms need to be set with desired max_segment_size to avoid
+	 * DMA map API returning multiple IOVA's for the buffer size > 64KB.
+	 */
+	pdev->dev.dma_parms = &nvmap_dma_parameters;
+	dev->dev_user.minor = MISC_DYNAMIC_MINOR;
+	dev->dev_user.name = "nvmap";
+	dev->dev_user.fops = &nvmap_user_fops;
+	dev->dev_user.parent = &pdev->dev;
+	dev->handles = RB_ROOT;
+
+	if (of_property_read_bool(pdev->dev.of_node,
+				"no-cache-maint-by-set-ways"))
+		nvmap_cache_maint_by_set_ways = 0;
+
+	nvmap_override_cache_ops();
+#ifdef CONFIG_NVMAP_PAGE_POOLS
+	e = nvmap_page_pool_init(dev);
+	if (e)
+		goto fail;
+#endif
+
+	spin_lock_init(&dev->handle_lock);
+	INIT_LIST_HEAD(&dev->clients);
+	dev->pids = RB_ROOT;
+	mutex_init(&dev->clients_lock);
+	INIT_LIST_HEAD(&dev->lru_handles);
+	spin_lock_init(&dev->lru_lock);
+	dev->tags = RB_ROOT;
+	mutex_init(&dev->tags_lock);
+
+	e = misc_register(&dev->dev_user);
+	if (e) {
+		dev_err(&pdev->dev, "unable to register miscdevice %s\n",
+			dev->dev_user.name);
+		goto fail;
+	}
+
+	nvmap_debug_root = debugfs_create_dir("nvmap", NULL);
+	nvmap_dev->debug_root = nvmap_debug_root;
+	if (IS_ERR_OR_NULL(nvmap_debug_root))
+		dev_err(&pdev->dev, "couldn't create debug files\n");
+
+	debugfs_create_u32("max_handle_count", S_IRUGO,
+			nvmap_debug_root, &nvmap_max_handle_count);
+
+	nvmap_dev->dynamic_dma_map_mask = ~0;
+	nvmap_dev->cpu_access_mask = ~0;
+	for (i = 0; i < plat->nr_carveouts; i++)
+		(void)nvmap_create_carveout(&plat->carveouts[i]);
+
+	nvmap_iovmm_debugfs_init();
+#ifdef CONFIG_NVMAP_PAGE_POOLS
+	nvmap_page_pool_debugfs_init(nvmap_dev->debug_root);
+#endif
+	nvmap_cache_debugfs_init(nvmap_dev->debug_root);
+	nvmap_dev->handles_by_pid = debugfs_create_dir("handles_by_pid",
+							nvmap_debug_root);
+#if defined(CONFIG_DEBUG_FS)
+	debugfs_create_ulong("nvmap_init_time", S_IRUGO | S_IWUSR,
+				nvmap_dev->debug_root, &nvmap_init_time);
+#endif
+	nvmap_stats_init(nvmap_debug_root);
+	platform_set_drvdata(pdev, dev);
+
+	e = nvmap_dmabuf_stash_init();
+	if (e)
+		goto fail_heaps;
+
+	for (i = 0; i < dev->nr_carveouts; i++)
+		if (dev->heaps[i].heap_bit & NVMAP_HEAP_CARVEOUT_GENERIC)
+			generic_carveout_present = 1;
+
+	if (generic_carveout_present) {
+		if (!iommu_present(&platform_bus_type))
+			nvmap_convert_iovmm_to_carveout = 1;
+		else if (!of_property_read_bool(pdev->dev.of_node,
+				"dont-convert-iovmm-to-carveout"))
+			nvmap_convert_iovmm_to_carveout = 1;
+	} else {
+		BUG_ON(!iommu_present(&platform_bus_type));
+		nvmap_convert_carveout_to_iovmm = 1;
+	}
+
+#ifdef CONFIG_NVMAP_PAGE_POOLS
+	if (nvmap_convert_iovmm_to_carveout)
+		nvmap_page_pool_fini(dev);
+#endif
+
+	goto finish;
+fail_heaps:
+	for (i = 0; i < dev->nr_carveouts; i++) {
+		struct nvmap_carveout_node *node = &dev->heaps[i];
+		nvmap_heap_destroy(node->carveout);
+	}
+fail:
+#ifdef CONFIG_NVMAP_PAGE_POOLS
+	nvmap_page_pool_fini(nvmap_dev);
+#endif
+	kfree(dev->heaps);
+	if (dev->dev_user.minor != MISC_DYNAMIC_MINOR)
+		misc_deregister(&dev->dev_user);
+	nvmap_dev = NULL;
+free_dev:
+	kfree(dev);
+finish:
+	nvmap_init_time += sched_clock() - start_time;
+	return e;
+}
+
+int nvmap_remove(struct platform_device *pdev)
+{
+	struct nvmap_device *dev = platform_get_drvdata(pdev);
+	struct rb_node *n;
+	struct nvmap_handle *h;
+	int i;
+
+	misc_deregister(&dev->dev_user);
+
+	while ((n = rb_first(&dev->handles))) {
+		h = rb_entry(n, struct nvmap_handle, node);
+		rb_erase(&h->node, &dev->handles);
+		kfree(h);
+	}
+
+	for (i = 0; i < dev->nr_carveouts; i++) {
+		struct nvmap_carveout_node *node = &dev->heaps[i];
+		nvmap_heap_destroy(node->carveout);
+	}
+	kfree(dev->heaps);
+
+	kfree(dev);
+	nvmap_dev = NULL;
+	return 0;
+}
diff --git a/drivers/video/tegra/nvmap/nvmap_dmabuf.c b/drivers/video/tegra/nvmap/nvmap_dmabuf.c
new file mode 100644
index 000000000000..951191bd85fc
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nvmap_dmabuf.c
@@ -0,0 +1,794 @@
+/*
+ * dma_buf exporter for nvmap
+ *
+ * Copyright (c) 2012-2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"nvmap: %s() " fmt, __func__
+
+#include <linux/list.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/export.h>
+#include <linux/nvmap.h>
+#include <linux/dma-buf.h>
+#include <linux/spinlock.h>
+#include <linux/mutex.h>
+#include <linux/atomic.h>
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+#include <linux/stringify.h>
+#include <linux/of.h>
+#include <linux/platform/tegra/tegra_fd.h>
+#include <linux/version.h>
+#include <linux/iommu.h>
+
+#include <trace/events/nvmap.h>
+
+#include "nvmap_priv.h"
+#include "nvmap_ioctl.h"
+
+/**
+ * List node for maps of nvmap handles via the dma_buf API. These store the
+ * necessary info for stashing mappings.
+ *
+ * @iommu_domain Domain for which this SGT is valid - for supporting multi-asid.
+ * @dir DMA direction.
+ * @sgt The scatter gather table to stash.
+ * @refs Reference counting.
+ * @maps_entry Entry on a given attachment's list of maps.
+ * @stash_entry Entry on the stash list.
+ * @owner The owner of this struct. There can be only one.
+ */
+struct nvmap_handle_sgt {
+	struct iommu_domain *domain;
+	enum dma_data_direction dir;
+	struct sg_table *sgt;
+	struct device *dev;
+
+	atomic_t refs;
+
+	struct list_head maps_entry;
+	struct list_head stash_entry; /* lock the stash before accessing. */
+
+	struct nvmap_handle_info *owner;
+} ____cacheline_aligned_in_smp;
+
+static DEFINE_MUTEX(nvmap_stashed_maps_lock);
+static LIST_HEAD(nvmap_stashed_maps);
+static struct kmem_cache *handle_sgt_cache;
+static struct dma_buf_ops nvmap_dma_buf_ops;
+
+static bool nvmap_attach_handle_same_asid(struct dma_buf_attachment *attach,
+					struct nvmap_handle_sgt *nvmap_sgt)
+{
+	return iommu_get_domain_for_dev(attach->dev) == nvmap_sgt->domain;
+
+}
+
+/*
+ * Initialize a kmem cache for allocating nvmap_handle_sgt's.
+ */
+int nvmap_dmabuf_stash_init(void)
+{
+	handle_sgt_cache = KMEM_CACHE(nvmap_handle_sgt, 0);
+	if (IS_ERR_OR_NULL(handle_sgt_cache)) {
+		pr_err("Failed to make kmem cache for nvmap_handle_sgt.\n");
+		return -ENOMEM;
+	}
+
+	return 0;
+}
+
+static int nvmap_dmabuf_attach(struct dma_buf *dmabuf, struct device *dev,
+			       struct dma_buf_attachment *attach)
+{
+	struct nvmap_handle_info *info = dmabuf->priv;
+
+	trace_nvmap_dmabuf_attach(dmabuf, dev);
+
+	dev_dbg(dev, "%s() 0x%p\n", __func__, info->handle);
+	return 0;
+}
+
+static void nvmap_dmabuf_detach(struct dma_buf *dmabuf,
+				struct dma_buf_attachment *attach)
+{
+	struct nvmap_handle_info *info = dmabuf->priv;
+
+	trace_nvmap_dmabuf_detach(dmabuf, attach->dev);
+
+	dev_dbg(attach->dev, "%s() 0x%p\n", __func__, info->handle);
+}
+
+/*
+ * Make sure this mapping is no longer stashed - this corresponds to a "hit". If
+ * the mapping is not stashed this is just a no-op.
+ */
+static void __nvmap_dmabuf_del_stash(struct nvmap_handle_sgt *nvmap_sgt)
+{
+	mutex_lock(&nvmap_stashed_maps_lock);
+	if (list_empty(&nvmap_sgt->stash_entry)) {
+		mutex_unlock(&nvmap_stashed_maps_lock);
+		return;
+	}
+
+	pr_debug("Removing map from stash.\n");
+	list_del_init(&nvmap_sgt->stash_entry);
+	mutex_unlock(&nvmap_stashed_maps_lock);
+}
+
+static inline bool access_vpr_phys(struct device *dev)
+{
+	if (!device_is_iommuable(dev))
+		return true;
+
+	/*
+	 * Assumes gpu nodes always have DT entry, this is valid as device
+	 * specifying access-vpr-phys will do so through its DT entry.
+	 */
+	if (!dev->of_node)
+		return false;
+
+	return !!of_find_property(dev->of_node, "access-vpr-phys", NULL);
+}
+
+/*
+ * Free an sgt completely. This will bypass the ref count. This also requires
+ * the nvmap_sgt's owner's lock is already taken.
+ */
+static void __nvmap_dmabuf_free_sgt_locked(struct nvmap_handle_sgt *nvmap_sgt)
+{
+	struct nvmap_handle_info *info = nvmap_sgt->owner;
+	DEFINE_DMA_ATTRS(attrs);
+
+	list_del(&nvmap_sgt->maps_entry);
+
+	if (!(nvmap_dev->dynamic_dma_map_mask & info->handle->heap_type)) {
+		sg_dma_address(nvmap_sgt->sgt->sgl) = 0;
+	} else if (info->handle->heap_type == NVMAP_HEAP_CARVEOUT_VPR &&
+			access_vpr_phys(nvmap_sgt->dev)) {
+		sg_dma_address(nvmap_sgt->sgt->sgl) = 0;
+	} else {
+		dma_set_attr(DMA_ATTR_SKIP_IOVA_GAP, __DMA_ATTR(attrs));
+		dma_set_attr(DMA_ATTR_SKIP_CPU_SYNC, __DMA_ATTR(attrs));
+		dma_unmap_sg_attrs(nvmap_sgt->dev,
+				   nvmap_sgt->sgt->sgl, nvmap_sgt->sgt->nents,
+				   nvmap_sgt->dir, __DMA_ATTR(attrs));
+	}
+	__nvmap_free_sg_table(NULL, info->handle, nvmap_sgt->sgt);
+
+	WARN(atomic_read(&nvmap_sgt->refs), "nvmap: Freeing reffed SGT!");
+	kmem_cache_free(handle_sgt_cache, nvmap_sgt);
+}
+
+/*
+ * Evict an entry from the IOVA stash. This does not do anything to the actual
+ * mapping itself - this merely takes the passed nvmap_sgt out of the stash
+ * and decrements the necessary cache stats.
+ */
+static void __nvmap_dmabuf_evict_stash_locked(
+			struct nvmap_handle_sgt *nvmap_sgt)
+{
+	if (!list_empty(&nvmap_sgt->stash_entry))
+		list_del_init(&nvmap_sgt->stash_entry);
+}
+
+/*
+ * Locks the stash before doing the eviction.
+ */
+static void __nvmap_dmabuf_evict_stash(struct nvmap_handle_sgt *nvmap_sgt)
+{
+	mutex_lock(&nvmap_stashed_maps_lock);
+	__nvmap_dmabuf_evict_stash_locked(nvmap_sgt);
+	mutex_unlock(&nvmap_stashed_maps_lock);
+}
+
+/*
+ * Prepare an SGT for potential stashing later on.
+ */
+static int __nvmap_dmabuf_prep_sgt_locked(struct dma_buf_attachment *attach,
+				   enum dma_data_direction dir,
+				   struct sg_table *sgt)
+{
+	struct nvmap_handle_sgt *nvmap_sgt;
+	struct nvmap_handle_info *info = attach->dmabuf->priv;
+
+	pr_debug("Prepping SGT.\n");
+	nvmap_sgt = kmem_cache_alloc(handle_sgt_cache, GFP_KERNEL);
+	if (IS_ERR_OR_NULL(nvmap_sgt)) {
+		pr_err("Prepping SGT failed.\n");
+		return -ENOMEM;
+	}
+
+	nvmap_sgt->domain = iommu_get_domain_for_dev(attach->dev);
+	nvmap_sgt->dir = dir;
+	nvmap_sgt->sgt = sgt;
+	nvmap_sgt->dev = attach->dev;
+	nvmap_sgt->owner = info;
+	INIT_LIST_HEAD(&nvmap_sgt->stash_entry);
+	atomic_set(&nvmap_sgt->refs, 1);
+	list_add(&nvmap_sgt->maps_entry, &info->maps);
+	return 0;
+}
+
+/*
+ * Called when an SGT is no longer being used by a device. This will not
+ * necessarily free the SGT - instead it may stash the SGT.
+ */
+static void __nvmap_dmabuf_stash_sgt_locked(struct dma_buf_attachment *attach,
+				    enum dma_data_direction dir,
+				    struct sg_table *sgt)
+{
+	struct nvmap_handle_sgt *nvmap_sgt;
+	struct nvmap_handle_info *info = attach->dmabuf->priv;
+
+	pr_debug("Stashing SGT - if necessary.\n");
+	list_for_each_entry(nvmap_sgt, &info->maps, maps_entry) {
+		if (nvmap_sgt->sgt == sgt) {
+			if (!atomic_sub_and_test(1, &nvmap_sgt->refs))
+				goto done;
+
+			__nvmap_dmabuf_free_sgt_locked(nvmap_sgt);
+			goto done;
+		}
+	}
+
+done:
+	return;
+}
+
+/*
+ * Checks if there is already a map for this attachment. If so increment the
+ * ref count on said map and return the associated sg_table. Otherwise return
+ * NULL.
+ *
+ * If it turns out there is a map, this also checks to see if the map needs to
+ * be removed from the stash - if so, the map is removed.
+ */
+static struct sg_table *__nvmap_dmabuf_get_sgt_locked(
+	struct dma_buf_attachment *attach, enum dma_data_direction dir)
+{
+	struct nvmap_handle_sgt *nvmap_sgt;
+	struct sg_table *sgt = NULL;
+	struct nvmap_handle_info *info = attach->dmabuf->priv;
+
+	pr_debug("Getting SGT from stash.\n");
+	list_for_each_entry(nvmap_sgt, &info->maps, maps_entry) {
+		if (!nvmap_attach_handle_same_asid(attach, nvmap_sgt))
+			continue;
+
+		/* We have a hit. */
+		pr_debug("Stash hit (%s)!\n", dev_name(attach->dev));
+		sgt = nvmap_sgt->sgt;
+		atomic_inc(&nvmap_sgt->refs);
+		__nvmap_dmabuf_del_stash(nvmap_sgt);
+		break;
+	}
+
+	return sgt;
+}
+
+/*
+ * If stashing is disabled then the stash related ops become no-ops.
+ */
+struct sg_table *_nvmap_dmabuf_map_dma_buf(
+	struct dma_buf_attachment *attach, enum dma_data_direction dir)
+{
+	struct nvmap_handle_info *info = attach->dmabuf->priv;
+	int ents = 0;
+	struct sg_table *sgt;
+	DEFINE_DMA_ATTRS(attrs);
+
+	trace_nvmap_dmabuf_map_dma_buf(attach->dmabuf, attach->dev);
+
+	nvmap_lru_reset(info->handle);
+	mutex_lock(&info->maps_lock);
+
+	atomic_inc(&info->handle->pin);
+
+	sgt = __nvmap_dmabuf_get_sgt_locked(attach, dir);
+	if (sgt)
+		goto cache_hit;
+
+	sgt = __nvmap_sg_table(NULL, info->handle);
+	if (IS_ERR(sgt)) {
+		atomic_dec(&info->handle->pin);
+		mutex_unlock(&info->maps_lock);
+		return sgt;
+	}
+
+	if (!info->handle->alloc) {
+		goto err_map;
+	} else if (!(nvmap_dev->dynamic_dma_map_mask &
+			info->handle->heap_type)) {
+		sg_dma_address(sgt->sgl) = info->handle->carveout->base;
+	} else if (info->handle->heap_type == NVMAP_HEAP_CARVEOUT_VPR &&
+			access_vpr_phys(attach->dev)) {
+		sg_dma_address(sgt->sgl) = 0;
+	} else {
+		dma_set_attr(DMA_ATTR_SKIP_IOVA_GAP, __DMA_ATTR(attrs));
+		dma_set_attr(DMA_ATTR_SKIP_CPU_SYNC, __DMA_ATTR(attrs));
+		ents = dma_map_sg_attrs(attach->dev, sgt->sgl,
+					sgt->nents, dir, __DMA_ATTR(attrs));
+		if (ents <= 0)
+			goto err_map;
+	}
+
+	if (__nvmap_dmabuf_prep_sgt_locked(attach, dir, sgt)) {
+		WARN(1, "No mem to prep sgt.\n");
+		goto err_prep;
+	}
+
+cache_hit:
+	attach->priv = sgt;
+	mutex_unlock(&info->maps_lock);
+	return sgt;
+
+err_prep:
+	dma_unmap_sg_attrs(attach->dev, sgt->sgl, sgt->nents, dir, __DMA_ATTR(attrs));
+err_map:
+	__nvmap_free_sg_table(NULL, info->handle, sgt);
+	atomic_dec(&info->handle->pin);
+	mutex_unlock(&info->maps_lock);
+	return ERR_PTR(-ENOMEM);
+}
+
+__weak struct sg_table *nvmap_dmabuf_map_dma_buf(
+	struct dma_buf_attachment *attach, enum dma_data_direction dir)
+{
+	return _nvmap_dmabuf_map_dma_buf(attach, dir);
+}
+
+void _nvmap_dmabuf_unmap_dma_buf(struct dma_buf_attachment *attach,
+				       struct sg_table *sgt,
+				       enum dma_data_direction dir)
+{
+	struct nvmap_handle_info *info = attach->dmabuf->priv;
+
+	trace_nvmap_dmabuf_unmap_dma_buf(attach->dmabuf, attach->dev);
+
+	mutex_lock(&info->maps_lock);
+	if (!atomic_add_unless(&info->handle->pin, -1, 0)) {
+		mutex_unlock(&info->maps_lock);
+		WARN(1, "Unpinning handle that has yet to be pinned!\n");
+		return;
+	}
+	__nvmap_dmabuf_stash_sgt_locked(attach, dir, sgt);
+	mutex_unlock(&info->maps_lock);
+}
+
+__weak void nvmap_dmabuf_unmap_dma_buf(struct dma_buf_attachment *attach,
+				       struct sg_table *sgt,
+				       enum dma_data_direction dir)
+{
+	_nvmap_dmabuf_unmap_dma_buf(attach, sgt, dir);
+}
+
+static void nvmap_dmabuf_release(struct dma_buf *dmabuf)
+{
+	struct nvmap_handle_info *info = dmabuf->priv;
+	struct nvmap_handle_sgt *nvmap_sgt;
+
+	trace_nvmap_dmabuf_release(info->handle->owner ?
+				   info->handle->owner->name : "unknown",
+				   info->handle,
+				   dmabuf);
+
+	mutex_lock(&info->handle->lock);
+	BUG_ON(dmabuf != info->handle->dmabuf);
+	info->handle->dmabuf = NULL;
+	mutex_unlock(&info->handle->lock);
+
+	mutex_lock(&info->maps_lock);
+	while (!list_empty(&info->maps)) {
+		nvmap_sgt = list_first_entry(&info->maps,
+					     struct nvmap_handle_sgt,
+					     maps_entry);
+		__nvmap_dmabuf_evict_stash(nvmap_sgt);
+		__nvmap_dmabuf_free_sgt_locked(nvmap_sgt);
+	}
+	mutex_unlock(&info->maps_lock);
+
+	nvmap_handle_put(info->handle);
+	kfree(info);
+}
+
+static int nvmap_dmabuf_begin_cpu_access(struct dma_buf *dmabuf,
+					  size_t start, size_t len,
+					  enum dma_data_direction dir)
+{
+	struct nvmap_handle_info *info = dmabuf->priv;
+
+	trace_nvmap_dmabuf_begin_cpu_access(dmabuf, start, len);
+	return __nvmap_do_cache_maint(NULL, info->handle, start, start + len,
+				      NVMAP_CACHE_OP_WB_INV, false);
+}
+
+static void nvmap_dmabuf_end_cpu_access(struct dma_buf *dmabuf,
+				       size_t start, size_t len,
+				       enum dma_data_direction dir)
+{
+	struct nvmap_handle_info *info = dmabuf->priv;
+
+	trace_nvmap_dmabuf_end_cpu_access(dmabuf, start, len);
+	__nvmap_do_cache_maint(NULL, info->handle,
+				   start, start + len,
+				   NVMAP_CACHE_OP_WB, false);
+}
+
+static void *nvmap_dmabuf_kmap(struct dma_buf *dmabuf, unsigned long page_num)
+{
+	struct nvmap_handle_info *info = dmabuf->priv;
+
+	trace_nvmap_dmabuf_kmap(dmabuf);
+	return __nvmap_kmap(info->handle, page_num);
+}
+
+static void nvmap_dmabuf_kunmap(struct dma_buf *dmabuf,
+		unsigned long page_num, void *addr)
+{
+	struct nvmap_handle_info *info = dmabuf->priv;
+
+	trace_nvmap_dmabuf_kunmap(dmabuf);
+	__nvmap_kunmap(info->handle, page_num, addr);
+}
+
+static void *nvmap_dmabuf_kmap_atomic(struct dma_buf *dmabuf,
+				      unsigned long page_num)
+{
+	WARN(1, "%s() can't be called from atomic\n", __func__);
+	return NULL;
+}
+
+int __nvmap_map(struct nvmap_handle *h, struct vm_area_struct *vma)
+{
+	struct nvmap_vma_priv *priv;
+
+	h = nvmap_handle_get(h);
+	if (!h)
+		return -EINVAL;
+
+	if (!(h->heap_type & nvmap_dev->cpu_access_mask)) {
+		nvmap_handle_put(h);
+		return -EPERM;
+	}
+
+	/*
+	 * If the handle is RO and RW mapping is requested, then
+	 * return error.
+	 */
+	if (h->from_va && h->is_ro && (vma->vm_flags & VM_WRITE)) {
+		nvmap_handle_put(h);
+		return -EPERM;
+	}
+	/*
+	 * Don't allow mmap on VPR memory as it would be mapped
+	 * as device memory. User space shouldn't be accessing
+	 * device memory.
+	 */
+	if (h->heap_type &
+		(NVMAP_HEAP_CARVEOUT_VPR | NVMAP_HEAP_CARVEOUT_IVM_VPR)) {
+		nvmap_handle_put(h);
+		return -EPERM;
+	}
+
+	priv = kzalloc(sizeof(*priv), GFP_KERNEL);
+	if (!priv) {
+		nvmap_handle_put(h);
+		return -ENOMEM;
+	}
+	priv->handle = h;
+
+	vma->vm_flags |= VM_SHARED | VM_DONTEXPAND |
+			  VM_DONTDUMP | VM_DONTCOPY |
+			  (h->heap_pgalloc ? 0 : VM_PFNMAP);
+	vma->vm_ops = &nvmap_vma_ops;
+	BUG_ON(vma->vm_private_data != NULL);
+	vma->vm_private_data = priv;
+	vma->vm_page_prot = nvmap_pgprot(h, vma->vm_page_prot);
+	nvmap_vma_open(vma);
+	return 0;
+}
+
+static int nvmap_dmabuf_mmap(struct dma_buf *dmabuf, struct vm_area_struct *vma)
+{
+	struct nvmap_handle_info *info = dmabuf->priv;
+
+	trace_nvmap_dmabuf_mmap(dmabuf);
+
+	return __nvmap_map(info->handle, vma);
+}
+
+static void *nvmap_dmabuf_vmap(struct dma_buf *dmabuf)
+{
+	struct nvmap_handle_info *info = dmabuf->priv;
+
+	trace_nvmap_dmabuf_vmap(dmabuf);
+	return __nvmap_mmap(info->handle);
+}
+
+static void nvmap_dmabuf_vunmap(struct dma_buf *dmabuf, void *vaddr)
+{
+	struct nvmap_handle_info *info = dmabuf->priv;
+
+	trace_nvmap_dmabuf_vunmap(dmabuf);
+	__nvmap_munmap(info->handle, vaddr);
+}
+
+static int nvmap_dmabuf_set_private(struct dma_buf *dmabuf,
+		struct device *dev, void *priv, void (*delete)(void *priv))
+{
+	struct nvmap_handle_info *info = dmabuf->priv;
+	struct nvmap_handle *handle = info->handle;
+	struct nvmap_handle_dmabuf_priv *curr = NULL;
+	int ret = 0;
+
+	mutex_lock(&handle->lock);
+	list_for_each_entry(curr, &handle->dmabuf_priv, list) {
+		if (curr->dev == dev) {
+			ret = -EEXIST;
+			goto unlock;
+		}
+	}
+
+	curr = kmalloc(sizeof(*curr), GFP_KERNEL);
+	if (!curr) {
+		ret = -ENOMEM;
+		goto unlock;
+	}
+	curr->priv = priv;
+	curr->dev = dev;
+	curr->priv_release = delete;
+	list_add_tail(&curr->list, &handle->dmabuf_priv);
+unlock:
+	mutex_unlock(&handle->lock);
+	return ret;
+}
+
+static void *nvmap_dmabuf_get_private(struct dma_buf *dmabuf,
+		struct device *dev)
+{
+	struct nvmap_handle_dmabuf_priv *curr = NULL;
+	struct nvmap_handle_info *info;
+	struct nvmap_handle *handle;
+	void *priv = NULL;
+
+	if (dmabuf && dmabuf->priv)
+		info = dmabuf->priv;
+	else
+		return NULL;
+
+	if (info && info->handle)
+		handle = info->handle;
+	else
+		return NULL;
+
+	mutex_lock(&handle->lock);
+	list_for_each_entry(curr, &handle->dmabuf_priv, list) {
+		if (curr->dev == dev) {
+			priv = curr->priv;
+			goto unlock;
+		}
+	}
+unlock:
+	mutex_unlock(&handle->lock);
+	return priv;
+}
+
+static struct dma_buf_ops nvmap_dma_buf_ops = {
+	.attach		= nvmap_dmabuf_attach,
+	.detach		= nvmap_dmabuf_detach,
+	.map_dma_buf	= nvmap_dmabuf_map_dma_buf,
+	.unmap_dma_buf	= nvmap_dmabuf_unmap_dma_buf,
+	.release	= nvmap_dmabuf_release,
+	.begin_cpu_access = nvmap_dmabuf_begin_cpu_access,
+	.end_cpu_access = nvmap_dmabuf_end_cpu_access,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	.map_atomic	= nvmap_dmabuf_kmap_atomic,
+	.map		= nvmap_dmabuf_kmap,
+	.unmap		= nvmap_dmabuf_kunmap,
+#else
+	.kmap_atomic	= nvmap_dmabuf_kmap_atomic,
+	.kmap		= nvmap_dmabuf_kmap,
+	.kunmap		= nvmap_dmabuf_kunmap,
+#endif
+	.mmap		= nvmap_dmabuf_mmap,
+	.vmap		= nvmap_dmabuf_vmap,
+	.vunmap		= nvmap_dmabuf_vunmap,
+	.set_drvdata	= nvmap_dmabuf_set_private,
+	.get_drvdata	= nvmap_dmabuf_get_private,
+};
+
+bool dmabuf_is_nvmap(struct dma_buf *dmabuf)
+{
+	return dmabuf->ops == &nvmap_dma_buf_ops;
+}
+EXPORT_SYMBOL(dmabuf_is_nvmap);
+
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0)
+static struct dma_buf *__dma_buf_export(struct nvmap_handle_info *info,
+					size_t size, bool ro_buf)
+{
+	DEFINE_DMA_BUF_EXPORT_INFO(exp_info);
+
+	exp_info.priv = info;
+	exp_info.ops = &nvmap_dma_buf_ops;
+	exp_info.size = size;
+
+	if (ro_buf) {
+		exp_info.flags = O_RDONLY;
+	} else {
+		exp_info.flags = O_RDWR;
+	}
+
+	exp_info.exp_flags = DMABUF_CAN_DEFER_UNMAP |
+				DMABUF_SKIP_CACHE_SYNC;
+
+	return dma_buf_export(&exp_info);
+}
+#else
+#define __dma_buf_export(info, size) \
+	dma_buf_export(info, &nvmap_dma_buf_ops, size, O_RDWR, NULL)
+#endif
+
+/*
+ * Make a dmabuf object for an nvmap handle.
+ */
+struct dma_buf *__nvmap_make_dmabuf(struct nvmap_client *client,
+				    struct nvmap_handle *handle, bool ro_buf)
+{
+	int err;
+	struct dma_buf *dmabuf;
+	struct nvmap_handle_info *info;
+
+	info = kzalloc(sizeof(*info), GFP_KERNEL);
+	if (!info) {
+		err = -ENOMEM;
+		goto err_nomem;
+	}
+	info->handle = handle;
+	INIT_LIST_HEAD(&info->maps);
+	mutex_init(&info->maps_lock);
+
+	dmabuf = __dma_buf_export(info, handle->size, ro_buf);
+	if (IS_ERR(dmabuf)) {
+		err = PTR_ERR(dmabuf);
+		goto err_export;
+	}
+	nvmap_handle_get(handle);
+
+	trace_nvmap_make_dmabuf(client->name, handle, dmabuf);
+	return dmabuf;
+
+err_export:
+	kfree(info);
+err_nomem:
+	return ERR_PTR(err);
+}
+
+int __nvmap_dmabuf_fd(struct nvmap_client *client,
+		      struct dma_buf *dmabuf, int flags)
+{
+	int start_fd = CONFIG_NVMAP_FD_START;
+
+#ifdef CONFIG_NVMAP_DEFER_FD_RECYCLE
+	if (client->next_fd < CONFIG_NVMAP_FD_START)
+		client->next_fd = CONFIG_NVMAP_FD_START;
+	start_fd = client->next_fd++;
+	if (client->next_fd >= CONFIG_NVMAP_DEFER_FD_RECYCLE_MAX_FD)
+		client->next_fd = CONFIG_NVMAP_FD_START;
+#endif
+	if (!dmabuf || !dmabuf->file)
+		return -EINVAL;
+	/* Allocate fd from start_fd(>=1024) onwards to overcome
+	 * __FD_SETSIZE limitation issue for select(),
+	 * pselect() syscalls.
+	 */
+	return tegra_alloc_fd(current->files, start_fd, flags);
+}
+
+int nvmap_get_dmabuf_fd(struct nvmap_client *client, struct nvmap_handle *h)
+{
+	int fd;
+	struct dma_buf *dmabuf;
+
+	dmabuf = __nvmap_dmabuf_export(client, h);
+	if (IS_ERR(dmabuf))
+		return PTR_ERR(dmabuf);
+
+	fd = __nvmap_dmabuf_fd(client, dmabuf, O_CLOEXEC);
+	if (IS_ERR_VALUE((uintptr_t)fd))
+		dma_buf_put(dmabuf);
+	return fd;
+}
+
+struct dma_buf *__nvmap_dmabuf_export(struct nvmap_client *client,
+				 struct nvmap_handle *handle)
+{
+	struct dma_buf *buf;
+
+	handle = nvmap_handle_get(handle);
+	if (!handle)
+		return ERR_PTR(-EINVAL);
+	buf = handle->dmabuf;
+	if (WARN(!buf, "Attempting to get a freed dma_buf!\n")) {
+		nvmap_handle_put(handle);
+		return NULL;
+	}
+
+	get_dma_buf(buf);
+
+	/*
+	 * Don't want to take out refs on the handle here.
+	 */
+	nvmap_handle_put(handle);
+
+	return handle->dmabuf;
+}
+EXPORT_SYMBOL(__nvmap_dmabuf_export);
+
+/*
+ * Returns the nvmap handle ID associated with the passed dma_buf's fd. This
+ * does not affect the ref count of the dma_buf.
+ * NOTE: Callers of this utility function must invoke nvmap_handle_put after
+ * using the returned nvmap_handle. Call to nvmap_handle_get is required in
+ * this utility function to avoid race conditions in code where nvmap_handle
+ * returned by this function is freed concurrently while the caller is still
+ * using it.
+ */
+struct nvmap_handle *nvmap_handle_get_from_dmabuf_fd(
+					struct nvmap_client *client, int fd)
+{
+	struct nvmap_handle *handle = ERR_PTR(-EINVAL);
+	struct dma_buf *dmabuf;
+	struct nvmap_handle_info *info;
+
+	dmabuf = dma_buf_get(fd);
+	if (IS_ERR(dmabuf))
+		return ERR_CAST(dmabuf);
+	if (dmabuf_is_nvmap(dmabuf)) {
+		info = dmabuf->priv;
+		handle = info->handle;
+		if (!nvmap_handle_get(handle))
+			handle = ERR_PTR(-EINVAL);
+	}
+	dma_buf_put(dmabuf);
+	return handle;
+}
+
+/*
+ * Duplicates a generic dma_buf fd. nvmap dma_buf fd has to be duplicated
+ * using existing code paths to preserve memory accounting behavior, so this
+ * function returns -EINVAL on dma_buf fds created by nvmap.
+ */
+int nvmap_dmabuf_duplicate_gen_fd(struct nvmap_client *client,
+		struct dma_buf *dmabuf)
+{
+	int ret = 0;
+
+	if (dmabuf_is_nvmap(dmabuf)) {
+		ret = -EINVAL;
+		goto error;
+	}
+
+	ret = __nvmap_dmabuf_fd(client, dmabuf, O_CLOEXEC);
+	if (ret < 0)
+		goto error;
+
+	return ret;
+
+error:
+	dma_buf_put(dmabuf);
+	return ret;
+}
diff --git a/drivers/video/tegra/nvmap/nvmap_dmabuf_t19x.c b/drivers/video/tegra/nvmap/nvmap_dmabuf_t19x.c
new file mode 100644
index 000000000000..176331a93f7a
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nvmap_dmabuf_t19x.c
@@ -0,0 +1,88 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_dmabuf_t19x.c
+ *
+ * Copyright (c) 2016-2017, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"nvmap: %s() " fmt, __func__
+
+#include <linux/of.h>
+#include <linux/miscdevice.h>
+#include <linux/nvmap_t19x.h>
+
+#include "nvmap_priv.h"
+
+extern bool of_dma_is_coherent(struct device_node *np);
+
+static void nvmap_handle_t19x_free(void *ptr)
+{
+	struct nvmap_handle_t19x *handle_t19x =
+		(struct nvmap_handle_t19x *)ptr;
+	int outstanding_nc_pin = atomic_read(&handle_t19x->nc_pin);
+
+	WARN(outstanding_nc_pin,
+		"outstanding dma maps from %d coherent devices",
+		outstanding_nc_pin);
+	kfree(handle_t19x);
+}
+
+struct sg_table *nvmap_dmabuf_map_dma_buf(
+	struct dma_buf_attachment *attach, enum dma_data_direction dir)
+{
+	struct nvmap_handle_info *info = attach->dmabuf->priv;
+	struct nvmap_handle *handle = info->handle;
+	struct nvmap_handle_t19x *handle_t19x = NULL;
+	struct device *dev = nvmap_dev->dev_user.parent;
+	struct sg_table *sg_table;
+
+	if (!nvmap_version_t19x)
+		goto dmabuf_map;
+
+	handle_t19x = dma_buf_get_drvdata(handle->dmabuf, dev);
+	if (!handle_t19x && !of_dma_is_coherent(attach->dev->of_node)) {
+		handle_t19x = kmalloc(sizeof(*handle_t19x), GFP_KERNEL);
+		if (WARN(!handle_t19x, "No memory!!"))
+			return ERR_PTR(-ENOMEM);
+
+		atomic_set(&handle_t19x->nc_pin, 0);
+		dma_buf_set_drvdata(handle->dmabuf, dev,
+				handle_t19x, nvmap_handle_t19x_free);
+	}
+
+	if (!of_dma_is_coherent(attach->dev->of_node))
+		atomic_inc(&handle_t19x->nc_pin);
+
+dmabuf_map:
+	sg_table = _nvmap_dmabuf_map_dma_buf(attach, dir);
+	/* no need to free handle_t19x, it is freed with handle */
+	if (IS_ERR(sg_table))
+		if (handle_t19x)
+			atomic_dec(&handle_t19x->nc_pin);
+
+	return sg_table;
+}
+
+void nvmap_dmabuf_unmap_dma_buf(struct dma_buf_attachment *attach,
+	 struct sg_table *sgt, enum dma_data_direction dir)
+{
+	struct nvmap_handle_info *info = attach->dmabuf->priv;
+	struct nvmap_handle *handle = info->handle;
+	struct device *dev = nvmap_dev->dev_user.parent;
+	struct nvmap_handle_t19x *handle_t19x;
+
+	_nvmap_dmabuf_unmap_dma_buf(attach, sgt, dir);
+
+	handle_t19x = dma_buf_get_drvdata(handle->dmabuf, dev);
+	if (handle_t19x && !of_dma_is_coherent(attach->dev->of_node))
+		atomic_dec(&handle_t19x->nc_pin);
+}
+
diff --git a/drivers/video/tegra/nvmap/nvmap_fault.c b/drivers/video/tegra/nvmap/nvmap_fault.c
new file mode 100644
index 000000000000..99b958d6a9ef
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nvmap_fault.c
@@ -0,0 +1,292 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_fault.c
+ *
+ * Copyright (c) 2011-2021, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"%s: " fmt, __func__
+
+#include <trace/events/nvmap.h>
+#include <linux/highmem.h>
+
+#include "nvmap_priv.h"
+
+static void nvmap_vma_close(struct vm_area_struct *vma);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+static int nvmap_vma_fault(struct vm_fault *vmf);
+#else
+static int nvmap_vma_fault(struct vm_area_struct *vma, struct vm_fault *vmf);
+#endif
+static bool nvmap_fixup_prot(struct vm_area_struct *vma,
+		unsigned long addr, pgoff_t pgoff);
+
+struct vm_operations_struct nvmap_vma_ops = {
+	.open		= nvmap_vma_open,
+	.close		= nvmap_vma_close,
+	.fault		= nvmap_vma_fault,
+	.fixup_prot	= nvmap_fixup_prot,
+};
+
+int is_nvmap_vma(struct vm_area_struct *vma)
+{
+	return vma->vm_ops == &nvmap_vma_ops;
+}
+
+/* to ensure that the backing store for the VMA isn't freed while a fork'd
+ * reference still exists, nvmap_vma_open increments the reference count on
+ * the handle, and nvmap_vma_close decrements it. alternatively, we could
+ * disallow copying of the vma, or behave like pmem and zap the pages. FIXME.
+*/
+void nvmap_vma_open(struct vm_area_struct *vma)
+{
+	struct nvmap_vma_priv *priv;
+	struct nvmap_handle *h;
+	struct nvmap_vma_list *vma_list, *tmp;
+	struct list_head *tmp_head = NULL;
+	pid_t current_pid = task_tgid_nr(current);
+	bool vma_pos_found = false;
+	size_t nr_page, i;
+	ulong vma_open_count;
+
+	priv = vma->vm_private_data;
+	BUG_ON(!priv);
+	BUG_ON(!priv->handle);
+
+	h = priv->handle;
+	nvmap_umaps_inc(h);
+
+	mutex_lock(&h->lock);
+	vma_open_count = atomic_inc_return(&priv->count);
+	if (vma_open_count == 1 && h->heap_pgalloc) {
+		nr_page = h->size >> PAGE_SHIFT;
+		for (i = 0; i < nr_page; i++) {
+			struct page *page = nvmap_to_page(h->pgalloc.pages[i]);
+			/* This is necessry to avoid page being accounted
+			 * under NR_FILE_MAPPED. This way NR_FILE_MAPPED would
+			 * be fully accounted under NR_FILE_PAGES. This allows
+			 * Android low mem killer detect low memory condition
+			 * precisely.
+			 * This has a side effect of inaccurate pss accounting
+			 * for NvMap memory mapped into user space. Android
+			 * procrank and NvMap Procrank both would have same
+			 * issue. Subtracting NvMap_Procrank pss from
+			 * procrank pss would give non-NvMap pss held by process
+			 * and adding NvMap memory used by process represents
+			 * entire memroy consumption by the process.
+			 */
+			atomic_inc(&page->_mapcount);
+		}
+	}
+	mutex_unlock(&h->lock);
+
+	vma_list = kmalloc(sizeof(*vma_list), GFP_KERNEL);
+	if (vma_list) {
+		mutex_lock(&h->lock);
+		tmp_head = &h->vmas;
+
+		/* insert vma into handle's vmas list in the increasing order of
+		 * handle offsets
+		 */
+		list_for_each_entry(tmp, &h->vmas, list) {
+			/* if vma exists in list, just increment refcount */
+			if (tmp->vma == vma) {
+				atomic_inc(&tmp->ref);
+				kfree(vma_list);
+				goto unlock;
+			}
+
+			if (!vma_pos_found && (current_pid == tmp->pid)) {
+				if (vma->vm_pgoff < tmp->vma->vm_pgoff) {
+					tmp_head = &tmp->list;
+					vma_pos_found = true;
+				} else {
+					tmp_head = tmp->list.next;
+				}
+			}
+		}
+
+		vma_list->vma = vma;
+		vma_list->pid = current_pid;
+		vma_list->save_vm_flags = vma->vm_flags;
+		atomic_set(&vma_list->ref, 1);
+		list_add_tail(&vma_list->list, tmp_head);
+unlock:
+		mutex_unlock(&h->lock);
+	} else {
+		WARN(1, "vma not tracked");
+	}
+}
+
+static void nvmap_vma_close(struct vm_area_struct *vma)
+{
+	struct nvmap_vma_priv *priv = vma->vm_private_data;
+	struct nvmap_vma_list *vma_list;
+	struct nvmap_handle *h;
+	bool vma_found = false;
+	size_t nr_page, i;
+
+	if (!priv)
+		return;
+
+	BUG_ON(!priv->handle);
+
+	h = priv->handle;
+	nr_page = h->size >> PAGE_SHIFT;
+
+	mutex_lock(&h->lock);
+	list_for_each_entry(vma_list, &h->vmas, list) {
+		if (vma_list->vma != vma)
+			continue;
+		if (atomic_dec_return(&vma_list->ref) == 0) {
+			list_del(&vma_list->list);
+			kfree(vma_list);
+		}
+		vma_found = true;
+		break;
+	}
+	BUG_ON(!vma_found);
+	nvmap_umaps_dec(h);
+
+	if (__atomic_add_unless(&priv->count, -1, 0) == 1) {
+		if (h->heap_pgalloc) {
+			for (i = 0; i < nr_page; i++) {
+				struct page *page;
+				page = nvmap_to_page(h->pgalloc.pages[i]);
+				atomic_dec(&page->_mapcount);
+			}
+		}
+		mutex_unlock(&h->lock);
+		if (priv->handle)
+			nvmap_handle_put(priv->handle);
+		vma->vm_private_data = NULL;
+		kfree(priv);
+	} else {
+		mutex_unlock(&h->lock);
+	}
+}
+
+static int nvmap_vma_fault(struct vm_fault *vmf)
+{
+	struct page *page;
+	struct nvmap_vma_priv *priv;
+	unsigned long offs;
+	struct vm_area_struct *vma = vmf->vma;
+	unsigned long vmf_address = vmf->address;
+
+	offs = (unsigned long)(vmf_address - vma->vm_start);
+	priv = vma->vm_private_data;
+	if (!priv || !priv->handle || !priv->handle->alloc)
+		return VM_FAULT_SIGBUS;
+
+	offs += priv->offs;
+	/* if the VMA was split for some reason, vm_pgoff will be the VMA's
+	 * offset from the original VMA */
+	offs += (vma->vm_pgoff << PAGE_SHIFT);
+
+	if (offs >= priv->handle->size)
+		return VM_FAULT_SIGBUS;
+
+	if (!priv->handle->heap_pgalloc) {
+		unsigned long pfn;
+		BUG_ON(priv->handle->carveout->base & ~PAGE_MASK);
+		pfn = ((priv->handle->carveout->base + offs) >> PAGE_SHIFT);
+		if (!pfn_valid(pfn)) {
+			vm_insert_pfn(vma,
+				(unsigned long)vmf_address, pfn);
+			return VM_FAULT_NOPAGE;
+		}
+		/* CMA memory would get here */
+		page = pfn_to_page(pfn);
+	} else {
+		void *kaddr;
+
+		offs >>= PAGE_SHIFT;
+		if (atomic_read(&priv->handle->pgalloc.reserved))
+			return VM_FAULT_SIGBUS;
+		page = nvmap_to_page(priv->handle->pgalloc.pages[offs]);
+
+		if (!nvmap_handle_track_dirty(priv->handle))
+			goto finish;
+
+		mutex_lock(&priv->handle->lock);
+		if (nvmap_page_dirty(priv->handle->pgalloc.pages[offs])) {
+			mutex_unlock(&priv->handle->lock);
+			goto finish;
+		}
+
+		/* inner cache maint */
+		kaddr  = kmap(page);
+		BUG_ON(!kaddr);
+		inner_cache_maint(NVMAP_CACHE_OP_WB_INV, kaddr, PAGE_SIZE);
+		kunmap(page);
+
+		if (priv->handle->flags & NVMAP_HANDLE_INNER_CACHEABLE)
+			goto make_dirty;
+
+make_dirty:
+		nvmap_page_mkdirty(&priv->handle->pgalloc.pages[offs]);
+		atomic_inc(&priv->handle->pgalloc.ndirty);
+		mutex_unlock(&priv->handle->lock);
+	}
+
+finish:
+	if (page)
+		get_page(page);
+	vmf->page = page;
+	return (page) ? 0 : VM_FAULT_SIGBUS;
+}
+
+static bool nvmap_fixup_prot(struct vm_area_struct *vma,
+		unsigned long addr, pgoff_t pgoff)
+{
+	struct page *page;
+	struct nvmap_vma_priv *priv;
+	unsigned long offs;
+	void *kaddr;
+
+	priv = vma->vm_private_data;
+	if (!priv || !priv->handle || !priv->handle->alloc)
+		return false;
+
+	offs = pgoff << PAGE_SHIFT;
+	offs += priv->offs;
+	if ((offs >= priv->handle->size) || !priv->handle->heap_pgalloc)
+		return false;
+
+	if (atomic_read(&priv->handle->pgalloc.reserved))
+		return false;
+
+	if (!nvmap_handle_track_dirty(priv->handle))
+		return true;
+
+	mutex_lock(&priv->handle->lock);
+	offs >>= PAGE_SHIFT;
+	if (nvmap_page_dirty(priv->handle->pgalloc.pages[offs]))
+		goto unlock;
+
+	page = nvmap_to_page(priv->handle->pgalloc.pages[offs]);
+	/* inner cache maint */
+	kaddr  = kmap(page);
+	BUG_ON(!kaddr);
+	inner_cache_maint(NVMAP_CACHE_OP_WB_INV, kaddr, PAGE_SIZE);
+	kunmap(page);
+
+	if (priv->handle->flags & NVMAP_HANDLE_INNER_CACHEABLE)
+		goto make_dirty;
+
+make_dirty:
+	nvmap_page_mkdirty(&priv->handle->pgalloc.pages[offs]);
+	atomic_inc(&priv->handle->pgalloc.ndirty);
+unlock:
+	mutex_unlock(&priv->handle->lock);
+	return true;
+}
diff --git a/drivers/video/tegra/nvmap/nvmap_handle.c b/drivers/video/tegra/nvmap/nvmap_handle.c
new file mode 100644
index 000000000000..1487bae5aa20
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nvmap_handle.c
@@ -0,0 +1,408 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_handle.c
+ *
+ * Handle allocation and freeing routines for nvmap
+ *
+ * Copyright (c) 2009-2021, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"%s: " fmt, __func__
+
+#include <linux/err.h>
+#include <linux/io.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/mm.h>
+#include <linux/rbtree.h>
+#include <linux/dma-buf.h>
+#include <linux/moduleparam.h>
+#include <linux/nvmap.h>
+#include <soc/tegra/fuse.h>
+
+#include <asm/pgtable.h>
+
+#include <trace/events/nvmap.h>
+
+#include "nvmap_priv.h"
+#include "nvmap_ioctl.h"
+
+/*
+ * Verifies that the passed ID is a valid handle ID. Then the passed client's
+ * reference to the handle is returned.
+ *
+ * Note: to call this function make sure you own the client ref lock.
+ */
+struct nvmap_handle_ref *__nvmap_validate_locked(struct nvmap_client *c,
+						 struct nvmap_handle *h)
+{
+	struct rb_node *n = c->handle_refs.rb_node;
+
+	while (n) {
+		struct nvmap_handle_ref *ref;
+		ref = rb_entry(n, struct nvmap_handle_ref, node);
+		if (ref->handle == h)
+			return ref;
+		else if ((uintptr_t)h > (uintptr_t)ref->handle)
+			n = n->rb_right;
+		else
+			n = n->rb_left;
+	}
+
+	return NULL;
+}
+/* adds a newly-created handle to the device master tree */
+void nvmap_handle_add(struct nvmap_device *dev, struct nvmap_handle *h)
+{
+	struct rb_node **p;
+	struct rb_node *parent = NULL;
+
+	spin_lock(&dev->handle_lock);
+	p = &dev->handles.rb_node;
+	while (*p) {
+		struct nvmap_handle *b;
+
+		parent = *p;
+		b = rb_entry(parent, struct nvmap_handle, node);
+		if (h > b)
+			p = &parent->rb_right;
+		else
+			p = &parent->rb_left;
+	}
+	rb_link_node(&h->node, parent, p);
+	rb_insert_color(&h->node, &dev->handles);
+	nvmap_lru_add(h);
+	spin_unlock(&dev->handle_lock);
+}
+
+/* remove a handle from the device's tree of all handles; called
+ * when freeing handles. */
+int nvmap_handle_remove(struct nvmap_device *dev, struct nvmap_handle *h)
+{
+	spin_lock(&dev->handle_lock);
+
+	/* re-test inside the spinlock if the handle really has no clients;
+	 * only remove the handle if it is unreferenced */
+	if (atomic_add_return(0, &h->ref) > 0) {
+		spin_unlock(&dev->handle_lock);
+		return -EBUSY;
+	}
+	smp_rmb();
+	BUG_ON(atomic_read(&h->ref) < 0);
+	BUG_ON(atomic_read(&h->pin) != 0);
+
+	nvmap_lru_del(h);
+	rb_erase(&h->node, &dev->handles);
+
+	spin_unlock(&dev->handle_lock);
+	return 0;
+}
+
+/* Validates that a handle is in the device master tree and that the
+ * client has permission to access it. */
+struct nvmap_handle *nvmap_validate_get(struct nvmap_handle *id)
+{
+	struct nvmap_handle *h = NULL;
+	struct rb_node *n;
+
+	spin_lock(&nvmap_dev->handle_lock);
+
+	n = nvmap_dev->handles.rb_node;
+
+	while (n) {
+		h = rb_entry(n, struct nvmap_handle, node);
+		if (h == id) {
+			h = nvmap_handle_get(h);
+			spin_unlock(&nvmap_dev->handle_lock);
+			return h;
+		}
+		if (id > h)
+			n = n->rb_right;
+		else
+			n = n->rb_left;
+	}
+	spin_unlock(&nvmap_dev->handle_lock);
+	return NULL;
+}
+
+static void add_handle_ref(struct nvmap_client *client,
+			   struct nvmap_handle_ref *ref)
+{
+	struct rb_node **p, *parent = NULL;
+
+	nvmap_ref_lock(client);
+	p = &client->handle_refs.rb_node;
+	while (*p) {
+		struct nvmap_handle_ref *node;
+		parent = *p;
+		node = rb_entry(parent, struct nvmap_handle_ref, node);
+		if (ref->handle > node->handle)
+			p = &parent->rb_right;
+		else
+			p = &parent->rb_left;
+	}
+	rb_link_node(&ref->node, parent, p);
+	rb_insert_color(&ref->node, &client->handle_refs);
+	client->handle_count++;
+	if (client->handle_count > nvmap_max_handle_count)
+		nvmap_max_handle_count = client->handle_count;
+	atomic_inc(&ref->handle->share_count);
+	nvmap_ref_unlock(client);
+}
+
+struct nvmap_handle_ref *nvmap_create_handle_from_va(struct nvmap_client *client,
+						     ulong vaddr, size_t size,
+						     u32 flags)
+{
+	struct vm_area_struct *vma;
+	struct nvmap_handle_ref *ref;
+	vm_flags_t vm_flags;
+
+	/* don't allow non-page aligned addresses. */
+	if (vaddr & ~PAGE_MASK)
+		return ERR_PTR(-EINVAL);
+
+	vma = find_vma(current->mm, vaddr);
+	if (unlikely(!vma))
+		return ERR_PTR(-EINVAL);
+
+	if (!size)
+		size = vma->vm_end - vaddr;
+
+	/* Don't allow exuberantly large sizes. */
+	if (!nvmap_memory_available(size)) {
+		pr_debug("Cannot allocate %zu bytes.\n", size);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	vm_flags = vma->vm_flags;
+	/*
+	 * If buffer is malloc/mprotect as RO but alloc flag is not passed
+	 * as RO, don't create handle.
+	 */
+	if (!(vm_flags & VM_WRITE) && !(flags & NVMAP_HANDLE_RO))
+		return ERR_PTR(-EINVAL);
+
+	ref = nvmap_create_handle(client, size, flags & NVMAP_HANDLE_RO);
+	if (!IS_ERR(ref))
+		ref->handle->orig_size = size;
+
+	return ref;
+}
+
+struct nvmap_handle_ref *nvmap_create_handle(struct nvmap_client *client,
+					     size_t size, bool ro_buf)
+{
+	void *err = ERR_PTR(-ENOMEM);
+	struct nvmap_handle *h;
+	struct nvmap_handle_ref *ref = NULL;
+
+	if (!client)
+		return ERR_PTR(-EINVAL);
+
+	if (!size)
+		return ERR_PTR(-EINVAL);
+
+	h = kzalloc(sizeof(*h), GFP_KERNEL);
+	if (!h)
+		return ERR_PTR(-ENOMEM);
+
+	ref = kzalloc(sizeof(*ref), GFP_KERNEL);
+	if (!ref)
+		goto ref_alloc_fail;
+
+	atomic_set(&h->ref, 1);
+	atomic_set(&h->pin, 0);
+	h->owner = client;
+	BUG_ON(!h->owner);
+	h->orig_size = size;
+	h->size = PAGE_ALIGN(size);
+	h->flags = NVMAP_HANDLE_WRITE_COMBINE;
+	h->peer = NVMAP_IVM_INVALID_PEER;
+	mutex_init(&h->lock);
+	INIT_LIST_HEAD(&h->vmas);
+	INIT_LIST_HEAD(&h->lru);
+	INIT_LIST_HEAD(&h->dmabuf_priv);
+
+	/*
+	 * This takes out 1 ref on the dambuf. This corresponds to the
+	 * handle_ref that gets automatically made by nvmap_create_handle().
+	 */
+	h->dmabuf = __nvmap_make_dmabuf(client, h, ro_buf);
+	if (IS_ERR(h->dmabuf)) {
+		err = h->dmabuf;
+		goto make_dmabuf_fail;
+	}
+
+	nvmap_handle_add(nvmap_dev, h);
+
+	/*
+	 * Major assumption here: the dma_buf object that the handle contains
+	 * is created with a ref count of 1.
+	 */
+	atomic_set(&ref->dupes, 1);
+	ref->handle = h;
+	add_handle_ref(client, ref);
+	trace_nvmap_create_handle(client, client->name, h, size, ref);
+	return ref;
+
+make_dmabuf_fail:
+	kfree(ref);
+ref_alloc_fail:
+	kfree(h);
+	return err;
+}
+
+struct nvmap_handle_ref *nvmap_try_duplicate_by_ivmid(
+		struct nvmap_client *client, u64 ivm_id,
+		struct nvmap_heap_block **block)
+{
+	struct nvmap_handle *h = NULL;
+	struct nvmap_handle_ref *ref = NULL;
+	struct rb_node *n;
+
+	spin_lock(&nvmap_dev->handle_lock);
+
+	n = nvmap_dev->handles.rb_node;
+	for (n = rb_first(&nvmap_dev->handles); n; n = rb_next(n)) {
+		h = rb_entry(n, struct nvmap_handle, node);
+		if (h->ivm_id == ivm_id) {
+			BUG_ON(!virt_addr_valid(h));
+			/* get handle's ref only if non-zero */
+			if (atomic_inc_not_zero(&h->ref) == 0) {
+				*block = h->carveout;
+				/* strip handle's block and fail duplication */
+				h->carveout = NULL;
+				break;
+			}
+			spin_unlock(&nvmap_dev->handle_lock);
+			goto found;
+		}
+	}
+
+	spin_unlock(&nvmap_dev->handle_lock);
+	/* handle is either freed or being freed, don't duplicate it */
+	goto finish;
+
+	/*
+	 * From this point, handle and its buffer are valid and won't be
+	 * freed as a reference is taken on it. The dmabuf can still be
+	 * freed anytime till reference is taken on it below.
+	 */
+found:
+	mutex_lock(&h->lock);
+	/*
+	 * Save this block. If dmabuf's reference is not held in time,
+	 * this can be reused to avoid the delay to free the buffer
+	 * in this old handle and allocate it for a new handle from
+	 * the ivm allocation ioctl.
+	 */
+	*block = h->carveout;
+	if (!h->dmabuf)
+		goto fail;
+	BUG_ON(!h->dmabuf->file);
+	/* This is same as get_dma_buf() if file->f_count was non-zero */
+	if (atomic_long_inc_not_zero(&h->dmabuf->file->f_count) == 0)
+		goto fail;
+	mutex_unlock(&h->lock);
+
+	/* h->dmabuf can't be NULL anymore. Duplicate the handle. */
+	ref = nvmap_duplicate_handle(client, h, true);
+	/* put the extra ref taken using get_dma_buf. */
+	dma_buf_put(h->dmabuf);
+finish:
+	return ref;
+fail:
+	/* free handle but not its buffer */
+	h->carveout = NULL;
+	mutex_unlock(&h->lock);
+	nvmap_handle_put(h);
+	return NULL;
+}
+
+struct nvmap_handle_ref *nvmap_duplicate_handle(struct nvmap_client *client,
+					struct nvmap_handle *h, bool skip_val)
+{
+	struct nvmap_handle_ref *ref = NULL;
+
+	BUG_ON(!client);
+
+	if (!skip_val)
+		/* on success, the reference count for the handle should be
+		 * incremented, so the success paths will not call
+		 * nvmap_handle_put */
+		h = nvmap_validate_get(h);
+
+	if (!h) {
+		pr_debug("%s duplicate handle failed\n",
+			    current->group_leader->comm);
+		return ERR_PTR(-EPERM);
+	}
+
+	if (!h->alloc) {
+		pr_err("%s duplicating unallocated handle\n",
+			current->group_leader->comm);
+		nvmap_handle_put(h);
+		return ERR_PTR(-EINVAL);
+	}
+
+	nvmap_ref_lock(client);
+	ref = __nvmap_validate_locked(client, h);
+
+	if (ref) {
+		/* handle already duplicated in client; just increment
+		 * the reference count rather than re-duplicating it */
+		atomic_inc(&ref->dupes);
+		nvmap_ref_unlock(client);
+		goto out;
+	}
+
+	nvmap_ref_unlock(client);
+
+	ref = kzalloc(sizeof(*ref), GFP_KERNEL);
+	if (!ref) {
+		nvmap_handle_put(h);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	atomic_set(&ref->dupes, 1);
+	ref->handle = h;
+	add_handle_ref(client, ref);
+
+	/*
+	 * Ref counting on the dma_bufs follows the creation and destruction of
+	 * nvmap_handle_refs. That is every time a handle_ref is made the
+	 * dma_buf ref count goes up and everytime a handle_ref is destroyed
+	 * the dma_buf ref count goes down.
+	 */
+	get_dma_buf(h->dmabuf);
+
+out:
+	NVMAP_TAG_TRACE(trace_nvmap_duplicate_handle,
+		NVMAP_TP_ARGS_CHR(client, h, ref));
+	return ref;
+}
+
+struct nvmap_handle_ref *nvmap_create_handle_from_fd(
+			struct nvmap_client *client, int fd)
+{
+	struct nvmap_handle *handle;
+	struct nvmap_handle_ref *ref;
+
+	BUG_ON(!client);
+
+	handle = nvmap_handle_get_from_dmabuf_fd(client, fd);
+	if (IS_ERR(handle))
+		return ERR_CAST(handle);
+	ref = nvmap_duplicate_handle(client, handle, false);
+	nvmap_handle_put(handle);
+	return ref;
+}
diff --git a/drivers/video/tegra/nvmap/nvmap_heap.c b/drivers/video/tegra/nvmap/nvmap_heap.c
new file mode 100644
index 000000000000..492deb695595
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nvmap_heap.c
@@ -0,0 +1,582 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_heap.c
+ *
+ * GPU heap allocator.
+ *
+ * Copyright (c) 2011-2022, NVIDIA Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"%s: " fmt, __func__
+
+#include <linux/debugfs.h>
+#include <linux/device.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/mm.h>
+#include <linux/mutex.h>
+#include <linux/slab.h>
+#include <linux/err.h>
+#include <linux/bug.h>
+#include <linux/stat.h>
+#include <linux/sizes.h>
+#include <linux/io.h>
+#include <linux/version.h>
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+#include <linux/sched/clock.h>
+#endif
+
+#include <linux/nvmap.h>
+#include <linux/dma-mapping.h>
+//#include <linux/dma-contiguous.h>
+
+#include "nvmap_priv.h"
+#include "nvmap_heap.h"
+
+/*
+ * "carveouts" are platform-defined regions of physically contiguous memory
+ * which are not managed by the OS. A platform may specify multiple carveouts,
+ * for either small special-purpose memory regions (like IRAM on Tegra SoCs)
+ * or reserved regions of main system memory.
+ *
+ * The carveout allocator returns allocations which are physically contiguous.
+ */
+
+static struct kmem_cache *heap_block_cache;
+
+struct list_block {
+	struct nvmap_heap_block block;
+	struct list_head all_list;
+	unsigned int mem_prot;
+	phys_addr_t orig_addr;
+	size_t size;
+	size_t align;
+	struct nvmap_heap *heap;
+	struct list_head free_list;
+};
+
+struct nvmap_heap {
+	struct list_head all_list;
+	struct mutex lock;
+	const char *name;
+	void *arg;
+	/* heap base */
+	phys_addr_t base;
+	/* heap size */
+	size_t len;
+	struct device *cma_dev;
+	struct device *dma_dev;
+	bool is_ivm;
+	bool can_alloc;  /* Used only if is_ivm == true */
+	int peer;        /* Used only if is_ivm == true */
+	int vm_id;       /* Used only if is_ivm == true */
+	bool is_ivm_vpr; /* Used only if is_ivm == true */
+	struct nvmap_pm_ops pm_ops;
+};
+
+struct device *dma_dev_from_handle(unsigned long type)
+{
+	int i;
+	struct nvmap_carveout_node *co_heap;
+
+	for (i = 0; i < nvmap_dev->nr_carveouts; i++) {
+		co_heap = &nvmap_dev->heaps[i];
+
+		if (!(co_heap->heap_bit & type))
+			continue;
+
+		return co_heap->carveout->dma_dev;
+	}
+	return ERR_PTR(-ENODEV);
+}
+
+int nvmap_query_heap_peer(struct nvmap_heap *heap)
+{
+	if (!heap || !heap->is_ivm)
+		return -EINVAL;
+
+	return heap->peer;
+}
+
+size_t nvmap_query_heap_size(struct nvmap_heap *heap)
+{
+	if (!heap)
+		return -EINVAL;
+
+	return heap->len;
+}
+
+void nvmap_heap_debugfs_init(struct dentry *heap_root, struct nvmap_heap *heap)
+{
+	if (sizeof(heap->base) == sizeof(u64))
+		debugfs_create_x64("base", S_IRUGO,
+			heap_root, (u64 *)&heap->base);
+	else
+		debugfs_create_x32("base", S_IRUGO,
+			heap_root, (u32 *)&heap->base);
+	if (sizeof(heap->len) == sizeof(u64))
+		debugfs_create_x64("size", S_IRUGO,
+			heap_root, (u64 *)&heap->len);
+	else
+		debugfs_create_x32("size", S_IRUGO,
+			heap_root, (u32 *)&heap->len);
+}
+
+static phys_addr_t nvmap_alloc_mem(struct nvmap_heap *h, size_t len,
+				   phys_addr_t *start)
+{
+	phys_addr_t pa;
+	DEFINE_DMA_ATTRS(attrs);
+	struct device *dev = h->dma_dev;
+
+	dma_set_attr(DMA_ATTR_ALLOC_EXACT_SIZE, __DMA_ATTR(attrs));
+
+#ifdef CONFIG_TEGRA_VIRTUALIZATION
+	if (start && h->is_ivm) {
+		void *ret;
+		pa = h->base + (*start);
+		ret = dma_mark_declared_memory_occupied(dev, pa, len,
+					__DMA_ATTR(attrs));
+		if (IS_ERR(ret)) {
+			dev_err(dev, "Failed to reserve (%pa) len(%zu)\n",
+					&pa, len);
+			return DMA_ERROR_CODE;
+		} else {
+			dev_dbg(dev, "reserved (%pa) len(%zu)\n",
+				&pa, len);
+		}
+	} else
+#endif
+	{
+		(void)dma_alloc_attrs(dev, len, &pa,
+				GFP_KERNEL, __DMA_ATTR(attrs));
+		if (!dma_mapping_error(dev, pa)) {
+			int ret;
+
+			dev_dbg(dev, "Allocated addr (%pa) len(%zu)\n",
+					&pa, len);
+			if (!dma_is_coherent_dev(dev) && h->cma_dev) {
+				ret = nvmap_cache_maint_phys_range(
+					NVMAP_CACHE_OP_WB, pa, pa + len,
+					true, true);
+				if (!ret)
+					return pa;
+
+				dev_err(dev, "cache WB on (%pa, %zu) failed\n",
+					&pa, len);
+			}
+		}
+	}
+
+	return pa;
+}
+
+static void nvmap_free_mem(struct nvmap_heap *h, phys_addr_t base,
+				size_t len)
+{
+	struct device *dev = h->dma_dev;
+	DEFINE_DMA_ATTRS(attrs);
+
+	dma_set_attr(DMA_ATTR_ALLOC_EXACT_SIZE, __DMA_ATTR(attrs));
+	dev_dbg(dev, "Free base (%pa) size (%zu)\n", &base, len);
+#ifdef CONFIG_TEGRA_VIRTUALIZATION
+	if (h->is_ivm && !h->can_alloc) {
+		dma_mark_declared_memory_unoccupied(dev, base, len, __DMA_ATTR(attrs));
+	} else
+#endif
+	{
+		dma_free_attrs(dev, len,
+			        (void *)(uintptr_t)base,
+			        (dma_addr_t)base, __DMA_ATTR(attrs));
+	}
+}
+
+/*
+ * base_max limits position of allocated chunk in memory.
+ * if base_max is 0 then there is no such limitation.
+ */
+static struct nvmap_heap_block *do_heap_alloc(struct nvmap_heap *heap,
+					      size_t len, size_t align,
+					      unsigned int mem_prot,
+					      phys_addr_t base_max,
+					      phys_addr_t *start)
+{
+	struct list_block *heap_block = NULL;
+	dma_addr_t dev_base;
+	struct device *dev = heap->dma_dev;
+
+	/* since pages are only mappable with one cache attribute,
+	 * and most allocations from carveout heaps are DMA coherent
+	 * (i.e., non-cacheable), round cacheable allocations up to
+	 * a page boundary to ensure that the physical pages will
+	 * only be mapped one way. */
+	if (mem_prot == NVMAP_HANDLE_CACHEABLE ||
+	    mem_prot == NVMAP_HANDLE_INNER_CACHEABLE) {
+		align = max_t(size_t, align, PAGE_SIZE);
+		len = PAGE_ALIGN(len);
+	}
+
+	if (heap->is_ivm)
+		align = max_t(size_t, align, NVMAP_IVM_ALIGNMENT);
+
+	heap_block = kmem_cache_zalloc(heap_block_cache, GFP_KERNEL);
+	if (!heap_block) {
+		dev_err(dev, "%s: failed to alloc heap block %s\n",
+			__func__, dev_name(dev));
+		goto fail_heap_block_alloc;
+	}
+
+	dev_base = nvmap_alloc_mem(heap, len, start);
+	if (dma_mapping_error(dev, dev_base)) {
+		dev_err(dev, "failed to alloc mem of size (%zu)\n",
+			len);
+		if (dma_is_coherent_dev(dev)) {
+			struct dma_coherent_stats stats;
+
+			dma_get_coherent_stats(dev, &stats);
+			dev_err(dev, "used:%zu,curr_size:%zu max:%zu\n",
+				stats.used, stats.size, stats.max);
+		}
+		goto fail_dma_alloc;
+	}
+
+	heap_block->block.base = dev_base;
+	heap_block->orig_addr = dev_base;
+	heap_block->size = len;
+
+	list_add_tail(&heap_block->all_list, &heap->all_list);
+	heap_block->heap = heap;
+	heap_block->mem_prot = mem_prot;
+	heap_block->align = align;
+	return &heap_block->block;
+
+fail_dma_alloc:
+	kmem_cache_free(heap_block_cache, heap_block);
+fail_heap_block_alloc:
+	return NULL;
+}
+
+static struct list_block *do_heap_free(struct nvmap_heap_block *block)
+{
+	struct list_block *b = container_of(block, struct list_block, block);
+	struct nvmap_heap *heap = b->heap;
+
+	list_del(&b->all_list);
+
+	nvmap_free_mem(heap, block->base, b->size);
+	kmem_cache_free(heap_block_cache, b);
+
+	return b;
+}
+
+/* nvmap_heap_alloc: allocates a block of memory of len bytes, aligned to
+ * align bytes. */
+struct nvmap_heap_block *nvmap_heap_alloc(struct nvmap_heap *h,
+					  struct nvmap_handle *handle,
+					  phys_addr_t *start)
+{
+	struct nvmap_heap_block *b;
+	size_t len        = handle->size;
+	size_t align      = handle->align;
+	unsigned int prot = handle->flags;
+
+	mutex_lock(&h->lock);
+
+	if (h->is_ivm) { /* Is IVM carveout? */
+		/* Check if this correct IVM heap */
+		if (handle->peer != h->peer) {
+			mutex_unlock(&h->lock);
+			return NULL;
+		} else {
+			if (h->can_alloc && start) {
+				/* If this partition does actual allocation, it
+				 * should not specify start_offset.
+				 */
+				mutex_unlock(&h->lock);
+				return NULL;
+			} else if (!h->can_alloc && !start) {
+				/* If this partition does not do actual
+				 * allocation, it should specify start_offset.
+				 */
+				mutex_unlock(&h->lock);
+				return NULL;
+			}
+		}
+	}
+
+	/*
+	 * If this HEAP has pm_ops defined and powering on the
+	 * RAM attached with the HEAP returns error, don't
+	 * allocate from the heap and return NULL.
+	 */
+	if (h->pm_ops.busy) {
+		if (h->pm_ops.busy() < 0) {
+			pr_err("Unable to power on the heap device\n");
+			mutex_unlock(&h->lock);
+			return NULL;
+		}
+	}
+
+	align = max_t(size_t, align, L1_CACHE_BYTES);
+	b = do_heap_alloc(h, len, align, prot, 0, start);
+	if (b) {
+		b->handle = handle;
+		handle->carveout = b;
+		/* Generate IVM for partition that can alloc */
+		if (h->is_ivm && h->can_alloc) {
+			u64 size_temp = len;
+
+			/* h->base is the address of the whole IVM carveout.
+			 * b->base is an address of an allocation region that
+			 * lives inside the carveout.
+			 */
+			u64 offs_temp = b->base - h->base;
+			u64 peer_temp = h->vm_id;
+
+			/* Is offset NVMAP_IVM_ALIGNMENT aligned? */
+			BUG_ON(offs_temp & (NVMAP_IVM_ALIGNMENT - 1));
+
+			/* Is size PAGE aligned */
+			BUG_ON(size_temp & ~PAGE_MASK);
+
+			// Offset as multiples of NVMAP_IVM_ALIGNMENT.
+			offs_temp >>= (ffs(NVMAP_IVM_ALIGNMENT) - 1);
+
+			// Size as multiples of a PAGE.
+			size_temp >>= PAGE_SHIFT;
+
+			// Check they fit into their bitfields.
+			BUG_ON((size_temp << NVMAP_IVM_SIZE_SHIFT) &
+				~(NVMAP_IVM_SIZE_MASK));
+			BUG_ON((offs_temp << NVMAP_IVM_OFFSET_SHIFT) &
+				~(NVMAP_IVM_OFFSET_MASK));
+			BUG_ON((peer_temp << NVMAP_IVM_PEER_SHIFT) &
+				~(NVMAP_IVM_PEER_MASK));
+
+			handle->ivm_id = size_temp << NVMAP_IVM_SIZE_SHIFT;
+			handle->ivm_id |= offs_temp << NVMAP_IVM_OFFSET_SHIFT;
+			handle->ivm_id |= peer_temp << NVMAP_IVM_PEER_SHIFT;
+
+			if (h->is_ivm_vpr)
+				handle->ivm_id |= 1ULL << NVMAP_IVM_ISVPR_SHIFT;
+		}
+	}
+	mutex_unlock(&h->lock);
+	return b;
+}
+
+struct nvmap_heap *nvmap_block_to_heap(struct nvmap_heap_block *b)
+{
+	struct list_block *lb;
+	lb = container_of(b, struct list_block, block);
+	return lb->heap;
+}
+
+/* nvmap_heap_free: frees block b*/
+void nvmap_heap_free(struct nvmap_heap_block *b)
+{
+	struct nvmap_heap *h;
+	struct list_block *lb;
+
+	if (!b)
+		return;
+
+	h = nvmap_block_to_heap(b);
+	mutex_lock(&h->lock);
+
+	lb = container_of(b, struct list_block, block);
+	nvmap_flush_heap_block(NULL, b, lb->size, lb->mem_prot);
+	do_heap_free(b);
+	/*
+	 * If this HEAP has pm_ops defined and powering off the
+	 * RAM attached with the HEAP returns error, raise warning.
+	 */
+	if (h->pm_ops.idle) {
+		if (h->pm_ops.idle() < 0)
+			WARN_ON(1);
+	}
+
+	mutex_unlock(&h->lock);
+}
+
+/* nvmap_heap_create: create a heap object of len bytes, starting from
+ * address base.
+ */
+struct nvmap_heap *nvmap_heap_create(struct device *parent,
+				     const struct nvmap_platform_carveout *co,
+				     phys_addr_t base, size_t len, void *arg)
+{
+	struct nvmap_heap *h;
+
+	h = kzalloc(sizeof(*h), GFP_KERNEL);
+	if (!h) {
+		dev_err(parent, "%s: out of memory\n", __func__);
+		return NULL;
+	}
+
+	h->dma_dev = co->dma_dev;
+	if (co->cma_dev) {
+#ifdef CONFIG_DMA_CMA
+		struct dma_contiguous_stats stats;
+
+		if (dma_get_contiguous_stats(co->cma_dev, &stats))
+			goto fail;
+
+		base = stats.base;
+		len = stats.size;
+		h->cma_dev = co->cma_dev;
+#else
+		dev_err(parent, "invalid resize config for carveout %s\n",
+				co->name);
+		goto fail;
+#endif
+	} else if (!co->init_done) {
+		int err;
+
+		/* declare Non-CMA heap */
+		err = dma_declare_coherent_memory(h->dma_dev, 0, base, len,
+				DMA_MEMORY_NOMAP | DMA_MEMORY_EXCLUSIVE);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+		if (!err) {
+#else
+		if (err & DMA_MEMORY_NOMAP) {
+#endif
+			dev_info(parent,
+				"%s :dma coherent mem declare %pa,%zu\n",
+				co->name, &base, len);
+		} else {
+			dev_err(parent,
+				"%s: dma coherent declare fail %pa,%zu\n",
+				co->name, &base, len);
+			goto fail;
+		}
+	}
+
+	dev_set_name(h->dma_dev, "%s", co->name);
+	dma_set_coherent_mask(h->dma_dev, DMA_BIT_MASK(64));
+	h->name = co->name;
+	h->arg = arg;
+	h->base = base;
+	h->can_alloc = !!co->can_alloc;
+	h->is_ivm = co->is_ivm;
+	h->is_ivm_vpr = (co->usage_mask == NVMAP_HEAP_CARVEOUT_IVM_VPR);
+	BUG_ON(h->is_ivm_vpr && !h->is_ivm);
+	h->len = len;
+	h->peer = co->peer;
+	h->vm_id = co->vmid;
+
+	if (co->pm_ops.busy)
+		h->pm_ops.busy = co->pm_ops.busy;
+
+	if (co->pm_ops.idle)
+		h->pm_ops.idle = co->pm_ops.idle;
+
+	INIT_LIST_HEAD(&h->all_list);
+	mutex_init(&h->lock);
+	if (!co->no_cpu_access &&
+		nvmap_cache_maint_phys_range(NVMAP_CACHE_OP_WB_INV,
+				base, base + len, true, true)) {
+		dev_err(parent, "cache flush failed\n");
+		goto fail;
+	}
+	wmb();
+
+	if (co->disable_dynamic_dma_map)
+		nvmap_dev->dynamic_dma_map_mask &= ~co->usage_mask;
+
+	if (co->no_cpu_access)
+		nvmap_dev->cpu_access_mask &= ~co->usage_mask;
+
+	dev_info(parent, "created heap %s base 0x%p size (%zuKiB)\n",
+		co->name, (void *)(uintptr_t)base, len/1024);
+	return h;
+fail:
+	kfree(h);
+	return NULL;
+}
+
+/* nvmap_heap_destroy: frees all resources in heap */
+void nvmap_heap_destroy(struct nvmap_heap *heap)
+{
+	WARN_ON(!list_is_singular(&heap->all_list));
+	while (!list_empty(&heap->all_list)) {
+		struct list_block *l;
+		l = list_first_entry(&heap->all_list, struct list_block,
+				     all_list);
+		list_del(&l->all_list);
+		kmem_cache_free(heap_block_cache, l);
+	}
+	kfree(heap);
+}
+
+int nvmap_heap_init(void)
+{
+	ulong start_time = sched_clock();
+
+	heap_block_cache = KMEM_CACHE(list_block, 0);
+	if (!heap_block_cache) {
+		pr_err("%s: unable to create heap block cache\n", __func__);
+		return -ENOMEM;
+	}
+	pr_info("%s: created heap block cache\n", __func__);
+	nvmap_init_time += sched_clock() - start_time;
+	return 0;
+}
+
+void nvmap_heap_deinit(void)
+{
+	if (heap_block_cache)
+		kmem_cache_destroy(heap_block_cache);
+
+	heap_block_cache = NULL;
+}
+
+/*
+ * This routine is used to flush the carveout memory from cache.
+ * Why cache flush is needed for carveout? Consider the case, where a piece of
+ * carveout is allocated as cached and released. After this, if the same memory is
+ * allocated for uncached request and the memory is not flushed out from cache.
+ * In this case, the client might pass this to H/W engine and it could start modify
+ * the memory. As this was cached earlier, it might have some portion of it in cache.
+ * During cpu request to read/write other memory, the cached portion of this memory
+ * might get flushed back to main memory and would cause corruptions, if it happens
+ * after H/W writes data to memory.
+ *
+ * But flushing out the memory blindly on each carveout allocation is redundant.
+ *
+ * In order to optimize the carveout buffer cache flushes, the following
+ * strategy is used.
+ *
+ * The whole Carveout is flushed out from cache during its initialization.
+ * During allocation, carveout buffers are not flused from cache.
+ * During deallocation, carveout buffers are flushed, if they were allocated as cached.
+ * if they were allocated as uncached/writecombined, no cache flush is needed.
+ * Just draining store buffers is enough.
+ */
+int nvmap_flush_heap_block(struct nvmap_client *client,
+	struct nvmap_heap_block *block, size_t len, unsigned int prot)
+{
+	phys_addr_t phys = block->base;
+	phys_addr_t end = block->base + len;
+	int ret = 0;
+
+	if (prot == NVMAP_HANDLE_UNCACHEABLE || prot == NVMAP_HANDLE_WRITE_COMBINE)
+		goto out;
+
+	ret = nvmap_cache_maint_phys_range(NVMAP_CACHE_OP_WB_INV, phys, end,
+				true, prot != NVMAP_HANDLE_INNER_CACHEABLE);
+	if (ret)
+		goto out;
+out:
+	wmb();
+	return ret;
+}
diff --git a/drivers/video/tegra/nvmap/nvmap_heap.h b/drivers/video/tegra/nvmap/nvmap_heap.h
new file mode 100644
index 000000000000..b635eb84292f
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nvmap_heap.h
@@ -0,0 +1,57 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_heap.h
+ *
+ * GPU heap allocator.
+ *
+ * Copyright (c) 2010-2018, NVIDIA Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __NVMAP_HEAP_H
+#define __NVMAP_HEAP_H
+
+struct device;
+struct nvmap_heap;
+struct nvmap_client;
+
+struct nvmap_heap_block {
+	phys_addr_t	base;
+	unsigned int	type;
+	struct nvmap_handle *handle;
+};
+
+struct nvmap_heap *nvmap_heap_create(struct device *parent,
+				     const struct nvmap_platform_carveout *co,
+				     phys_addr_t base, size_t len, void *arg);
+
+void nvmap_heap_destroy(struct nvmap_heap *heap);
+
+struct nvmap_heap_block *nvmap_heap_alloc(struct nvmap_heap *heap,
+					  struct nvmap_handle *handle,
+					  phys_addr_t *start);
+
+struct nvmap_heap *nvmap_block_to_heap(struct nvmap_heap_block *b);
+
+void nvmap_heap_free(struct nvmap_heap_block *block);
+
+int __init nvmap_heap_init(void);
+
+void nvmap_heap_deinit(void);
+
+int nvmap_flush_heap_block(struct nvmap_client *client,
+	struct nvmap_heap_block *block, size_t len, unsigned int prot);
+
+void nvmap_heap_debugfs_init(struct dentry *heap_root, struct nvmap_heap *heap);
+
+int nvmap_query_heap_peer(struct nvmap_heap *heap);
+size_t nvmap_query_heap_size(struct nvmap_heap *heap);
+
+#endif
diff --git a/drivers/video/tegra/nvmap/nvmap_init.c b/drivers/video/tegra/nvmap/nvmap_init.c
new file mode 100644
index 000000000000..48f3727b5e05
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nvmap_init.c
@@ -0,0 +1,622 @@
+/*
+ * Copyright (c) 2014-2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt) "%s: " fmt, __func__
+
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/of_fdt.h>
+#include <linux/of_platform.h>
+#include <linux/nvmap.h>
+#include <linux/tegra-ivc.h>
+#include <linux/dma-contiguous.h>
+#include <linux/version.h>
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+#include <linux/sched/clock.h>
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0)
+#include <linux/cma.h>
+#endif
+
+#include <asm/dma-contiguous.h>
+
+#include "nvmap_priv.h"
+#include "iomap.h"
+#include "board.h"
+#include <linux/platform/tegra/common.h>
+
+#include <soc/tegra/chip-id.h>
+#if IS_ENABLED(CONFIG_TEGRA_VIRTUALIZATION)
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0)
+#include <soc/tegra/virt/syscalls.h>
+#else
+#include "../../../drivers/virt/tegra/syscalls.h"
+#endif
+#endif
+
+phys_addr_t __weak tegra_carveout_start;
+phys_addr_t __weak tegra_carveout_size;
+
+phys_addr_t __weak tegra_vpr_start;
+phys_addr_t __weak tegra_vpr_size;
+bool __weak tegra_vpr_resize;
+
+struct device __weak tegra_generic_dev;
+
+struct device __weak tegra_vpr_dev;
+EXPORT_SYMBOL(tegra_vpr_dev);
+
+struct device __weak tegra_iram_dev;
+struct device __weak tegra_generic_cma_dev;
+struct device __weak tegra_vpr_cma_dev;
+struct dma_resize_notifier_ops __weak vpr_dev_ops;
+
+__weak const struct of_device_id nvmap_of_ids[] = {
+	{ .compatible = "nvidia,carveouts" },
+	{ .compatible = "nvidia,carveouts-t18x" },
+	{ }
+};
+
+static struct dma_declare_info generic_dma_info = {
+	.name = "generic",
+	.size = 0,
+	.notifier.ops = NULL,
+};
+
+static struct dma_declare_info vpr_dma_info = {
+	.name = "vpr",
+	.size = SZ_32M,
+	.notifier.ops = &vpr_dev_ops,
+};
+
+static struct nvmap_platform_carveout nvmap_carveouts[] = {
+	[0] = {
+		.name		= "iram",
+		.usage_mask	= NVMAP_HEAP_CARVEOUT_IRAM,
+		.base		= 0,
+		.size		= 0,
+		.dma_dev	= &tegra_iram_dev,
+		.disable_dynamic_dma_map = true,
+	},
+	[1] = {
+		.name		= "generic-0",
+		.usage_mask	= NVMAP_HEAP_CARVEOUT_GENERIC,
+		.base		= 0,
+		.size		= 0,
+		.dma_dev	= &tegra_generic_dev,
+		.cma_dev	= &tegra_generic_cma_dev,
+		.dma_info	= &generic_dma_info,
+	},
+	/* This is the "classic" VPR from pre-virtualization days, intended to
+	 * be used by a native OS.
+	 */
+	[2] = {
+		.name		= "vpr",
+		.usage_mask	= NVMAP_HEAP_CARVEOUT_VPR,
+		.base		= 0,
+		.size		= 0,
+		.dma_dev	= &tegra_vpr_dev,
+		.cma_dev	= &tegra_vpr_cma_dev,
+		.dma_info	= &vpr_dma_info,
+		.enable_static_dma_map = true,
+	},
+	[3] = {
+		.name		= "vidmem",
+		.usage_mask	= NVMAP_HEAP_CARVEOUT_VIDMEM,
+		.base		= 0,
+		.size		= 0,
+		.disable_dynamic_dma_map = true,
+		.no_cpu_access = true,
+	},
+	/* This is a VPR-backed IVM carveout. It is intended to be used in a
+	 * cross-VM setup, for example, VM0 being a producer and VM1 being a
+	 * consumer.
+	 */
+	[4] = {
+		.name		= NULL,
+		.usage_mask	= NVMAP_HEAP_CARVEOUT_IVM_VPR,
+	},
+	/* Need uninitialized entries for IVM carveouts */
+	[5] = {
+		.name		= NULL,
+		.usage_mask	= NVMAP_HEAP_CARVEOUT_IVM,
+	},
+	[6] = {
+		.name		= NULL,
+		.usage_mask	= NVMAP_HEAP_CARVEOUT_IVM,
+	},
+	[7] = {
+		.name		= NULL,
+		.usage_mask	= NVMAP_HEAP_CARVEOUT_IVM,
+	},
+	[8] = {
+		.name		= NULL,
+		.usage_mask	= NVMAP_HEAP_CARVEOUT_IVM,
+	},
+};
+
+static struct nvmap_platform_data nvmap_data = {
+	.carveouts	= nvmap_carveouts,
+	.nr_carveouts	= 5,
+};
+
+static struct nvmap_platform_carveout *nvmap_get_carveout_pdata(const char *name)
+{
+	struct nvmap_platform_carveout *co;
+	for (co = nvmap_carveouts;
+	     co < nvmap_carveouts + ARRAY_SIZE(nvmap_carveouts); co++) {
+		int i = min_t(int, strcspn(name, "_"), strcspn(name, "-"));
+		if (co->usage_mask == NVMAP_HEAP_CARVEOUT_IVM_VPR)
+			continue;
+
+		/* handle IVM carveouts */
+		if ((co->usage_mask == NVMAP_HEAP_CARVEOUT_IVM) &&  !co->name)
+			goto found;
+
+		if (strncmp(co->name, name, i))
+			continue;
+found:
+		co->dma_dev = co->dma_dev ? co->dma_dev : &co->dev;
+		return co;
+	}
+	pr_err("not enough space for all nvmap carveouts\n");
+	return NULL;
+}
+
+int nvmap_register_vidmem_carveout(struct device *dma_dev,
+				phys_addr_t base, size_t size)
+{
+	struct nvmap_platform_carveout *vidmem_co;
+
+	if (!base || !size || (base != PAGE_ALIGN(base)) ||
+	    (size != PAGE_ALIGN(size)))
+		return -EINVAL;
+
+	vidmem_co = nvmap_get_carveout_pdata("vidmem");
+	if (!vidmem_co)
+		return -ENODEV;
+
+	if (vidmem_co->base || vidmem_co->size)
+		return -EEXIST;
+
+	vidmem_co->base = base;
+	vidmem_co->size = size;
+	if (dma_dev)
+		vidmem_co->dma_dev = dma_dev;
+	return nvmap_create_carveout(vidmem_co);
+}
+EXPORT_SYMBOL(nvmap_register_vidmem_carveout);
+
+#if IS_ENABLED(CONFIG_TEGRA_VIRTUALIZATION)
+int __init nvmap_populate_ivm_carveout(struct reserved_mem *rmem)
+{
+	u32 id;
+	struct tegra_hv_ivm_cookie *ivm;
+	struct nvmap_platform_carveout *co;
+	unsigned int guestid;
+	unsigned long fdt_node = rmem->fdt_node;
+	const __be32 *prop;
+	int len;
+	int ret = 0;
+
+	co = nvmap_get_carveout_pdata(rmem->name);
+	if (!co)
+		return -ENOMEM;
+
+	if (hyp_read_gid(&guestid)) {
+		pr_err("failed to read gid\n");
+		return -EINVAL;
+	}
+
+	prop = of_get_flat_dt_prop(fdt_node, "ivm", &len);
+	if (!prop) {
+		pr_err("failed to read ivm property\n");
+		return -EINVAL;
+	}
+
+	id = of_read_number(prop + 1, 1);
+	ivm = tegra_hv_mempool_reserve(id);
+	if (IS_ERR_OR_NULL(ivm)) {
+		pr_err("failed to reserve IVM memory pool %d\n", id);
+		return -ENOMEM;
+	}
+
+	/* XXX: Are these the available fields from IVM cookie? */
+	co->base     = (phys_addr_t)ivm->ipa;
+	co->peer     = ivm->peer_vmid;
+	co->size     = ivm->size;
+	co->vmid     = (int)guestid;
+
+	if (!co->base || !co->size) {
+		ret = -EINVAL;
+		goto fail;
+	}
+
+	/* See if this VM can allocate (or just create handle from ID)
+	 * generated by peer partition */
+	prop = of_get_flat_dt_prop(fdt_node, "alloc", &len);
+	if (!prop) {
+		pr_err("failed to read alloc property\n");
+		ret = -EINVAL;
+		goto fail;
+	}
+
+	co->can_alloc = of_read_number(prop, 1);
+	co->is_ivm    = true;
+
+	co->name = kasprintf(GFP_KERNEL, "ivm%02d%02d%02d", co->vmid, co->peer,
+		co->can_alloc);
+	if (!co->name) {
+		ret = -ENOMEM;
+		goto fail;
+	}
+
+	pr_info("IVM carveout IPA:%p, size=%zu, peer vmid=%d, name=%s\n",
+		(void *)(uintptr_t)co->base, co->size, co->peer, co->name);
+
+	nvmap_data.nr_carveouts++;
+
+	return 0;
+
+fail:
+	co->base     = 0;
+	co->peer     = 0;
+	co->size     = 0;
+	co->vmid     = 0;
+	tegra_hv_mempool_unreserve(ivm);
+
+	return ret;
+}
+
+int __init nvmap_populate_ivm_vpr_carveout(
+		struct nvmap_platform_carveout *co)
+{
+	unsigned int guestid;
+	struct tegra_hv_ivm_cookie *ivm;
+	int ret = 0;
+
+	if (!co)
+		return -ENOMEM;
+
+	if (hyp_read_gid(&guestid)) {
+		pr_err("failed to read gid\n");
+		return -EINVAL;
+	}
+
+	ivm = tegra_hv_mempool_reserve_vpr();
+	if (IS_ERR_OR_NULL(ivm)) {
+		if (PTR_ERR(ivm) == -ENODEV) {
+			/* Skip silently if there is no IVM-VPR defined */
+			return 0;
+		}
+
+		if (PTR_ERR(ivm) == -EPROBE_DEFER)
+			pr_err("mempool query while not ready yet\n");
+		else
+			pr_err("failed to reserve VPR memory pool\n");
+
+		return -ENOMEM;
+	}
+
+	co->base = (phys_addr_t)ivm->ipa;
+	co->peer = ivm->peer_vmid;
+	co->size = ivm->size;
+	co->vmid = (int)guestid;
+
+	if (!co->base || !co->size) {
+		ret = -EINVAL;
+		goto fail;
+	}
+
+	co->can_alloc = ivm->can_alloc;
+	co->is_ivm = true;
+	co->dma_dev = &co->dev;
+
+	co->name = kasprintf(GFP_KERNEL, "ivmvpr%02d%02d%02d", co->vmid,
+		co->peer, co->can_alloc);
+	if (!co->name) {
+		ret = -ENOMEM;
+		goto fail;
+	}
+
+	pr_info("IVM-VPR carveout IPA:%p, size=%zu, peer vmid=%d, name=%s\n",
+		(void *)(uintptr_t)co->base, co->size, co->peer, co->name);
+
+	return 0;
+
+fail:
+	co->base = 0;
+	co->peer = 0;
+	co->size = 0;
+	co->vmid = 0;
+	tegra_hv_mempool_unreserve(ivm);
+
+	return ret;
+}
+#else
+int __init nvmap_populate_ivm_carveout(struct reserved_mem *rmem)
+{
+	return -EINVAL;
+}
+
+int __init nvmap_populate_ivm_vpr_carveout(
+		struct nvmap_platform_carveout *co)
+{
+	return -EINVAL;
+}
+#endif
+
+static int __nvmap_init_legacy(struct device *dev);
+static int __nvmap_init_dt(struct platform_device *pdev)
+{
+	if (!of_match_device(nvmap_of_ids, &pdev->dev)) {
+		pr_err("Missing DT entry!\n");
+		return -EINVAL;
+	}
+
+	/* For VM_2 we need carveout. So, enabling it here */
+	__nvmap_init_legacy(&pdev->dev);
+
+	pdev->dev.platform_data = &nvmap_data;
+
+	return 0;
+}
+
+static int __init nvmap_co_device_init(struct reserved_mem *rmem,
+					struct device *dev)
+{
+	struct nvmap_platform_carveout *co = rmem->priv;
+	int err;
+
+	if (!co)
+		return -ENODEV;
+
+	if (co->usage_mask == NVMAP_HEAP_CARVEOUT_IVM)
+		return nvmap_populate_ivm_carveout(rmem);
+
+	if (co->usage_mask == NVMAP_HEAP_CARVEOUT_IVM_VPR)
+		return nvmap_populate_ivm_vpr_carveout(co);
+
+	/* if co size is 0, => co is not present. So, skip init. */
+	if (!co->size)
+		return 0;
+
+	if (!co->cma_dev) {
+		err = dma_declare_coherent_memory(co->dma_dev, 0,
+				co->base, co->size,
+				DMA_MEMORY_NOMAP | DMA_MEMORY_EXCLUSIVE);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+		if (!err) {
+#else
+		if (err & DMA_MEMORY_NOMAP) {
+#endif
+			dev_info(dev,
+				 "%s :dma coherent mem declare %pa,%zu\n",
+				 co->name, &co->base, co->size);
+			co->init_done = true;
+			err = 0;
+		} else
+			dev_err(dev,
+				"%s :dma coherent mem declare fail %pa,%zu,err:%d\n",
+				co->name, &co->base, co->size, err);
+	} else {
+		/*
+		 * When vpr memory is reserved, kmemleak tries to scan vpr
+		 * memory for pointers. vpr memory should not be accessed
+		 * from cpu so avoid scanning it. When vpr memory is removed,
+		 * the memblock_remove() API ensures that kmemleak won't scan
+		 * a removed block.
+		 */
+		if (!strncmp(co->name, "vpr", 3))
+			kmemleak_no_scan(__va(co->base));
+
+		co->dma_info->cma_dev = co->cma_dev;
+		err = dma_declare_coherent_resizable_cma_memory(
+				co->dma_dev, co->dma_info);
+		if (err)
+			dev_err(dev, "%s coherent memory declaration failed\n",
+				     co->name);
+		else
+			co->init_done = true;
+	}
+	return err;
+}
+
+static void nvmap_co_device_release(struct reserved_mem *rmem,struct device *dev)
+{
+	struct nvmap_platform_carveout *co = rmem->priv;
+
+	if (!co)
+		return;
+
+	if (co->usage_mask == NVMAP_HEAP_CARVEOUT_IVM)
+		kfree(co->name);
+}
+
+static const struct reserved_mem_ops nvmap_co_ops = {
+	.device_init	= nvmap_co_device_init,
+	.device_release	= nvmap_co_device_release,
+};
+
+int __init nvmap_co_setup(struct reserved_mem *rmem)
+{
+	struct nvmap_platform_carveout *co;
+	int ret = 0;
+	struct cma *cma;
+	ulong start = sched_clock();
+
+	co = nvmap_get_carveout_pdata(rmem->name);
+	if (!co)
+		return ret;
+
+	rmem->ops = &nvmap_co_ops;
+	rmem->priv = co;
+
+	/* IVM carveouts */
+	if (!co->name)
+		goto finish;
+
+	co->base = rmem->base;
+	co->size = rmem->size;
+
+	if (!of_get_flat_dt_prop(rmem->fdt_node, "reusable", NULL) ||
+	    of_get_flat_dt_prop(rmem->fdt_node, "no-map", NULL))
+		goto skip_cma;
+
+	WARN_ON(!rmem->base);
+	if (dev_get_cma_area(co->cma_dev)) {
+		pr_info("cma area initialed in legacy way already\n");
+		goto finish;
+	}
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	ret = cma_init_reserved_mem(rmem->base, rmem->size, 0,
+					rmem->name, &cma);
+#else
+	ret = cma_init_reserved_mem(rmem->base, rmem->size, 0, &cma);
+#endif
+	if (ret) {
+		pr_info("cma_init_reserved_mem fails for %s\n", rmem->name);
+		goto finish;
+	}
+
+	dma_contiguous_early_fixup(rmem->base, rmem->size);
+	dev_set_cma_area(co->cma_dev, cma);
+	pr_debug("tegra-carveouts carveout=%s %pa@%pa\n",
+		 rmem->name, &rmem->size, &rmem->base);
+	goto finish;
+
+skip_cma:
+	co->cma_dev = NULL;
+finish:
+	nvmap_init_time += sched_clock() - start;
+	return ret;
+}
+RESERVEDMEM_OF_DECLARE(nvmap_co, "nvidia,generic_carveout", nvmap_co_setup);
+RESERVEDMEM_OF_DECLARE(nvmap_ivm_co, "nvidia,ivm_carveout", nvmap_co_setup);
+RESERVEDMEM_OF_DECLARE(nvmap_iram_co, "nvidia,iram-carveout", nvmap_co_setup);
+
+/* "Classic" VPR is not supported in virtualized systems. */
+#if !IS_ENABLED(CONFIG_TEGRA_VIRTUALIZATION)
+RESERVEDMEM_OF_DECLARE(nvmap_vpr_co, "nvidia,vpr-carveout", nvmap_co_setup);
+#endif
+
+/*
+ * This requires proper kernel arguments to have been passed.
+ */
+static int __nvmap_init_legacy(struct device *dev)
+{
+	/* Carveout. */
+	if (!nvmap_carveouts[1].base) {
+		nvmap_carveouts[1].base = tegra_carveout_start;
+		nvmap_carveouts[1].size = tegra_carveout_size;
+		if (!tegra_vpr_resize)
+			nvmap_carveouts[1].cma_dev = NULL;
+	}
+
+	/* VPR */
+	if (!nvmap_carveouts[2].base) {
+		nvmap_carveouts[2].base = tegra_vpr_start;
+		nvmap_carveouts[2].size = tegra_vpr_size;
+		if (!tegra_vpr_resize)
+			nvmap_carveouts[2].cma_dev = NULL;
+	}
+
+	return 0;
+}
+
+/*
+ * Fills in the platform data either from the device tree or with the
+ * legacy path.
+ */
+int __init nvmap_init(struct platform_device *pdev)
+{
+	int err;
+	struct reserved_mem rmem;
+
+	if (pdev->dev.of_node) {
+		err = __nvmap_init_dt(pdev);
+		if (err)
+			return err;
+	}
+
+	err = of_reserved_mem_device_init(&pdev->dev);
+	if (err)
+		pr_debug("reserved_mem_device_init fails, try legacy init\n");
+
+	/* try legacy init */
+	if (!nvmap_carveouts[1].init_done) {
+		rmem.priv = &nvmap_carveouts[1];
+		err = nvmap_co_device_init(&rmem, &pdev->dev);
+		if (err)
+			goto end;
+	}
+
+	/* VPR */
+	if (!nvmap_carveouts[2].init_done) {
+		rmem.priv = &nvmap_carveouts[2];
+		err = nvmap_co_device_init(&rmem, &pdev->dev);
+		if (err)
+			goto end;
+	}
+
+	/* IVM-VPR */
+	if (is_tegra_hypervisor_mode())
+		if (!nvmap_carveouts[4].init_done) {
+			rmem.priv = &nvmap_carveouts[4];
+			err = nvmap_co_device_init(&rmem, &pdev->dev);
+		}
+end:
+	return err;
+}
+
+static struct platform_driver __refdata nvmap_driver = {
+	.probe		= nvmap_probe,
+	.remove		= nvmap_remove,
+
+	.driver = {
+		.name	= "tegra-carveouts",
+		.owner	= THIS_MODULE,
+		.of_match_table = nvmap_of_ids,
+		.probe_type = PROBE_PREFER_ASYNCHRONOUS,
+		.suppress_bind_attrs = true,
+	},
+};
+
+static int __init nvmap_init_driver(void)
+{
+	int e = 0;
+
+	e = nvmap_heap_init();
+	if (e)
+		goto fail;
+
+	e = platform_driver_register(&nvmap_driver);
+	if (e) {
+		nvmap_heap_deinit();
+		goto fail;
+	}
+
+fail:
+	return e;
+}
+fs_initcall(nvmap_init_driver);
+
+static void __exit nvmap_exit_driver(void)
+{
+	platform_driver_unregister(&nvmap_driver);
+	nvmap_heap_deinit();
+	nvmap_dev = NULL;
+}
+module_exit(nvmap_exit_driver);
diff --git a/drivers/video/tegra/nvmap/nvmap_init_t19x.c b/drivers/video/tegra/nvmap/nvmap_init_t19x.c
new file mode 100644
index 000000000000..ef430b95cc60
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nvmap_init_t19x.c
@@ -0,0 +1,308 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_init_t19x.c
+ *
+ * Copyright (c) 2016-2021, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"nvmap: %s() " fmt, __func__
+
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/of_reserved_mem.h>
+#include <linux/platform_device.h>
+#include <linux/nvmap_t19x.h>
+
+#include "nvmap_priv.h"
+
+bool nvmap_version_t19x;
+
+const struct of_device_id nvmap_of_ids[] = {
+	{ .compatible = "nvidia,carveouts" },
+	{ .compatible = "nvidia,carveouts-t18x" },
+	{ .compatible = "nvidia,carveouts-t19x" },
+	{ }
+};
+
+int nvmap_register_cvsram_carveout(struct device *dma_dev,
+		phys_addr_t base, size_t size, int (*busy)(void),
+		int (*idle)(void))
+{
+	static struct nvmap_platform_carveout cvsram = {
+		.name = "cvsram",
+		.usage_mask = NVMAP_HEAP_CARVEOUT_CVSRAM,
+		.disable_dynamic_dma_map = true,
+		.no_cpu_access = true,
+	};
+
+	cvsram.pm_ops.busy = busy;
+	cvsram.pm_ops.idle = idle;
+
+	if (!base || !size || (base != PAGE_ALIGN(base)) ||
+	    (size != PAGE_ALIGN(size)))
+		return -EINVAL;
+	cvsram.base = base;
+	cvsram.size = size;
+
+	cvsram.dma_dev = &cvsram.dev;
+	return nvmap_create_carveout(&cvsram);
+}
+EXPORT_SYMBOL(nvmap_register_cvsram_carveout);
+
+static struct nvmap_platform_carveout gosmem = {
+	.name = "gosmem",
+	.usage_mask = NVMAP_HEAP_CARVEOUT_GOS,
+};
+
+static struct cv_dev_info *cvdev_info;
+static int count = 0;
+
+static void nvmap_gosmem_device_release(struct reserved_mem *rmem,
+		struct device *dev)
+{
+	int i;
+	struct reserved_mem_ops *rmem_ops =
+		(struct reserved_mem_ops *)rmem->ops;
+
+	for (i = 0; i < count; i++)
+		of_node_put(cvdev_info[i].np);
+	kfree(cvdev_info);
+	rmem_ops->device_release(rmem, dev);
+}
+
+static int __init nvmap_gosmem_device_init(struct reserved_mem *rmem,
+		struct device *dev)
+{
+	struct of_phandle_args outargs;
+	struct device_node *np;
+	DEFINE_DMA_ATTRS(attrs);
+	dma_addr_t dma_addr;
+	void *cpu_addr;
+	int ret = 0, i, idx, bytes;
+	struct reserved_mem_ops *rmem_ops =
+		(struct reserved_mem_ops *)rmem->priv;
+	struct sg_table *sgt;
+
+	dma_set_attr(DMA_ATTR_ALLOC_EXACT_SIZE, __DMA_ATTR(attrs));
+
+	np = of_find_node_by_phandle(rmem->phandle);
+	if (!np) {
+		pr_err("Can't find the node using compatible\n");
+		return -ENODEV;
+	}
+
+	if (count) {
+		pr_err("Gosmem initialized already\n");
+		return -EBUSY;
+	}
+
+	count = of_count_phandle_with_args(np, "cvdevs", NULL);
+	if (!count) {
+		pr_err("No cvdevs to use the gosmem!!\n");
+		return -EINVAL;
+	}
+
+	cpu_addr = dma_alloc_coherent(gosmem.dma_dev, count * SZ_4K,
+				&dma_addr, GFP_KERNEL);
+	if (!cpu_addr) {
+		pr_err("Failed to allocate from Gos mem carveout\n");
+		return -ENOMEM;
+	}
+
+	bytes = sizeof(*cvdev_info) * count;
+	bytes += sizeof(struct sg_table) * count * count;
+	cvdev_info = kzalloc(bytes, GFP_KERNEL);
+	if (!cvdev_info) {
+		pr_err("kzalloc failed. No memory!!!\n");
+		ret = -ENOMEM;
+		goto unmap_dma;
+	}
+
+	for (idx = 0; idx < count; idx++) {
+		struct device_node *temp;
+
+		ret = of_parse_phandle_with_args(np, "cvdevs",
+			NULL, idx, &outargs);
+		if (ret < 0) {
+			/* skip empty (null) phandles */
+			if (ret == -ENOENT)
+				continue;
+			else
+				goto free_cvdev;
+		}
+		temp = outargs.np;
+
+		cvdev_info[idx].np = of_node_get(temp);
+		if (!cvdev_info[idx].np)
+			continue;
+		cvdev_info[idx].count = count;
+		cvdev_info[idx].idx = idx;
+		cvdev_info[idx].sgt =
+			(struct sg_table *)(cvdev_info + count);
+		cvdev_info[idx].sgt += idx * count;
+		cvdev_info[idx].cpu_addr = cpu_addr + idx * SZ_4K;
+
+		for (i = 0; i < count; i++) {
+			sgt = cvdev_info[idx].sgt + i;
+
+			ret = sg_alloc_table(sgt, 1, GFP_KERNEL);
+			if (ret) {
+				pr_err("sg_alloc_table failed:%d\n", ret);
+				goto free;
+			}
+			sg_set_page(sgt->sgl, virt_to_page(cpu_addr + i * SZ_4K),
+					      SZ_4K,
+					      offset_in_page(cpu_addr + i * SZ_4K));
+
+		}
+	}
+	rmem->priv = &gosmem;
+	ret = rmem_ops->device_init(rmem, dev);
+	if (ret)
+		goto free;
+	return ret;
+free:
+	sgt = (struct sg_table *)(cvdev_info + count);
+	for (i = 0; i < count * count; i++)
+		sg_free_table(sgt++);
+free_cvdev:
+	kfree(cvdev_info);
+unmap_dma:
+	dma_free_coherent(gosmem.dma_dev, count * SZ_4K, cpu_addr, dma_addr);
+	return ret;
+}
+
+static struct reserved_mem_ops gosmem_rmem_ops = {
+	.device_init = nvmap_gosmem_device_init,
+	.device_release = nvmap_gosmem_device_release,
+};
+
+static int __init nvmap_gosmem_setup(struct reserved_mem *rmem)
+{
+	int ret;
+
+	rmem->priv = &gosmem;
+	ret = nvmap_co_setup(rmem);
+	if (ret)
+		return ret;
+
+	rmem->priv = (struct reserved_mem_ops *)rmem->ops;
+	rmem->ops = &gosmem_rmem_ops;
+	return 0;
+}
+RESERVEDMEM_OF_DECLARE(nvmap_co, "nvidia,gosmem", nvmap_gosmem_setup);
+
+struct cv_dev_info *nvmap_fetch_cv_dev_info(struct device *dev);
+
+static int nvmap_gosmem_notifier(struct notifier_block *nb,
+		unsigned long event, void *_dev)
+{
+	struct device *dev = _dev;
+	int ents, i, ret;
+	struct cv_dev_info *gos_owner;
+
+	if ((event != BUS_NOTIFY_BOUND_DRIVER) &&
+		(event != BUS_NOTIFY_UNBIND_DRIVER))
+		return NOTIFY_DONE;
+
+	if ((event == BUS_NOTIFY_BOUND_DRIVER) &&
+		nvmap_dev && (dev == nvmap_dev->dev_user.parent)) {
+		struct of_device_id nvmap_t19x_of_ids[] = {
+			{.compatible = "nvidia,carveouts-t19x"},
+			{ }
+		};
+
+		/*
+		 * user space IOCTL and dmabuf ops happen much later in boot
+		 * flow. So, setting the version here to ensure all of those
+		 * callbacks can safely query the proper version of nvmap
+		 */
+		if (of_match_node((struct of_device_id *)&nvmap_t19x_of_ids,
+				dev->of_node)) {
+			nvmap_version_t19x = 1;
+			/* FIX ME: Update correct value after evaluation */
+			nvmap_cache_maint_by_set_ways = 0;
+			cache_maint_inner_threshold = SZ_2M;
+		}
+
+		return NOTIFY_DONE;
+	}
+
+	gos_owner = nvmap_fetch_cv_dev_info(dev);
+	if (!gos_owner)
+		return NOTIFY_DONE;
+
+	ret = _dma_declare_coherent_memory(&gos_owner->offset_dev, 0, 0, SZ_256,
+				ffs(sizeof(u32)) - ffs(sizeof(u8)), DMA_MEMORY_NOMAP);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+	if (ret) {
+#else
+	if (!(ret & DMA_MEMORY_NOMAP)) {
+#endif
+		dev_err(dev, "declare coherent memory for gosmem chunk failed\n");
+		return NOTIFY_DONE;
+	}
+
+	for (i = 0; i < count; i++) {
+		DEFINE_DMA_ATTRS(attrs);
+		enum dma_data_direction dir;
+
+		dir = DMA_BIDIRECTIONAL;
+		dma_set_attr(DMA_ATTR_SKIP_IOVA_GAP, __DMA_ATTR(attrs));
+		if (cvdev_info[i].np != dev->of_node) {
+			dma_set_attr(DMA_ATTR_READ_ONLY, __DMA_ATTR(attrs));
+			dir = DMA_TO_DEVICE;
+		}
+
+		switch (event) {
+		case BUS_NOTIFY_BOUND_DRIVER:
+			ents = dma_map_sg_attrs(dev, gos_owner->sgt[i].sgl,
+					gos_owner->sgt[i].nents, dir, __DMA_ATTR(attrs));
+			if (ents != 1) {
+				pr_err("mapping gosmem chunk %d for %s failed\n",
+					i, dev_name(dev));
+				return NOTIFY_DONE;
+			}
+			break;
+		case BUS_NOTIFY_UNBIND_DRIVER:
+			dma_unmap_sg_attrs(dev, gos_owner->sgt[i].sgl,
+					gos_owner->sgt[i].nents, dir, __DMA_ATTR(attrs));
+			break;
+		default:
+			return NOTIFY_DONE;
+		};
+	}
+	return NOTIFY_DONE;
+}
+
+static struct notifier_block nvmap_gosmem_nb = {
+	.notifier_call = nvmap_gosmem_notifier,
+};
+
+static int nvmap_t19x_init(void)
+{
+	return bus_register_notifier(&platform_bus_type,
+			&nvmap_gosmem_nb);
+}
+core_initcall(nvmap_t19x_init);
+
+struct cv_dev_info *nvmap_fetch_cv_dev_info(struct device *dev)
+{
+	int i;
+
+	if (!dev || !cvdev_info || !dev->of_node)
+		return NULL;
+
+	for (i = 0; i < count; i++)
+		if (cvdev_info[i].np == dev->of_node)
+			return &cvdev_info[i];
+	return NULL;
+}
diff --git a/drivers/video/tegra/nvmap/nvmap_ioctl.c b/drivers/video/tegra/nvmap/nvmap_ioctl.c
new file mode 100644
index 000000000000..a6c4544d91de
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nvmap_ioctl.c
@@ -0,0 +1,1057 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_ioctl.c
+ *
+ * User-space interface to nvmap
+ *
+ * Copyright (c) 2011-2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"nvmap: %s() " fmt, __func__
+
+#include <linux/dma-mapping.h>
+#include <linux/export.h>
+#include <linux/fs.h>
+#include <linux/io.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/nvmap.h>
+#include <linux/vmalloc.h>
+#include <linux/highmem.h>
+#include <linux/mm.h>
+
+#include <asm/io.h>
+#include <asm/memory.h>
+#include <asm/uaccess.h>
+#include <soc/tegra/common.h>
+#include <trace/events/nvmap.h>
+
+#include "nvmap_ioctl.h"
+#include "nvmap_priv.h"
+#include "nvmap_heap.h"
+
+
+extern struct device tegra_vpr_dev;
+
+static ssize_t rw_handle(struct nvmap_client *client, struct nvmap_handle *h,
+			 int is_read, unsigned long h_offs,
+			 unsigned long sys_addr, unsigned long h_stride,
+			 unsigned long sys_stride, unsigned long elem_size,
+			 unsigned long count);
+
+struct nvmap_handle *nvmap_handle_get_from_fd(int fd)
+{
+	struct nvmap_handle *h;
+
+	h = nvmap_handle_get_from_dmabuf_fd(NULL, fd);
+	if (!IS_ERR(h))
+		return h;
+	return NULL;
+}
+
+static int nvmap_install_fd(struct nvmap_client *client,
+	struct nvmap_handle *handle, int fd, void __user *arg,
+	void *op, size_t op_size, bool free, struct dma_buf *dmabuf)
+{
+	int err = 0;
+
+	if (IS_ERR_VALUE((uintptr_t)fd)) {
+		err = fd;
+		goto fd_fail;
+	}
+
+	if (copy_to_user(arg, op, op_size)) {
+		err = -EFAULT;
+		goto copy_fail;
+	}
+
+	fd_install(fd, dmabuf->file);
+	return err;
+
+copy_fail:
+	put_unused_fd(fd);
+fd_fail:
+	if (dmabuf)
+		dma_buf_put(dmabuf);
+	if (free && handle)
+		nvmap_free_handle(client, handle);
+	return err;
+}
+
+int nvmap_ioctl_getfd(struct file *filp, void __user *arg)
+{
+	struct nvmap_handle *handle;
+	struct nvmap_create_handle op;
+	struct nvmap_client *client = filp->private_data;
+	struct dma_buf *dmabuf;
+
+	if (copy_from_user(&op, arg, sizeof(op)))
+		return -EFAULT;
+
+	handle = nvmap_handle_get_from_fd(op.handle);
+	if (handle) {
+		op.fd = nvmap_get_dmabuf_fd(client, handle);
+		nvmap_handle_put(handle);
+		dmabuf = IS_ERR_VALUE((uintptr_t)op.fd) ? NULL : handle->dmabuf;
+	} else {
+		/* if we get an error, the fd might be non-nvmap dmabuf fd */
+		dmabuf = dma_buf_get(op.handle);
+		if (IS_ERR(dmabuf))
+			return PTR_ERR(dmabuf);
+		op.fd = nvmap_dmabuf_duplicate_gen_fd(client, dmabuf);
+	}
+
+	return nvmap_install_fd(client, handle,
+				op.fd, arg, &op, sizeof(op), 0, dmabuf);
+}
+
+int nvmap_ioctl_alloc(struct file *filp, void __user *arg)
+{
+	struct nvmap_alloc_handle op;
+	struct nvmap_client *client = filp->private_data;
+	struct nvmap_handle *handle;
+	int err;
+
+	if (copy_from_user(&op, arg, sizeof(op)))
+		return -EFAULT;
+
+	if (op.align & (op.align - 1))
+		return -EINVAL;
+
+	handle = nvmap_handle_get_from_fd(op.handle);
+	if (!handle)
+		return -EINVAL;
+
+	if (!nvmap_memory_available(handle->size))
+		return -ENOMEM;
+
+	/* user-space handles are aligned to page boundaries, to prevent
+	 * data leakage. */
+	op.align = max_t(size_t, op.align, PAGE_SIZE);
+
+	err = nvmap_alloc_handle(client, handle, op.heap_mask, op.align,
+				  0, /* no kind */
+				  op.flags & (~NVMAP_HANDLE_KIND_SPECIFIED),
+				  NVMAP_IVM_INVALID_PEER);
+	nvmap_handle_put(handle);
+	return err;
+}
+
+int nvmap_ioctl_alloc_ivm(struct file *filp, void __user *arg)
+{
+	struct nvmap_alloc_ivm_handle op;
+	struct nvmap_client *client = filp->private_data;
+	struct nvmap_handle *handle;
+	int err;
+
+	if (copy_from_user(&op, arg, sizeof(op)))
+		return -EFAULT;
+
+	if (op.align & (op.align - 1))
+		return -EINVAL;
+
+	handle = nvmap_handle_get_from_fd(op.handle);
+	if (!handle)
+		return -EINVAL;
+
+	/* user-space handles are aligned to page boundaries, to prevent
+	 * data leakage. */
+	op.align = max_t(size_t, op.align, PAGE_SIZE);
+
+	err = nvmap_alloc_handle(client, handle, op.heap_mask, op.align,
+				  0, /* no kind */
+				  op.flags & (~NVMAP_HANDLE_KIND_SPECIFIED),
+				  op.peer);
+	nvmap_handle_put(handle);
+	return err;
+}
+
+int nvmap_ioctl_vpr_floor_size(struct file *filp, void __user *arg)
+{
+	int err=0;
+	u32 floor_size;
+
+	if (copy_from_user(&floor_size, arg, sizeof(floor_size)))
+		return -EFAULT;
+
+	err = dma_set_resizable_heap_floor_size(&tegra_vpr_dev, floor_size);
+	return err;
+}
+
+int nvmap_ioctl_create(struct file *filp, unsigned int cmd, void __user *arg)
+{
+	struct nvmap_create_handle op;
+	struct nvmap_handle_ref *ref = NULL;
+	struct nvmap_client *client = filp->private_data;
+	struct dma_buf *dmabuf = NULL;
+	struct nvmap_handle *handle = NULL;
+	int fd;
+
+	if (copy_from_user(&op, arg, sizeof(op)))
+		return -EFAULT;
+
+	if (!client)
+		return -ENODEV;
+
+	if (cmd == NVMAP_IOC_CREATE)
+		op.size64 = op.size;
+
+	if ((cmd == NVMAP_IOC_CREATE) || (cmd == NVMAP_IOC_CREATE_64)) {
+		ref = nvmap_create_handle(client, op.size64, false);
+		if (!IS_ERR(ref))
+			ref->handle->orig_size = op.size64;
+	} else if (cmd == NVMAP_IOC_FROM_FD) {
+		ref = nvmap_create_handle_from_fd(client, op.fd);
+
+		/* if we get an error, the fd might be non-nvmap dmabuf fd */
+		if (IS_ERR(ref)) {
+			dmabuf = dma_buf_get(op.fd);
+			if (IS_ERR(dmabuf))
+				return PTR_ERR(dmabuf);
+			fd = nvmap_dmabuf_duplicate_gen_fd(client, dmabuf);
+			if (fd < 0)
+				return fd;
+		}
+	} else {
+		return -EINVAL;
+	}
+
+	if (!IS_ERR(ref)) {
+		handle = ref->handle;
+		dmabuf = handle->dmabuf;
+		fd = nvmap_get_dmabuf_fd(client, ref->handle);
+	} else if (!dmabuf) {
+		return PTR_ERR(ref);
+	}
+
+	if (cmd == NVMAP_IOC_CREATE_64)
+		op.handle64 = fd;
+	else
+		op.handle = fd;
+	return nvmap_install_fd(client, handle, fd,
+				arg, &op, sizeof(op), 1, dmabuf);
+}
+
+int nvmap_ioctl_create_from_va(struct file *filp, void __user *arg)
+{
+	int fd;
+	int err;
+	struct nvmap_create_handle_from_va op;
+	struct nvmap_handle_ref *ref = NULL;
+	struct nvmap_client *client = filp->private_data;
+
+	if (copy_from_user(&op, arg, sizeof(op)))
+		return -EFAULT;
+
+	if (!client)
+		return -ENODEV;
+
+	ref = nvmap_create_handle_from_va(client, op.va,
+			op.size ? op.size : op.size64,
+			op.flags);
+	if (IS_ERR(ref))
+		return PTR_ERR(ref);
+
+	err = nvmap_alloc_handle_from_va(client, ref->handle,
+					 op.va, op.flags);
+	if (err) {
+		nvmap_free_handle(client, ref->handle);
+		return err;
+	}
+
+	fd = nvmap_get_dmabuf_fd(client, ref->handle);
+	op.handle = fd;
+	return nvmap_install_fd(client, ref->handle, fd,
+			arg, &op, sizeof(op), 1,  ref->handle->dmabuf);
+}
+
+static int set_vpr_fail_data(void *user_addr, ulong user_stride,
+		       ulong elem_size, ulong count)
+{
+	int ret = 0;
+	void *vaddr;
+
+	vaddr = vmalloc(PAGE_SIZE);
+	if (!vaddr)
+		return -ENOMEM;
+	memset(vaddr, 0xFF, PAGE_SIZE);
+
+	while (!ret && count--) {
+		ulong size_to_copy = elem_size;
+
+		while (!ret && size_to_copy) {
+			ret = copy_to_user(user_addr, vaddr,
+				size_to_copy > PAGE_SIZE ? PAGE_SIZE : size_to_copy);
+			size_to_copy -= (size_to_copy > PAGE_SIZE ? PAGE_SIZE : size_to_copy);
+		}
+		user_addr += user_stride;
+	}
+
+	vfree(vaddr);
+	return ret;
+}
+
+int nvmap_ioctl_rw_handle(struct file *filp, int is_read, void __user *arg,
+			  size_t op_size)
+{
+	struct nvmap_client *client = filp->private_data;
+	struct nvmap_rw_handle_64 __user *uarg64 = arg;
+	struct nvmap_rw_handle_64 op64;
+	struct nvmap_rw_handle __user *uarg = arg;
+	struct nvmap_rw_handle op;
+#ifdef CONFIG_COMPAT
+	struct nvmap_rw_handle_32 __user *uarg32 = arg;
+	struct nvmap_rw_handle_32 op32;
+#endif
+	struct nvmap_handle *h;
+	ssize_t copied;
+	int err = 0;
+	unsigned long addr, offset, elem_size, hmem_stride, user_stride;
+	unsigned long count;
+	int handle;
+	int ret;
+
+#ifdef CONFIG_COMPAT
+	if (op_size == sizeof(op32)) {
+		if (copy_from_user(&op32, arg, sizeof(op32)))
+			return -EFAULT;
+		addr = op32.addr;
+		handle = op32.handle;
+		offset = op32.offset;
+		elem_size = op32.elem_size;
+		hmem_stride = op32.hmem_stride;
+		user_stride = op32.user_stride;
+		count = op32.count;
+	} else
+#endif
+	{
+		if (op_size == sizeof(op)) {
+			if (copy_from_user(&op, arg, sizeof(op)))
+				return -EFAULT;
+			addr = op.addr;
+			handle = op.handle;
+			offset = op.offset;
+			elem_size = op.elem_size;
+			hmem_stride = op.hmem_stride;
+			user_stride = op.user_stride;
+			count = op.count;
+		} else {
+			if (copy_from_user(&op64, arg, sizeof(op64)))
+				return -EFAULT;
+			addr = op64.addr;
+			handle = op64.handle;
+			offset = op64.offset;
+			elem_size = op64.elem_size;
+			hmem_stride = op64.hmem_stride;
+			user_stride = op64.user_stride;
+			count = op64.count;
+		}
+	}
+
+	if (!addr || !count || !elem_size)
+		return -EINVAL;
+
+	h = nvmap_handle_get_from_fd(handle);
+	if (!h)
+		return -EINVAL;
+
+	if (is_read && soc_is_tegra186_n_later() &&
+		(h->heap_type &
+		(NVMAP_HEAP_CARVEOUT_VPR | NVMAP_HEAP_CARVEOUT_IVM_VPR))) {
+		/* VPR memory is not readable from CPU.
+		 * Memset buffer to all 0xFF's for backward compatibility. */
+		ret = set_vpr_fail_data((void *)addr, user_stride, elem_size, count);
+		nvmap_handle_put(h);
+		return ret ?: -EPERM;
+	}
+
+	/*
+	 * If Buffer is RO and write operation is asked from the buffer,
+	 * return error.
+	 */
+	if (h->is_ro && !is_read) {
+		nvmap_handle_put(h);
+		return -EPERM;
+	}
+
+	nvmap_kmaps_inc(h);
+	trace_nvmap_ioctl_rw_handle(client, h, is_read, offset,
+				    addr, hmem_stride,
+				    user_stride, elem_size, count);
+	copied = rw_handle(client, h, is_read, offset,
+			   addr, hmem_stride,
+			   user_stride, elem_size, count);
+	nvmap_kmaps_dec(h);
+
+	if (copied < 0) {
+		err = copied;
+		copied = 0;
+	} else if (copied < (count * elem_size))
+		err = -EINTR;
+
+#ifdef CONFIG_COMPAT
+	if (op_size == sizeof(op32))
+		__put_user(copied, &uarg32->count);
+	else
+#endif
+		if (op_size == sizeof(op))
+			__put_user(copied, &uarg->count);
+		else
+			__put_user(copied, &uarg64->count);
+
+	nvmap_handle_put(h);
+
+	return err;
+}
+
+int nvmap_ioctl_cache_maint(struct file *filp, void __user *arg, int op_size)
+{
+	struct nvmap_client *client = filp->private_data;
+	struct nvmap_cache_op op;
+	struct nvmap_cache_op_64 op64;
+#ifdef CONFIG_COMPAT
+	struct nvmap_cache_op_32 op32;
+#endif
+
+#ifdef CONFIG_COMPAT
+	if (op_size == sizeof(op32)) {
+		if (copy_from_user(&op32, arg, sizeof(op32)))
+			return -EFAULT;
+		op64.addr = op32.addr;
+		op64.handle = op32.handle;
+		op64.len = op32.len;
+		op64.op = op32.op;
+	} else
+#endif
+	{
+		if (op_size == sizeof(op)) {
+			if (copy_from_user(&op, arg, sizeof(op)))
+				return -EFAULT;
+			op64.addr = op.addr;
+			op64.handle = op.handle;
+			op64.len = op.len;
+			op64.op = op.op;
+		} else {
+			if (copy_from_user(&op64, arg, sizeof(op64)))
+				return -EFAULT;
+		}
+	}
+
+	return __nvmap_cache_maint(client, &op64);
+}
+
+int nvmap_ioctl_free(struct file *filp, unsigned long arg)
+{
+	struct nvmap_client *client = filp->private_data;
+
+	if (!arg)
+		return 0;
+
+	nvmap_free_handle_fd(client, arg);
+	return sys_close(arg);
+}
+
+static ssize_t rw_handle(struct nvmap_client *client, struct nvmap_handle *h,
+			 int is_read, unsigned long h_offs,
+			 unsigned long sys_addr, unsigned long h_stride,
+			 unsigned long sys_stride, unsigned long elem_size,
+			 unsigned long count)
+{
+	ssize_t copied = 0;
+	void *addr;
+	int ret = 0;
+
+	if (!(h->heap_type & nvmap_dev->cpu_access_mask))
+		return -EPERM;
+
+	if (!elem_size || !count)
+		return -EINVAL;
+
+	if (!h->alloc)
+		return -EFAULT;
+
+	if (elem_size == h_stride && elem_size == sys_stride && (h_offs % 8 == 0)) {
+		elem_size *= count;
+		h_stride = elem_size;
+		sys_stride = elem_size;
+		count = 1;
+	}
+
+	if (elem_size > h->size ||
+		h_offs >= h->size ||
+		elem_size > sys_stride ||
+		elem_size > h_stride ||
+		sys_stride > (h->size - h_offs) / count ||
+		h_offs + h_stride * (count - 1) + elem_size > h->size)
+		return -EINVAL;
+
+	if (!h->vaddr) {
+		if (!__nvmap_mmap(h))
+			return -ENOMEM;
+		__nvmap_munmap(h, h->vaddr);
+	}
+
+	addr = h->vaddr + h_offs;
+
+	while (count--) {
+		if (h_offs + elem_size > h->size) {
+			pr_warn("read/write outside of handle\n");
+			ret = -EFAULT;
+			break;
+		}
+		if (is_read &&
+		    !(h->userflags & NVMAP_HANDLE_CACHE_SYNC_AT_RESERVE))
+			__nvmap_do_cache_maint(client, h, h_offs,
+				h_offs + elem_size, NVMAP_CACHE_OP_INV, false);
+
+		if (is_read)
+			ret = copy_to_user((void *)sys_addr, addr, elem_size);
+		else {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 9, 0)
+			if (h->heap_type &
+				(NVMAP_HEAP_CARVEOUT_VPR |
+					NVMAP_HEAP_CARVEOUT_IVM_VPR)) {
+				uaccess_enable();
+				memcpy_toio(addr, (void *)sys_addr, elem_size);
+				uaccess_disable();
+				ret = 0;
+			} else
+#endif
+				ret = copy_from_user(addr, (void *)sys_addr, elem_size);
+		}
+
+		if (ret)
+			break;
+
+		if (!is_read &&
+		    !(h->userflags & NVMAP_HANDLE_CACHE_SYNC_AT_RESERVE))
+			__nvmap_do_cache_maint(client, h, h_offs,
+				h_offs + elem_size, NVMAP_CACHE_OP_WB_INV,
+				false);
+
+		copied += elem_size;
+		sys_addr += sys_stride;
+		h_offs += h_stride;
+		addr += h_stride;
+	}
+
+	return ret ?: copied;
+}
+
+int nvmap_ioctl_get_ivcid(struct file *filp, void __user *arg)
+{
+	struct nvmap_create_handle op;
+	struct nvmap_handle *h = NULL;
+
+	if (copy_from_user(&op, arg, sizeof(op)))
+		return -EFAULT;
+
+	h = nvmap_handle_get_from_fd(op.ivm_handle);
+	if (!h)
+		return -EINVAL;
+
+	if (!h->alloc) { /* || !h->ivm_id) { */
+		nvmap_handle_put(h);
+		return -EFAULT;
+	}
+
+	op.ivm_id = h->ivm_id;
+
+	nvmap_handle_put(h);
+
+	return copy_to_user(arg, &op, sizeof(op)) ? -EFAULT : 0;
+}
+
+int nvmap_ioctl_get_ivc_heap(struct file *filp, void __user *arg)
+{
+	const unsigned int allowed_heaps =
+		NVMAP_HEAP_CARVEOUT_IVM | NVMAP_HEAP_CARVEOUT_IVM_VPR;
+	struct nvmap_device *dev = nvmap_dev;
+	struct nvmap_available_ivm_heaps op;
+	int i;
+	unsigned int heap_mask_temp = 0;
+
+	if (copy_from_user(&op, arg, sizeof(op)))
+		return -EFAULT;
+
+	/* Only one heap type at a time can be requested */
+	if (hweight_long(op.requested_heap_type & allowed_heaps) != 1U)
+		return -EINVAL;
+
+	for (i = 0; i < dev->nr_carveouts; i++) {
+		struct nvmap_carveout_node *co_heap = &dev->heaps[i];
+		int peer;
+
+		if (!(co_heap->heap_bit & op.requested_heap_type))
+			continue;
+
+		peer = nvmap_query_heap_peer(co_heap->carveout);
+		if (peer < 0)
+			return -EINVAL;
+
+		heap_mask_temp |= BIT(peer);
+	}
+
+	op.heap_mask = heap_mask_temp;
+	if (copy_to_user(arg, &op, sizeof(op)))
+		return -EFAULT;
+
+	return 0;
+}
+
+int nvmap_ioctl_create_from_ivc(struct file *filp, void __user *arg)
+{
+	struct nvmap_create_handle op;
+	struct nvmap_handle_ref *ref;
+	struct nvmap_client *client = filp->private_data;
+	int fd;
+	phys_addr_t offs;
+	size_t size = 0;
+	int peer;
+	struct nvmap_heap_block *block = NULL;
+	bool is_ivm_vpr;
+
+	/* First create a new handle and then fake carveout allocation */
+	if (copy_from_user(&op, arg, sizeof(op)))
+		return -EFAULT;
+
+	if (!client)
+		return -ENODEV;
+
+	ref = nvmap_try_duplicate_by_ivmid(client, op.ivm_id, &block);
+	if (!ref) {
+		u32 type;
+		/*
+		 * See nvmap_heap_alloc() for encoding details.
+		 */
+		size = ((op.ivm_id & NVMAP_IVM_SIZE_MASK) >>
+			NVMAP_IVM_SIZE_SHIFT);
+		size <<= PAGE_SHIFT;
+
+		offs = ((op.ivm_id & NVMAP_IVM_OFFSET_MASK) >>
+			NVMAP_IVM_OFFSET_SHIFT);
+		offs <<= (ffs(NVMAP_IVM_ALIGNMENT) - 1);
+
+		peer = ((op.ivm_id & NVMAP_IVM_PEER_MASK) >>
+			NVMAP_IVM_PEER_SHIFT);
+		is_ivm_vpr = ((op.ivm_id & NVMAP_IVM_ISVPR_MASK) >>
+			NVMAP_IVM_ISVPR_SHIFT);
+
+		ref = nvmap_create_handle(client, size, false);
+		if (IS_ERR(ref)) {
+			nvmap_heap_free(block);
+			return PTR_ERR(ref);
+		}
+		ref->handle->orig_size = size;
+
+		ref->handle->peer = peer;
+
+		type = is_ivm_vpr ? NVMAP_HEAP_CARVEOUT_IVM_VPR :
+			NVMAP_HEAP_CARVEOUT_IVM;
+
+		if (!block) {
+			block = nvmap_carveout_alloc(client, ref->handle,
+				type, &offs);
+		}
+
+		if (!block) {
+			nvmap_free_handle(client, ref->handle);
+			return -ENOMEM;
+		}
+
+		ref->handle->heap_type = type;
+		ref->handle->heap_pgalloc = false;
+		ref->handle->ivm_id = op.ivm_id;
+		ref->handle->carveout = block;
+		block->handle = ref->handle;
+		mb();
+		ref->handle->alloc = true;
+		NVMAP_TAG_TRACE(trace_nvmap_alloc_handle_done,
+			NVMAP_TP_ARGS_CHR(client, ref->handle, ref));
+	}
+
+	fd = nvmap_get_dmabuf_fd(client, ref->handle);
+	op.ivm_handle = fd;
+	return nvmap_install_fd(client, ref->handle, fd,
+				arg, &op, sizeof(op), 1, ref->handle->dmabuf);
+}
+
+int nvmap_ioctl_cache_maint_list(struct file *filp, void __user *arg,
+				 bool is_reserve_ioctl)
+{
+	struct nvmap_cache_op_list op;
+	u32 *handle_ptr;
+	u64 *offset_ptr;
+	u64 *size_ptr;
+	struct nvmap_handle **refs;
+	int err = 0;
+	u32 i, n_unmarshal_handles = 0, count = 0;
+	size_t bytes;
+	size_t elem_size;
+	bool is_32;
+
+	if (copy_from_user(&op, arg, sizeof(op)))
+		return -EFAULT;
+
+	if (!op.nr || op.nr > UINT_MAX / sizeof(u32))
+		return -EINVAL;
+
+	bytes = op.nr * sizeof(*refs);
+	if (!access_ok(VERIFY_READ, op.handles, op.nr * sizeof(u32)))
+		return -EFAULT;
+
+	elem_size  = (op.op & NVMAP_ELEM_SIZE_U64) ?
+			sizeof(u64) : sizeof(u32);
+	op.op &= ~NVMAP_ELEM_SIZE_U64;
+	is_32 = elem_size == sizeof(u32) ? 1 : 0;
+
+	bytes += 2 * op.nr * elem_size;
+	bytes += op.nr * sizeof(u32);
+	refs = nvmap_altalloc(bytes);
+	if (!refs) {
+		pr_err("memory allocation failed\n");
+		return -ENOMEM;
+	}
+
+	offset_ptr = (u64 *)(refs + op.nr);
+	size_ptr = (u64 *)(((uintptr_t)offset_ptr) + op.nr * elem_size);
+	handle_ptr = (u32 *)(((uintptr_t)size_ptr) + op.nr * elem_size);
+
+	if (!op.handles || !op.offsets || !op.sizes) {
+		pr_err("pointers are invalid\n");
+		return -EINVAL;
+	}
+
+	if (!IS_ALIGNED((ulong)offset_ptr, elem_size) ||
+	    !IS_ALIGNED((ulong)size_ptr, elem_size) ||
+	    !IS_ALIGNED((ulong)handle_ptr, sizeof(u32))) {
+		pr_err("pointers are not properly aligned!!\n");
+		return -EINVAL;
+	}
+
+	if (copy_from_user(handle_ptr, (void *)op.handles,
+		op.nr * sizeof(u32))) {
+		pr_err("Can't copy from user pointer op.handles\n");
+		return -EFAULT;
+	}
+
+	if (copy_from_user(offset_ptr, (void *)op.offsets,
+		op.nr * elem_size)) {
+		pr_err("Can't copy from user pointer op.offsets\n");
+		return -EFAULT;
+	}
+
+	if (copy_from_user(size_ptr, (void *)op.sizes,
+		op.nr * elem_size)) {
+		pr_err("Can't copy from user pointer op.sizes\n");
+		return -EFAULT;
+	}
+
+	for (i = 0; i < op.nr; i++) {
+		refs[i] = nvmap_handle_get_from_fd(handle_ptr[i]);
+		if (!refs[i]) {
+			pr_err("invalid handle_ptr[%d] = %u\n",
+				i, handle_ptr[i]);
+			err = -EINVAL;
+			goto free_mem;
+		}
+		if (!(refs[i]->heap_type & nvmap_dev->cpu_access_mask)) {
+			pr_err("heap %x can't be accessed from cpu\n",
+				refs[i]->heap_type);
+			err = -EPERM;
+			goto free_mem;
+		}
+
+		n_unmarshal_handles++;
+	}
+
+	/*
+	 * Either all handles should have NVMAP_HANDLE_CACHE_SYNC_AT_RESERVE
+	 * or none should have it.
+	 */
+	for (i = 0; i < op.nr; i++)
+		if (refs[i]->userflags & NVMAP_HANDLE_CACHE_SYNC_AT_RESERVE)
+			count++;
+
+	if (op.nr && count % op.nr) {
+		pr_err("incorrect CACHE_SYNC_AT_RESERVE mix of handles\n");
+		err = -EINVAL;
+		goto free_mem;
+	}
+
+	/*
+	 * when NVMAP_HANDLE_CACHE_SYNC_AT_RESERVE is specified mix can cause
+	 * cache WB_INV at unreserve op on iovmm handles increasing overhead.
+	 * So, either all handles should have pages from carveout or from iovmm.
+	 */
+	if (count) {
+		for (i = 0; i < op.nr; i++)
+			if (refs[i]->heap_pgalloc)
+				count++;
+
+		if (op.nr && count % op.nr) {
+			pr_err("all or none of the handles should be from heap\n");
+			err = -EINVAL;
+			goto free_mem;
+		}
+	}
+
+	if (is_reserve_ioctl)
+		err = nvmap_reserve_pages(refs, offset_ptr, size_ptr,
+					  op.nr, op.op, is_32);
+	else
+		err = nvmap_do_cache_maint_list(refs, offset_ptr, size_ptr,
+						op.op, op.nr, is_32);
+
+free_mem:
+	for (i = 0; i < n_unmarshal_handles; i++)
+		nvmap_handle_put(refs[i]);
+	nvmap_altfree(refs, bytes);
+	return err;
+}
+
+int nvmap_ioctl_gup_test(struct file *filp, void __user *arg)
+{
+	int err = -EINVAL;
+	struct nvmap_gup_test op;
+	struct vm_area_struct *vma;
+	struct nvmap_handle *handle;
+	size_t i;
+	size_t nr_page;
+	struct page **pages;
+
+	if (copy_from_user(&op, arg, sizeof(op)))
+		return -EFAULT;
+
+	op.result = 1;
+	vma = find_vma(current->mm, op.va);
+	if (unlikely(!vma) || (unlikely(op.va < vma->vm_start )) ||
+	    unlikely(op.va >= vma->vm_end))
+		goto exit;
+
+	handle = nvmap_handle_get_from_fd(op.handle);
+	if (!handle)
+		goto exit;
+
+	if (vma->vm_end - vma->vm_start != handle->size) {
+		pr_err("handle size(0x%zx) and vma size(0x%lx) don't match\n",
+			 handle->size, vma->vm_end - vma->vm_start);
+		goto put_handle;
+	}
+
+	err = -ENOMEM;
+	nr_page = handle->size >> PAGE_SHIFT;
+	pages = nvmap_altalloc(nr_page * sizeof(*pages));
+	if (IS_ERR_OR_NULL(pages)) {
+		err = PTR_ERR(pages);
+		goto put_handle;
+	}
+
+	err = nvmap_get_user_pages(op.va & PAGE_MASK, nr_page, pages, false, 0);
+	if (err)
+		goto put_user_pages;
+
+	for (i = 0; i < nr_page; i++) {
+		if (handle->pgalloc.pages[i] != pages[i]) {
+			pr_err("page pointers don't match, %p %p\n",
+			       handle->pgalloc.pages[i], pages[i]);
+			op.result = 0;
+		}
+	}
+
+	if (op.result)
+		err = 0;
+
+	if (copy_to_user(arg, &op, sizeof(op)))
+		err = -EFAULT;
+
+put_user_pages:
+	nvmap_altfree(pages, nr_page * sizeof(*pages));
+put_handle:
+	nvmap_handle_put(handle);
+exit:
+	pr_info("GUP Test %s\n", err ? "failed" : "passed");
+	return err;
+}
+
+int nvmap_ioctl_set_tag_label(struct file *filp, void __user *arg)
+{
+	struct nvmap_set_tag_label op;
+	struct nvmap_device *dev = nvmap_dev;
+	int err;
+
+	if (copy_from_user(&op, arg, sizeof(op)))
+		return -EFAULT;
+
+	if (op.len > NVMAP_TAG_LABEL_MAXLEN)
+		op.len = NVMAP_TAG_LABEL_MAXLEN;
+
+	if (op.len)
+		err = nvmap_define_tag(dev, op.tag,
+			(const char __user *)op.addr, op.len);
+	else
+		err = nvmap_remove_tag(dev, op.tag);
+
+	return err;
+}
+
+int nvmap_ioctl_get_available_heaps(struct file *filp, void __user *arg)
+{
+	struct nvmap_available_heaps op;
+	int i;
+
+	memset(&op, 0, sizeof(op));
+
+	for (i = 0; i < nvmap_dev->nr_carveouts; i++)
+		op.heaps |= nvmap_dev->heaps[i].heap_bit;
+
+	if (copy_to_user(arg, &op, sizeof(op))) {
+		pr_err("copy_to_user failed\n");
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+int nvmap_ioctl_get_heap_size(struct file *filp, void __user *arg)
+{
+	struct nvmap_heap_size op;
+	struct nvmap_heap *heap;
+	int i;
+	memset(&op, 0, sizeof(op));
+
+	if (copy_from_user(&op, arg, sizeof(op)))
+		return -EFAULT;
+
+	for (i = 0; i < nvmap_dev->nr_carveouts; i++) {
+		if (op.heap & nvmap_dev->heaps[i].heap_bit) {
+			heap = nvmap_dev->heaps[i].carveout;
+			op.size = nvmap_query_heap_size(heap);
+			if (copy_to_user(arg, &op, sizeof(op)))
+				return -EFAULT;
+			return 0;
+		}
+	}
+	return -ENODEV;
+
+}
+
+static unsigned long system_heap_free_mem(void)
+{
+	struct sysinfo sys_heap;
+
+	si_meminfo(&sys_heap);
+
+	return sys_heap.freeram << PAGE_SHIFT;
+}
+
+static unsigned long system_heap_total_mem(void)
+{
+	struct sysinfo sys_heap;
+
+	si_meminfo(&sys_heap);
+
+	return sys_heap.totalram << PAGE_SHIFT;
+}
+
+int nvmap_ioctl_query_heap_params(struct file *filp, void __user *arg)
+{
+	unsigned int carveout_mask = NVMAP_HEAP_CARVEOUT_MASK;
+	unsigned int iovmm_mask = NVMAP_HEAP_IOVMM;
+	struct nvmap_query_heap_params op;
+	struct nvmap_heap *heap;
+	unsigned int type;
+	int ret = 0;
+	int i;
+
+	memset(&op, 0, sizeof(op));
+	if (copy_from_user(&op, arg, sizeof(op))) {
+		ret =  -EFAULT;
+		goto exit;
+	}
+
+	type = op.heap_mask;
+	WARN_ON(type & (type - 1));
+
+	if (nvmap_convert_carveout_to_iovmm) {
+		carveout_mask &= ~NVMAP_HEAP_CARVEOUT_GENERIC;
+		iovmm_mask |= NVMAP_HEAP_CARVEOUT_GENERIC;
+	} else if (nvmap_convert_iovmm_to_carveout) {
+		if (type & NVMAP_HEAP_IOVMM) {
+			type &= ~NVMAP_HEAP_IOVMM;
+			type |= NVMAP_HEAP_CARVEOUT_GENERIC;
+		}
+	}
+
+	/* To Do: select largest free block */
+	op.largest_free_block = PAGE_SIZE;
+
+	if (type & NVMAP_HEAP_CARVEOUT_MASK) {
+		for (i = 0; i < nvmap_dev->nr_carveouts; i++) {
+			if (type & nvmap_dev->heaps[i].heap_bit) {
+				heap = nvmap_dev->heaps[i].carveout;
+				op.total = nvmap_query_heap_size(heap);
+				break;
+			}
+		}
+	} else if (type & iovmm_mask) {
+		op.total = system_heap_total_mem();
+		op.free = system_heap_free_mem();
+	}
+
+	if (copy_to_user(arg, &op, sizeof(op)))
+		ret = -EFAULT;
+exit:
+	return ret;
+}
+
+int nvmap_ioctl_query_handle_parameters(struct file *filp, void __user *arg)
+{
+	struct nvmap_handle_parameters op;
+	struct nvmap_handle *handle;
+
+	if (copy_from_user(&op, arg, sizeof(op)))
+		return -EFAULT;
+
+	handle = nvmap_handle_get_from_fd(op.handle);
+	if (handle == NULL)
+		goto exit;
+
+	if (!handle->alloc)
+		op.heap = 0;
+	else
+		op.heap = handle->heap_type;
+
+	/* heap_number, only valid for IVM carveout */
+	op.heap_number = handle->peer;
+
+	op.size = handle->size;
+
+	if (handle->userflags & NVMAP_HANDLE_PHYS_CONTIG)
+		op.contig = 1U;
+	else
+		op.contig = 0U;
+
+	op.align = handle->align;
+
+	op.coherency = handle->flags;
+
+	nvmap_handle_put(handle);
+
+	if (copy_to_user(arg, &op, sizeof(op)))
+		return -EFAULT;
+	return 0;
+
+exit:
+	return -ENODEV;
+}
diff --git a/drivers/video/tegra/nvmap/nvmap_ioctl.h b/drivers/video/tegra/nvmap/nvmap_ioctl.h
new file mode 100644
index 000000000000..6c64d2abece0
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nvmap_ioctl.h
@@ -0,0 +1,72 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_ioctl.h
+ *
+ * ioctl declarations for nvmap
+ *
+ * Copyright (c) 2010-2020, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __VIDEO_TEGRA_NVMAP_IOCTL_H
+#define __VIDEO_TEGRA_NVMAP_IOCTL_H
+
+#include <linux/nvmap.h>
+
+int nvmap_ioctl_pinop(struct file *filp, bool is_pin, void __user *arg,
+	bool is32);
+
+int nvmap_ioctl_getid(struct file *filp, void __user *arg);
+
+int nvmap_ioctl_get_ivcid(struct file *filp, void __user *arg);
+
+int nvmap_ioctl_getfd(struct file *filp, void __user *arg);
+
+int nvmap_ioctl_alloc(struct file *filp, void __user *arg);
+
+int nvmap_ioctl_alloc_kind(struct file *filp, void __user *arg);
+
+int nvmap_ioctl_alloc_ivm(struct file *filp, void __user *arg);
+
+int nvmap_ioctl_vpr_floor_size(struct file *filp, void __user *arg);
+
+int nvmap_ioctl_free(struct file *filp, unsigned long arg);
+
+int nvmap_ioctl_create(struct file *filp, unsigned int cmd, void __user *arg);
+
+int nvmap_ioctl_create_from_va(struct file *filp, void __user *arg);
+
+int nvmap_ioctl_create_from_ivc(struct file *filp, void __user *arg);
+
+int nvmap_ioctl_get_ivc_heap(struct file *filp, void __user *arg);
+
+int nvmap_map_into_caller_ptr(struct file *filp, void __user *arg, bool is32);
+
+int nvmap_ioctl_cache_maint(struct file *filp, void __user *arg, int size);
+
+int nvmap_ioctl_rw_handle(struct file *filp, int is_read, void __user *arg,
+	size_t op_size);
+
+int nvmap_ioctl_cache_maint_list(struct file *filp, void __user *arg,
+	bool is_rsrv_op);
+
+int nvmap_ioctl_gup_test(struct file *filp, void __user *arg);
+
+int nvmap_ioctl_set_tag_label(struct file *filp, void __user *arg);
+
+int nvmap_ioctl_get_available_heaps(struct file *filp, void __user *arg);
+
+int nvmap_ioctl_get_heap_size(struct file *filp, void __user *arg);
+
+int nvmap_ioctl_query_heap_params(struct file *filp, void __user *arg);
+
+int nvmap_ioctl_query_handle_parameters(struct file *filp, void __user *arg);
+
+#endif	/*  __VIDEO_TEGRA_NVMAP_IOCTL_H */
diff --git a/drivers/video/tegra/nvmap/nvmap_mm.c b/drivers/video/tegra/nvmap/nvmap_mm.c
new file mode 100644
index 000000000000..b6244e51bae7
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nvmap_mm.c
@@ -0,0 +1,266 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_mm.c
+ *
+ * Some MM related functionality specific to nvmap.
+ *
+ * Copyright (c) 2013-2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#include <trace/events/nvmap.h>
+#include <linux/version.h>
+
+#include <asm/pgtable.h>
+
+#include "nvmap_priv.h"
+
+enum NVMAP_PROT_OP {
+	NVMAP_HANDLE_PROT_NONE = 1,
+	NVMAP_HANDLE_PROT_RESTORE = 2,
+};
+
+void nvmap_zap_handle(struct nvmap_handle *handle, u64 offset, u64 size)
+{
+	struct list_head *vmas;
+	struct nvmap_vma_list *vma_list;
+	struct vm_area_struct *vma;
+
+	if (!handle->heap_pgalloc)
+		return;
+
+	/* if no dirty page is present, no need to zap */
+	if (nvmap_handle_track_dirty(handle) && !atomic_read(&handle->pgalloc.ndirty))
+		return;
+
+	if (!size) {
+		offset = 0;
+		size = handle->size;
+	}
+
+	size = PAGE_ALIGN((offset & ~PAGE_MASK) + size);
+
+	mutex_lock(&handle->lock);
+	vmas = &handle->vmas;
+	list_for_each_entry(vma_list, vmas, list) {
+		struct nvmap_vma_priv *priv;
+		size_t vm_size = size;
+
+		vma = vma_list->vma;
+		priv = vma->vm_private_data;
+		if ((offset + size) > (vma->vm_end - vma->vm_start))
+			vm_size = vma->vm_end - vma->vm_start - offset;
+
+		if (priv->offs || vma->vm_pgoff)
+			/* vma mapping starts in the middle of handle memory.
+			 * zapping needs special care. zap entire range for now.
+			 * FIXME: optimze zapping.
+			 */
+			zap_page_range(vma, vma->vm_start,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+				vma->vm_end - vma->vm_start);
+#else
+				vma->vm_end - vma->vm_start, NULL);
+#endif
+		else
+			zap_page_range(vma, vma->vm_start + offset,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+				vm_size);
+#else
+				vm_size, NULL);
+#endif
+	}
+	mutex_unlock(&handle->lock);
+}
+
+static int nvmap_prot_handle(struct nvmap_handle *handle, u64 offset,
+		u64 size, int op)
+{
+	struct list_head *vmas;
+	struct nvmap_vma_list *vma_list;
+	struct vm_area_struct *vma;
+	int err = -EINVAL;
+
+	if (!handle->heap_pgalloc)
+		return err;
+
+	if ((offset >= handle->size) || (offset > handle->size - size) ||
+	    (size > handle->size)) {
+		pr_debug("%s offset: %lld h->size: %zu size: %lld\n", __func__,
+				offset, handle->size, size);
+		return err;
+	}
+
+	if (!size)
+		size = handle->size;
+
+	size = PAGE_ALIGN((offset & ~PAGE_MASK) + size);
+
+	mutex_lock(&handle->lock);
+	vmas = &handle->vmas;
+	list_for_each_entry(vma_list, vmas, list) {
+		struct nvmap_vma_priv *priv;
+		size_t vm_size = size;
+		struct vm_area_struct *prev;
+
+		vma = vma_list->vma;
+		prev = vma->vm_prev;
+		priv = vma->vm_private_data;
+		if ((offset + size) > (vma->vm_end - vma->vm_start))
+			vm_size = vma->vm_end - vma->vm_start - offset;
+
+		if ((priv->offs || vma->vm_pgoff) ||
+		    (size > (vma->vm_end - vma->vm_start)))
+			vm_size = vma->vm_end - vma->vm_start;
+
+		if (vma->vm_mm != current->mm)
+			down_write(&vma->vm_mm->mmap_sem);
+		switch (op) {
+		case NVMAP_HANDLE_PROT_NONE:
+			vma->vm_flags = vma_list->save_vm_flags;
+			(void)vma_set_page_prot(vma);
+			if (nvmap_handle_track_dirty(handle) &&
+			    !atomic_read(&handle->pgalloc.ndirty)) {
+				err = 0;
+				break;
+			}
+			err = mprotect_fixup(vma, &prev, vma->vm_start,
+					vma->vm_start + vm_size, VM_NONE);
+			if (err)
+				goto try_unlock;
+			vma->vm_flags = vma_list->save_vm_flags;
+			(void)vma_set_page_prot(vma);
+			break;
+		case NVMAP_HANDLE_PROT_RESTORE:
+			vma->vm_flags = VM_NONE;
+			(void)vma_set_page_prot(vma);
+			err = mprotect_fixup(vma, &prev, vma->vm_start,
+					vma->vm_start + vm_size,
+					vma_list->save_vm_flags);
+			if (err)
+				goto try_unlock;
+			_nvmap_handle_mkdirty(handle, 0, size);
+			break;
+		default:
+			BUG();
+		};
+try_unlock:
+		if (vma->vm_mm != current->mm)
+			up_write(&vma->vm_mm->mmap_sem);
+		if (err)
+			goto finish;
+	}
+finish:
+	mutex_unlock(&handle->lock);
+	return err;
+}
+
+static int nvmap_prot_handles(struct nvmap_handle **handles, u64 *offsets,
+		       u64 *sizes, u32 nr, int op, bool is_32)
+{
+	int i, err = 0;
+	u32 *offs_32 = (u32 *)offsets, *sizes_32 = (u32 *)sizes;
+
+	down_write(&current->mm->mmap_sem);
+	for (i = 0; i < nr; i++) {
+		err = nvmap_prot_handle(handles[i],
+				is_32 ? offs_32[i] : offsets[i],
+				is_32 ? sizes_32[i] : sizes[i], op);
+		if (err) {
+			pr_debug("%s nvmap_prot_handle failed [%d] is_32 %d\n",
+					__func__, err, is_32);
+			goto finish;
+		}
+	}
+finish:
+	up_write(&current->mm->mmap_sem);
+	return err;
+}
+
+int nvmap_reserve_pages(struct nvmap_handle **handles, u64 *offsets, u64 *sizes,
+			u32 nr, u32 op, bool is_32)
+{
+	int i, err;
+	u32 *offs_32 = (u32 *)offsets, *sizes_32 = (u32 *)sizes;
+
+	for (i = 0; i < nr; i++) {
+		u64 size = is_32 ? sizes_32[i] : sizes[i];
+		u64 offset = is_32 ? offs_32[i] : offsets[i];
+
+		size = size ?: handles[i]->size;
+		offset = offset ?: 0;
+
+		if ((offset != 0) || (size != handles[i]->size)) {
+			pr_debug("%s offset: %lld size: %lld h->size %zu\n",
+					__func__, offset, size,
+					handles[i]->size);
+			return -EINVAL;
+		}
+
+		if (op == NVMAP_PAGES_PROT_AND_CLEAN)
+			continue;
+
+		/*
+		 * NOTE: This unreserves the handle even when
+		 * NVMAP_PAGES_INSERT_ON_UNRESERVE is called on some portion
+		 * of the handle
+		 */
+		atomic_set(&handles[i]->pgalloc.reserved,
+				(op == NVMAP_PAGES_RESERVE) ? 1 : 0);
+	}
+
+	if (op == NVMAP_PAGES_PROT_AND_CLEAN)
+		op = NVMAP_PAGES_RESERVE;
+
+	switch (op) {
+	case NVMAP_PAGES_RESERVE:
+		err = nvmap_prot_handles(handles, offsets, sizes, nr,
+				NVMAP_HANDLE_PROT_NONE, is_32);
+		if (err)
+			return err;
+		break;
+	case NVMAP_INSERT_PAGES_ON_UNRESERVE:
+		err = nvmap_prot_handles(handles, offsets, sizes, nr,
+				NVMAP_HANDLE_PROT_RESTORE, is_32);
+		if (err)
+			return err;
+		break;
+	case NVMAP_PAGES_UNRESERVE:
+		for (i = 0; i < nr; i++)
+			if (nvmap_handle_track_dirty(handles[i]))
+				atomic_set(&handles[i]->pgalloc.ndirty, 0);
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	if (!(handles[0]->userflags & NVMAP_HANDLE_CACHE_SYNC_AT_RESERVE))
+		return 0;
+
+	if (op == NVMAP_PAGES_RESERVE) {
+		err = nvmap_do_cache_maint_list(handles, offsets, sizes,
+					  NVMAP_CACHE_OP_WB, nr, is_32);
+		if (err)
+			return err;
+		for (i = 0; i < nr; i++)
+			nvmap_handle_mkclean(handles[i],
+					is_32 ? offs_32[i] : offsets[i],
+					is_32 ? sizes_32[i] : sizes[i]);
+	} else if ((op == NVMAP_PAGES_UNRESERVE) && handles[0]->heap_pgalloc) {
+		/* Do nothing */
+	} else {
+		err = nvmap_do_cache_maint_list(handles, offsets, sizes,
+					  NVMAP_CACHE_OP_WB_INV, nr, is_32);
+		if (err)
+			return err;
+	}
+	return 0;
+}
+
diff --git a/drivers/video/tegra/nvmap/nvmap_pp.c b/drivers/video/tegra/nvmap/nvmap_pp.c
new file mode 100644
index 000000000000..cdb08fd029aa
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nvmap_pp.c
@@ -0,0 +1,745 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_pp.c
+ *
+ * Manage page pools to speed up page allocation.
+ *
+ * Copyright (c) 2009-2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt) "%s: " fmt, __func__
+
+#include <linux/kernel.h>
+#include <linux/vmalloc.h>
+#include <linux/moduleparam.h>
+#include <linux/nodemask.h>
+#include <linux/shrinker.h>
+#include <linux/kthread.h>
+#include <linux/debugfs.h>
+#include <linux/freezer.h>
+#include <linux/highmem.h>
+#include <linux/version.h>
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 14, 0)
+#include <linux/sched/clock.h>
+#include <uapi/linux/sched/types.h>
+#endif
+
+#include <trace/events/nvmap.h>
+
+#include "nvmap_priv.h"
+
+#define NVMAP_TEST_PAGE_POOL_SHRINKER     1
+#define PENDING_PAGES_SIZE                (SZ_1M / PAGE_SIZE)
+
+static bool enable_pp = 1;
+static u32 pool_size;
+
+static struct task_struct *background_allocator;
+static DECLARE_WAIT_QUEUE_HEAD(nvmap_bg_wait);
+
+#ifdef CONFIG_NVMAP_PAGE_POOL_DEBUG
+static inline void __pp_dbg_var_add(u64 *dbg_var, u32 nr)
+{
+	*dbg_var += nr;
+}
+#else
+#define __pp_dbg_var_add(dbg_var, nr)
+#endif
+
+#define pp_alloc_add(pool, nr) __pp_dbg_var_add(&(pool)->allocs, nr)
+#define pp_fill_add(pool, nr)  __pp_dbg_var_add(&(pool)->fills, nr)
+#define pp_hit_add(pool, nr)   __pp_dbg_var_add(&(pool)->hits, nr)
+#define pp_miss_add(pool, nr)  __pp_dbg_var_add(&(pool)->misses, nr)
+
+static int __nvmap_page_pool_fill_lots_locked(struct nvmap_page_pool *pool,
+				       struct page **pages, u32 nr);
+
+static inline struct page *get_zero_list_page(struct nvmap_page_pool *pool)
+{
+	struct page *page;
+
+	trace_get_zero_list_page(pool->to_zero);
+
+	if (list_empty(&pool->zero_list))
+		return NULL;
+
+	page = list_first_entry(&pool->zero_list, struct page, lru);
+	list_del(&page->lru);
+
+	pool->to_zero--;
+
+	return page;
+}
+
+static inline struct page *get_page_list_page(struct nvmap_page_pool *pool)
+{
+	struct page *page;
+
+	trace_get_page_list_page(pool->count);
+
+	if (list_empty(&pool->page_list))
+		return NULL;
+
+	page = list_first_entry(&pool->page_list, struct page, lru);
+	list_del(&page->lru);
+
+	pool->count--;
+
+	return page;
+}
+
+static inline struct page *get_page_list_page_bp(struct nvmap_page_pool *pool)
+{
+	struct page *page;
+
+	if (list_empty(&pool->page_list_bp))
+		return NULL;
+
+	page = list_first_entry(&pool->page_list_bp, struct page, lru);
+	list_del(&page->lru);
+
+	pool->count -= pool->pages_per_big_pg;
+	pool->big_page_count -= pool->pages_per_big_pg;
+
+	return page;
+}
+
+static inline bool nvmap_bg_should_run(struct nvmap_page_pool *pool)
+{
+	return !list_empty(&pool->zero_list);
+}
+
+static void nvmap_pp_zero_pages(struct page **pages, int nr)
+{
+	int i;
+
+	for (i = 0; i < nr; i++) {
+		clear_highpage(pages[i]);
+		nvmap_clean_cache_page(pages[i]);
+	}
+
+	trace_nvmap_pp_zero_pages(nr);
+}
+
+static void nvmap_pp_do_background_zero_pages(struct nvmap_page_pool *pool)
+{
+	int i;
+	struct page *page;
+	int ret;
+	/*
+	 * Statically declared array of pages to be zeroed in a batch,
+	 * local to this thread but too big for the stack.
+	 */
+	static struct page *pending_zero_pages[PENDING_PAGES_SIZE];
+
+	rt_mutex_lock(&pool->lock);
+	for (i = 0; i < PENDING_PAGES_SIZE; i++) {
+		page = get_zero_list_page(pool);
+		if (page == NULL)
+			break;
+		pending_zero_pages[i] = page;
+		pool->under_zero++;
+	}
+	rt_mutex_unlock(&pool->lock);
+
+	nvmap_pp_zero_pages(pending_zero_pages, i);
+
+	rt_mutex_lock(&pool->lock);
+	ret = __nvmap_page_pool_fill_lots_locked(pool, pending_zero_pages, i);
+	pool->under_zero -= i;
+	rt_mutex_unlock(&pool->lock);
+
+	trace_nvmap_pp_do_background_zero_pages(ret, i);
+
+	for (; ret < i; ret++)
+		__free_page(pending_zero_pages[ret]);
+}
+
+/*
+ * This thread fills the page pools with zeroed pages. We avoid releasing the
+ * pages directly back into the page pools since we would then have to zero
+ * them ourselves. Instead it is easier to just reallocate zeroed pages. This
+ * happens in the background so that the overhead of allocating zeroed pages is
+ * not directly seen by userspace. Of course if the page pools are empty user
+ * space will suffer.
+ */
+static int nvmap_background_zero_thread(void *arg)
+{
+	struct nvmap_page_pool *pool = &nvmap_dev->pool;
+	struct sched_param param = { .sched_priority = 0 };
+
+	pr_info("PP zeroing thread starting.\n");
+
+	set_freezable();
+	sched_setscheduler(current, SCHED_IDLE, &param);
+
+	while (!kthread_should_stop()) {
+		while (nvmap_bg_should_run(pool))
+			nvmap_pp_do_background_zero_pages(pool);
+
+		wait_event_freezable(nvmap_bg_wait,
+				nvmap_bg_should_run(pool) ||
+				kthread_should_stop());
+	}
+
+	return 0;
+}
+
+static void nvmap_pgcount(struct page *page, bool incr)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 9, 0)
+	if (incr)
+		atomic_inc(&page->_count);
+	else
+		atomic_dec(&page->_count);
+#else
+	page_ref_add(page, incr ? 1 : -1);
+#endif
+}
+
+/*
+ * Free the passed number of pages from the page pool. This happens regardless
+ * of whether the page pools are enabled. This lets one disable the page pools
+ * and then free all the memory therein.
+ *
+ * FIXME: Pages in pending_zero_pages[] can still be unreleased.
+ */
+static ulong nvmap_page_pool_free_pages_locked(struct nvmap_page_pool *pool,
+						      ulong nr_pages)
+{
+	struct page *page;
+	bool use_page_list = false;
+	bool use_page_list_bp = false;
+
+	pr_debug("req to release pages=%ld\n", nr_pages);
+
+	while (nr_pages) {
+		int i;
+
+		if (use_page_list_bp)
+			page = get_page_list_page_bp(pool);
+		else if (use_page_list)
+			page = get_page_list_page(pool);
+		else
+			page = get_zero_list_page(pool);
+
+		if (!page) {
+			if (!use_page_list) {
+				use_page_list = true;
+				continue;
+			} else if (!use_page_list_bp) {
+				use_page_list_bp = true;
+				continue;
+			}
+			break;
+		}
+
+		if (use_page_list_bp) {
+			for (i = 0; i < pool->pages_per_big_pg; i++)
+				__free_page(nth_page(page, i));
+			pr_debug("released %d pages\n", pool->pages_per_big_pg);
+			if (nr_pages > pool->pages_per_big_pg)
+				nr_pages -= pool->pages_per_big_pg;
+			else
+				nr_pages = 0;
+		} else {
+			__free_page(page);
+			nr_pages--;
+			pr_debug("released 1 page\n");
+		}
+	}
+
+	pr_debug("remaining pages to release=%ld\n", nr_pages);
+	return nr_pages;
+}
+
+/*
+ * Alloc a bunch of pages from the page pool. This will alloc as many as it can
+ * and return the number of pages allocated. Pages are placed into the passed
+ * array in a linear fashion starting from index 0.
+ */
+int nvmap_page_pool_alloc_lots(struct nvmap_page_pool *pool,
+				struct page **pages, u32 nr)
+{
+	u32 ind = 0;
+	u32 non_zero_idx;
+	u32 non_zero_cnt = 0;
+
+	if (!enable_pp || !nr)
+		return 0;
+
+	rt_mutex_lock(&pool->lock);
+
+	while (ind < nr) {
+		struct page *page = NULL;
+
+		if (!non_zero_cnt)
+			page = get_page_list_page(pool);
+
+		if (!page) {
+			page = get_zero_list_page(pool);
+			if (!page)
+				break;
+			if (!non_zero_cnt)
+				non_zero_idx = ind;
+			non_zero_cnt++;
+		}
+
+		pages[ind++] = page;
+		if (IS_ENABLED(CONFIG_NVMAP_PAGE_POOL_DEBUG)) {
+			nvmap_pgcount(page, false);
+			BUG_ON(page_count(page) != 1);
+		}
+	}
+
+	rt_mutex_unlock(&pool->lock);
+
+	/* Zero non-zeroed pages, if any */
+	if (non_zero_cnt)
+		nvmap_pp_zero_pages(&pages[non_zero_idx], non_zero_cnt);
+
+	pp_alloc_add(pool, ind);
+	pp_hit_add(pool, ind);
+	pp_miss_add(pool, nr - ind);
+
+	trace_nvmap_pp_alloc_lots(ind, nr);
+
+	return ind;
+}
+
+int nvmap_page_pool_alloc_lots_bp(struct nvmap_page_pool *pool,
+				struct page **pages, u32 nr)
+{
+	int ind = 0, nr_pages = nr;
+	struct page *page;
+
+	if (!enable_pp || pool->pages_per_big_pg <= 1 ||
+	    nr_pages < pool->pages_per_big_pg)
+		return 0;
+
+	rt_mutex_lock(&pool->lock);
+
+	while (nr_pages - ind >= pool->pages_per_big_pg) {
+		int i;
+
+		page = get_page_list_page_bp(pool);
+		if (!page)
+			break;
+
+		for (i = 0; i < pool->pages_per_big_pg; i++)
+			pages[ind + i] = nth_page(page, i);
+
+		ind += pool->pages_per_big_pg;
+	}
+
+	rt_mutex_unlock(&pool->lock);
+	return ind;
+}
+
+static bool nvmap_is_big_page(struct nvmap_page_pool *pool,
+			      struct page **pages, int idx, int nr)
+{
+	int i;
+	struct page *page = pages[idx];
+
+	if (pool->pages_per_big_pg <= 1)
+		return false;
+
+	if (nr - idx < pool->pages_per_big_pg)
+		return false;
+
+	/* Allow coalescing pages at big page boundary only */
+	if (page_to_phys(page) & (pool->big_pg_sz - 1))
+		return false;
+
+	for (i = 1; i < pool->pages_per_big_pg; i++)
+		if (pages[idx + i] != nth_page(page, i))
+			break;
+
+	return i == pool->pages_per_big_pg ? true: false;
+}
+
+/*
+ * Fill a bunch of pages into the page pool. This will fill as many as it can
+ * and return the number of pages filled. Pages are used from the start of the
+ * passed page pointer array in a linear fashion.
+ *
+ * You must lock the page pool before using this.
+ */
+static int __nvmap_page_pool_fill_lots_locked(struct nvmap_page_pool *pool,
+				       struct page **pages, u32 nr)
+{
+	int real_nr;
+	int ind = 0;
+
+	if (!enable_pp)
+		return 0;
+
+	real_nr = min_t(u32, pool->max - pool->count, nr);
+	BUG_ON(real_nr < 0);
+	if (real_nr == 0)
+		return 0;
+
+	while (real_nr > 0) {
+		if (IS_ENABLED(CONFIG_NVMAP_PAGE_POOL_DEBUG)) {
+			nvmap_pgcount(pages[ind], true);
+			BUG_ON(page_count(pages[ind]) != 2);
+		}
+
+		if (nvmap_is_big_page(pool, pages, ind, nr)) {
+			list_add_tail(&pages[ind]->lru, &pool->page_list_bp);
+			ind += pool->pages_per_big_pg;
+			real_nr -= pool->pages_per_big_pg;
+			pool->big_page_count += pool->pages_per_big_pg;
+		} else {
+			list_add_tail(&pages[ind++]->lru, &pool->page_list);
+			real_nr--;
+		}
+	}
+
+	pool->count += ind;
+	BUG_ON(pool->count > pool->max);
+	pp_fill_add(pool, ind);
+
+	return ind;
+}
+
+int nvmap_page_pool_fill_lots(struct nvmap_page_pool *pool,
+				       struct page **pages, u32 nr)
+{
+	int ret = 0;
+	int i;
+	u32 save_to_zero;
+
+	rt_mutex_lock(&pool->lock);
+
+	save_to_zero = pool->to_zero;
+
+	ret = min(nr, pool->max - pool->count - pool->to_zero - pool->under_zero);
+
+	for (i = 0; i < ret; i++) {
+		/* If page has additonal referecnces, Don't add it into
+		 * page pool. get_user_pages() on mmap'ed nvmap handle can
+		 * hold a refcount on the page. These pages can't be
+		 * reused till the additional refs are dropped.
+		 */
+		if (page_count(pages[i]) > 1) {
+			__free_page(pages[i]);
+		} else {
+			list_add_tail(&pages[i]->lru, &pool->zero_list);
+			pool->to_zero++;
+		}
+	}
+
+	if (pool->to_zero)
+		wake_up_interruptible(&nvmap_bg_wait);
+	ret = i;
+
+	trace_nvmap_pp_fill_zero_lots(save_to_zero, pool->to_zero,
+			ret, nr);
+
+	rt_mutex_unlock(&pool->lock);
+
+	return ret;
+}
+
+ulong nvmap_page_pool_get_unused_pages(void)
+{
+	int total = 0;
+
+	if (!nvmap_dev)
+		return 0;
+
+	total = nvmap_dev->pool.count + nvmap_dev->pool.to_zero;
+
+	return total;
+}
+
+/*
+ * Remove and free to the system all the pages currently in the page
+ * pool. This operation will happen even if the page pools are disabled.
+ */
+int nvmap_page_pool_clear(void)
+{
+	struct nvmap_page_pool *pool = &nvmap_dev->pool;
+
+	rt_mutex_lock(&pool->lock);
+
+	(void)nvmap_page_pool_free_pages_locked(pool, pool->count + pool->to_zero);
+
+	/* For some reason, if an error occured... */
+	if (!list_empty(&pool->page_list) || !list_empty(&pool->zero_list)) {
+		rt_mutex_unlock(&pool->lock);
+		return -ENOMEM;
+	}
+
+	rt_mutex_unlock(&pool->lock);
+
+	return 0;
+}
+
+/*
+ * Resizes the page pool to the passed size. If the passed size is 0 then
+ * all associated resources are released back to the system. This operation
+ * will only occur if the page pools are enabled.
+ */
+static void nvmap_page_pool_resize(struct nvmap_page_pool *pool, u32 size)
+{
+	u32 curr;
+
+	rt_mutex_lock(&pool->lock);
+
+	curr = nvmap_page_pool_get_unused_pages();
+	if (curr > size)
+		(void)nvmap_page_pool_free_pages_locked(pool, curr - size);
+
+	pr_debug("page pool resized to %d from %d pages\n", size, pool->max);
+	pool->max = size;
+
+	rt_mutex_unlock(&pool->lock);
+}
+
+static unsigned long nvmap_page_pool_count_objects(struct shrinker *shrinker,
+						   struct shrink_control *sc)
+{
+	return nvmap_page_pool_get_unused_pages();
+}
+
+static unsigned long nvmap_page_pool_scan_objects(struct shrinker *shrinker,
+						  struct shrink_control *sc)
+{
+	unsigned long remaining;
+
+	pr_debug("sh_pages=%lu", sc->nr_to_scan);
+
+	rt_mutex_lock(&nvmap_dev->pool.lock);
+	remaining = nvmap_page_pool_free_pages_locked(
+			&nvmap_dev->pool, sc->nr_to_scan);
+	rt_mutex_unlock(&nvmap_dev->pool.lock);
+
+	return (remaining == sc->nr_to_scan) ? \
+			   SHRINK_STOP : (sc->nr_to_scan - remaining);
+}
+
+static struct shrinker nvmap_page_pool_shrinker = {
+	.count_objects = nvmap_page_pool_count_objects,
+	.scan_objects = nvmap_page_pool_scan_objects,
+	.seeks = 1,
+};
+
+static void shrink_page_pools(int *total_pages, int *available_pages)
+{
+	struct shrink_control sc;
+
+	if (*total_pages == 0) {
+		sc.gfp_mask = GFP_KERNEL;
+		sc.nr_to_scan = 0;
+		*total_pages = nvmap_page_pool_count_objects(NULL, &sc);
+	}
+	sc.nr_to_scan = *total_pages;
+	nvmap_page_pool_scan_objects(NULL, &sc);
+	*available_pages = nvmap_page_pool_count_objects(NULL, &sc);
+}
+
+#if NVMAP_TEST_PAGE_POOL_SHRINKER
+static int shrink_pp;
+static int shrink_set(const char *arg, const struct kernel_param *kp)
+{
+	int cpu = smp_processor_id();
+	unsigned long long t1, t2;
+	int total_pages, available_pages;
+
+	param_set_int(arg, kp);
+
+	if (shrink_pp) {
+		total_pages = shrink_pp;
+		t1 = cpu_clock(cpu);
+		shrink_page_pools(&total_pages, &available_pages);
+		t2 = cpu_clock(cpu);
+		pr_debug("shrink page pools: time=%lldns, "
+			"total_pages_released=%d, free_pages_available=%d",
+			t2-t1, total_pages, available_pages);
+	}
+	return 0;
+}
+
+static int shrink_get(char *buff, const struct kernel_param *kp)
+{
+	return param_get_int(buff, kp);
+}
+
+static struct kernel_param_ops shrink_ops = {
+	.get = shrink_get,
+	.set = shrink_set,
+};
+
+module_param_cb(shrink_page_pools, &shrink_ops, &shrink_pp, 0644);
+#endif
+
+static int enable_pp_set(const char *arg, const struct kernel_param *kp)
+{
+	int ret;
+
+	ret = param_set_bool(arg, kp);
+	if (ret)
+		return ret;
+
+	if (!enable_pp)
+		nvmap_page_pool_clear();
+
+	return 0;
+}
+
+static int enable_pp_get(char *buff, const struct kernel_param *kp)
+{
+	return param_get_bool(buff, kp);
+}
+
+static struct kernel_param_ops enable_pp_ops = {
+	.get = enable_pp_get,
+	.set = enable_pp_set,
+};
+
+module_param_cb(enable_page_pools, &enable_pp_ops, &enable_pp, 0644);
+
+static int pool_size_set(const char *arg, const struct kernel_param *kp)
+{
+	int ret = param_set_uint(arg, kp);
+
+	if (!ret && (pool_size != nvmap_dev->pool.max))
+		nvmap_page_pool_resize(&nvmap_dev->pool, pool_size);
+
+	return ret;
+}
+
+static int pool_size_get(char *buff, const struct kernel_param *kp)
+{
+	return param_get_int(buff, kp);
+}
+
+static struct kernel_param_ops pool_size_ops = {
+	.get = pool_size_get,
+	.set = pool_size_set,
+};
+
+module_param_cb(pool_size, &pool_size_ops, &pool_size, 0644);
+
+int nvmap_page_pool_debugfs_init(struct dentry *nvmap_root)
+{
+	struct dentry *pp_root;
+
+	if (!nvmap_root)
+		return -ENODEV;
+
+	pp_root = debugfs_create_dir("pagepool", nvmap_root);
+	if (!pp_root)
+		return -ENODEV;
+
+	debugfs_create_u32("page_pool_available_pages",
+			   S_IRUGO, pp_root,
+			   &nvmap_dev->pool.count);
+	debugfs_create_u32("page_pool_pages_to_zero",
+			   S_IRUGO, pp_root,
+			   &nvmap_dev->pool.to_zero);
+	debugfs_create_u32("page_pool_available_big_pages",
+			   S_IRUGO, pp_root,
+			   &nvmap_dev->pool.big_page_count);
+	debugfs_create_u32("page_pool_big_page_size",
+			   S_IRUGO, pp_root,
+			   &nvmap_dev->pool.big_pg_sz);
+	debugfs_create_u64("total_big_page_allocs",
+			   S_IRUGO, pp_root,
+			   &nvmap_big_page_allocs);
+	debugfs_create_u64("total_page_allocs",
+			   S_IRUGO, pp_root,
+			   &nvmap_total_page_allocs);
+
+#ifdef CONFIG_NVMAP_PAGE_POOL_DEBUG
+	debugfs_create_u64("page_pool_allocs",
+			   S_IRUGO, pp_root,
+			   &nvmap_dev->pool.allocs);
+	debugfs_create_u64("page_pool_fills",
+			   S_IRUGO, pp_root,
+			   &nvmap_dev->pool.fills);
+	debugfs_create_u64("page_pool_hits",
+			   S_IRUGO, pp_root,
+			   &nvmap_dev->pool.hits);
+	debugfs_create_u64("page_pool_misses",
+			   S_IRUGO, pp_root,
+			   &nvmap_dev->pool.misses);
+#endif
+
+	return 0;
+}
+
+int nvmap_page_pool_init(struct nvmap_device *dev)
+{
+	struct sysinfo info;
+	struct nvmap_page_pool *pool = &dev->pool;
+
+	memset(pool, 0x0, sizeof(*pool));
+	rt_mutex_init(&pool->lock);
+	INIT_LIST_HEAD(&pool->page_list);
+	INIT_LIST_HEAD(&pool->zero_list);
+	INIT_LIST_HEAD(&pool->page_list_bp);
+
+	pool->big_pg_sz = NVMAP_PP_BIG_PAGE_SIZE;
+	pool->pages_per_big_pg = NVMAP_PP_BIG_PAGE_SIZE >> PAGE_SHIFT;
+
+	si_meminfo(&info);
+	pr_info("Total RAM pages: %lu\n", info.totalram);
+
+	if (!CONFIG_NVMAP_PAGE_POOL_SIZE)
+		/* The ratio is pool pages per 1K ram pages.
+		 * So, the >> 10 */
+		pool->max = (info.totalram * NVMAP_PP_POOL_SIZE) >> 10;
+	else
+		pool->max = CONFIG_NVMAP_PAGE_POOL_SIZE;
+
+	if (pool->max >= info.totalram)
+		goto fail;
+	pool_size = pool->max;
+
+	pr_info("nvmap page pool size: %u pages (%u MB)\n", pool->max,
+		(pool->max * info.mem_unit) >> 20);
+
+	background_allocator = kthread_run(nvmap_background_zero_thread,
+					    NULL, "nvmap-bz");
+	if (IS_ERR(background_allocator))
+		goto fail;
+
+	register_shrinker(&nvmap_page_pool_shrinker);
+
+	return 0;
+fail:
+	nvmap_page_pool_fini(dev);
+	return -ENOMEM;
+}
+
+int nvmap_page_pool_fini(struct nvmap_device *dev)
+{
+	struct nvmap_page_pool *pool = &dev->pool;
+
+	/*
+	 * if background allocator is not initialzed or not
+	 * properly initialized, then shrinker is also not
+	 * registered
+	 */
+	if (!IS_ERR_OR_NULL(background_allocator)) {
+		unregister_shrinker(&nvmap_page_pool_shrinker);
+		kthread_stop(background_allocator);
+	}
+
+	WARN_ON(!list_empty(&pool->page_list));
+
+	return 0;
+}
diff --git a/drivers/video/tegra/nvmap/nvmap_priv.h b/drivers/video/tegra/nvmap/nvmap_priv.h
new file mode 100644
index 000000000000..f3ad97cd12ea
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nvmap_priv.h
@@ -0,0 +1,770 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap.h
+ *
+ * GPU memory management driver for Tegra
+ *
+ * Copyright (c) 2009-2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __VIDEO_TEGRA_NVMAP_NVMAP_H
+#define __VIDEO_TEGRA_NVMAP_NVMAP_H
+
+#include <linux/list.h>
+#include <linux/mm.h>
+#include <linux/mutex.h>
+#include <linux/rtmutex.h>
+#include <linux/rbtree.h>
+#include <linux/sched.h>
+#include <linux/wait.h>
+#include <linux/atomic.h>
+#include <linux/dma-buf.h>
+#include <linux/syscalls.h>
+#include <linux/mm.h>
+#include <linux/miscdevice.h>
+#include <linux/nvmap.h>
+#include <linux/vmalloc.h>
+#include <linux/slab.h>
+#include <linux/version.h>
+
+#include <linux/workqueue.h>
+#include <linux/dma-mapping.h>
+#include <linux/dma-direction.h>
+#include <linux/platform_device.h>
+#include <linux/tegra-mce.h>
+#include <linux/of.h>
+#include <linux/of_reserved_mem.h>
+
+#include <asm/cacheflush.h>
+#ifndef CONFIG_ARM64
+#include <asm/outercache.h>
+#endif
+#include "nvmap_heap.h"
+#include "nvmap_stats.h"
+
+#define NVMAP_TAG_LABEL_MAXLEN	(63 - sizeof(struct nvmap_tag_entry))
+
+#define NVMAP_TP_ARGS_H(handle)					      	      \
+	handle,								      \
+	atomic_read(&handle->share_count),				      \
+	handle->heap_type == NVMAP_HEAP_IOVMM ? 0 : 			      \
+			(handle->carveout ? handle->carveout->base : 0),      \
+	handle->size,							      \
+	(handle->userflags & 0xFFFF),                                         \
+	(handle->userflags >> 16),					      \
+	__nvmap_tag_name(nvmap_dev, handle->userflags >> 16)
+
+#define NVMAP_TP_ARGS_CHR(client, handle, ref)			      	      \
+	client,                                                               \
+	client ? nvmap_client_pid((struct nvmap_client *)client) : 0,         \
+	(ref) ? atomic_read(&((struct nvmap_handle_ref *)ref)->dupes) : 1,    \
+	NVMAP_TP_ARGS_H(handle)
+
+#define NVMAP_TAG_TRACE(x, ...) 			\
+do {                                                    \
+	if (x##_enabled()) {                            \
+		mutex_lock(&nvmap_dev->tags_lock);      \
+		x(__VA_ARGS__);                         \
+		mutex_unlock(&nvmap_dev->tags_lock);    \
+	}                                               \
+} while (0)
+
+#define GFP_NVMAP       (GFP_KERNEL | __GFP_HIGHMEM | __GFP_NOWARN)
+
+struct page;
+struct nvmap_device;
+
+void _nvmap_handle_free(struct nvmap_handle *h);
+/* holds max number of handles allocted per process at any time */
+extern u32 nvmap_max_handle_count;
+extern u64 nvmap_big_page_allocs;
+extern u64 nvmap_total_page_allocs;
+
+extern bool nvmap_convert_iovmm_to_carveout;
+extern bool nvmap_convert_carveout_to_iovmm;
+
+extern struct vm_operations_struct nvmap_vma_ops;
+
+#ifdef CONFIG_ARM64
+#define PG_PROT_KERNEL PAGE_KERNEL
+#define FLUSH_DCACHE_AREA __flush_dcache_area
+#define outer_flush_range(s, e)
+#define outer_inv_range(s, e)
+#define outer_clean_range(s, e)
+#define outer_flush_all()
+#define outer_clean_all()
+extern void __clean_dcache_page(struct page *);
+#else
+#define PG_PROT_KERNEL pgprot_kernel
+#define FLUSH_DCACHE_AREA __cpuc_flush_dcache_area
+extern void __flush_dcache_page(struct address_space *, struct page *);
+#endif
+
+struct nvmap_vma_list {
+	struct list_head list;
+	struct vm_area_struct *vma;
+	unsigned long save_vm_flags;
+	pid_t pid;
+	atomic_t ref;
+};
+
+struct nvmap_carveout_node {
+	unsigned int		heap_bit;
+	struct nvmap_heap	*carveout;
+	int			index;
+	phys_addr_t		base;
+	size_t			size;
+};
+
+/* handles allocated using shared system memory (either IOVMM- or high-order
+ * page allocations */
+struct nvmap_pgalloc {
+	struct page **pages;
+	bool contig;			/* contiguous system memory */
+	atomic_t reserved;
+	atomic_t ndirty;	/* count number of dirty pages */
+};
+
+#define NVMAP_IVM_ALIGNMENT    (SZ_32K)
+
+/*
+ * Bits     | Field
+ * --------------------------------------
+ * [32]     | isVPR
+ * [31..29] | Peer
+ * [28..16] | Offset (aligned to 32K)
+ * [15..00] | Size (aligned to page_size)
+ */
+#define NVMAP_IVM_ISVPR_SHIFT  32U
+#define NVMAP_IVM_ISVPR_MASK   GENMASK_ULL(32, 32)
+#define NVMAP_IVM_PEER_SHIFT   29U
+#define NVMAP_IVM_PEER_MASK    GENMASK_ULL(31, 29)
+#define NVMAP_IVM_OFFSET_SHIFT 16U
+#define NVMAP_IVM_OFFSET_MASK  GENMASK_ULL(28, 16)
+#define NVMAP_IVM_SIZE_SHIFT   0U
+#define NVMAP_IVM_SIZE_MASK    GENMASK_ULL(15, 0)
+
+struct nvmap_handle_dmabuf_priv {
+	void *priv;
+	struct device *dev;
+	void (*priv_release)(void *priv);
+	struct list_head list;
+};
+
+struct nvmap_handle {
+	struct rb_node node;	/* entry on global handle tree */
+	atomic_t ref;		/* reference count (i.e., # of duplications) */
+	atomic_t pin;		/* pin count */
+	u32 flags;		/* caching flags */
+	size_t size;		/* padded (as-allocated) size */
+	size_t orig_size;	/* original (as-requested) size */
+	size_t align;
+	struct nvmap_client *owner;
+	struct dma_buf *dmabuf;
+	union {
+		struct nvmap_pgalloc pgalloc;
+		struct nvmap_heap_block *carveout;
+	};
+	bool heap_pgalloc;	/* handle is page allocated (sysmem / iovmm) */
+	bool alloc;		/* handle has memory allocated */
+	bool from_va;		/* handle memory is from VA */
+	u32 heap_type;		/* handle heap is allocated from */
+	u32 userflags;		/* flags passed from userspace */
+	void *vaddr;		/* mapping used inside kernel */
+	struct list_head vmas;	/* list of all user vma's */
+	atomic_t umap_count;	/* number of outstanding maps from user */
+	atomic_t kmap_count;	/* number of outstanding map from kernel */
+	atomic_t share_count;	/* number of processes sharing the handle */
+	struct list_head lru;	/* list head to track the lru */
+	struct mutex lock;
+	struct list_head dmabuf_priv;
+	u64 ivm_id;
+	int peer;		/* Peer VM number */
+	bool is_ro;		/* Is handle read-only? */
+};
+
+struct nvmap_handle_info {
+	struct nvmap_handle *handle;
+	struct list_head maps;
+	struct mutex maps_lock;
+};
+
+struct nvmap_tag_entry {
+	struct rb_node node;
+	atomic_t ref;		/* reference count (i.e., # of duplications) */
+	u32 tag;
+};
+
+/* handle_ref objects are client-local references to an nvmap_handle;
+ * they are distinct objects so that handles can be unpinned and
+ * unreferenced the correct number of times when a client abnormally
+ * terminates */
+struct nvmap_handle_ref {
+	struct nvmap_handle *handle;
+	struct rb_node	node;
+	atomic_t	dupes;	/* number of times to free on file close */
+};
+
+#ifdef CONFIG_NVMAP_PAGE_POOLS
+/*
+ * This is the default ratio defining pool size. It can be thought of as pool
+ * size in either MB per GB or KB per MB. That means the max this number can
+ * be is 1024 (all physical memory - not a very good idea) or 0 (no page pool
+ * at all).
+ */
+#define NVMAP_PP_POOL_SIZE               (128)
+
+#define NVMAP_PP_BIG_PAGE_SIZE           (0x10000)
+
+struct nvmap_page_pool {
+	struct rt_mutex lock;
+	u32 count;      /* Number of pages in the page & dirty list. */
+	u32 max;        /* Max no. of pages in all lists. */
+	u32 to_zero;    /* Number of pages on the zero list */
+	u32 under_zero; /* Number of pages getting zeroed */
+	u32 big_pg_sz;  /* big page size supported(64k, etc.) */
+	u32 big_page_count;   /* Number of zeroed big pages avaialble */
+	u32 pages_per_big_pg; /* Number of pages in big page */
+	struct list_head page_list;
+	struct list_head zero_list;
+	struct list_head page_list_bp;
+
+#ifdef CONFIG_NVMAP_PAGE_POOL_DEBUG
+	u64 allocs;
+	u64 fills;
+	u64 hits;
+	u64 misses;
+#endif
+};
+
+int nvmap_page_pool_init(struct nvmap_device *dev);
+int nvmap_page_pool_fini(struct nvmap_device *dev);
+struct page *nvmap_page_pool_alloc(struct nvmap_page_pool *pool);
+int nvmap_page_pool_alloc_lots(struct nvmap_page_pool *pool,
+					struct page **pages, u32 nr);
+int nvmap_page_pool_alloc_lots_bp(struct nvmap_page_pool *pool,
+					struct page **pages, u32 nr);
+int nvmap_page_pool_fill_lots(struct nvmap_page_pool *pool,
+				       struct page **pages, u32 nr);
+int nvmap_page_pool_clear(void);
+int nvmap_page_pool_debugfs_init(struct dentry *nvmap_root);
+#endif
+
+#define NVMAP_IVM_INVALID_PEER		(-1)
+
+struct nvmap_client {
+	const char			*name;
+	struct rb_root			handle_refs;
+	struct mutex			ref_lock;
+	bool				kernel_client;
+	atomic_t			count;
+	struct task_struct		*task;
+	struct list_head		list;
+	u32				handle_count;
+	u32				next_fd;
+	int				warned;
+	int				tag_warned;
+};
+
+struct nvmap_vma_priv {
+	struct nvmap_handle *handle;
+	size_t		offs;
+	atomic_t	count;	/* number of processes cloning the VMA */
+};
+
+struct nvmap_device {
+	struct rb_root	handles;
+	spinlock_t	handle_lock;
+	struct miscdevice dev_user;
+	struct nvmap_carveout_node *heaps;
+	int nr_heaps;
+	int nr_carveouts;
+#ifdef CONFIG_NVMAP_PAGE_POOLS
+	struct nvmap_page_pool pool;
+#endif
+	struct list_head clients;
+	struct rb_root pids;
+	struct mutex	clients_lock;
+	struct list_head lru_handles;
+	spinlock_t	lru_lock;
+	struct dentry *handles_by_pid;
+	struct dentry *debug_root;
+	struct nvmap_platform_data *plat;
+	struct rb_root	tags;
+	struct mutex	tags_lock;
+	u32 dynamic_dma_map_mask;
+	u32 cpu_access_mask;
+};
+
+
+extern struct nvmap_device *nvmap_dev;
+extern ulong nvmap_init_time;
+
+static inline void nvmap_ref_lock(struct nvmap_client *priv)
+{
+	mutex_lock(&priv->ref_lock);
+}
+
+static inline void nvmap_ref_unlock(struct nvmap_client *priv)
+{
+	mutex_unlock(&priv->ref_lock);
+}
+
+/*
+ * NOTE: this does not ensure the continued existence of the underlying
+ * dma_buf. If you want ensure the existence of the dma_buf you must get an
+ * nvmap_handle_ref as that is what tracks the dma_buf refs.
+ */
+static inline struct nvmap_handle *nvmap_handle_get(struct nvmap_handle *h)
+{
+	if (WARN_ON(!virt_addr_valid(h))) {
+		pr_err("%s: invalid handle\n", current->group_leader->comm);
+		return NULL;
+	}
+
+	if (unlikely(atomic_inc_return(&h->ref) <= 1)) {
+		pr_err("%s: %s attempt to get a freed handle\n",
+			__func__, current->group_leader->comm);
+		atomic_dec(&h->ref);
+		return NULL;
+	}
+	return h;
+}
+
+static inline pgprot_t nvmap_pgprot(struct nvmap_handle *h, pgprot_t prot)
+{
+	if (h->flags == NVMAP_HANDLE_UNCACHEABLE) {
+#ifdef CONFIG_ARM64
+		const bool heap_is_not_vpr = !(h->heap_type &
+			(NVMAP_HEAP_CARVEOUT_VPR |
+				NVMAP_HEAP_CARVEOUT_IVM_VPR));
+
+		if (heap_is_not_vpr && h->owner && !h->owner->warned) {
+			char task_comm[TASK_COMM_LEN];
+			h->owner->warned = 1;
+			get_task_comm(task_comm, h->owner->task);
+			pr_err("PID %d: %s: TAG: 0x%04x WARNING: "
+				"NVMAP_HANDLE_WRITE_COMBINE "
+				"should be used in place of "
+				"NVMAP_HANDLE_UNCACHEABLE on ARM64\n",
+				h->owner->task->pid, task_comm,
+				h->userflags >> 16);
+		}
+#endif
+		return pgprot_noncached(prot);
+	}
+	else if (h->flags == NVMAP_HANDLE_WRITE_COMBINE)
+		return pgprot_writecombine(prot);
+	return prot;
+}
+
+int nvmap_probe(struct platform_device *pdev);
+int nvmap_remove(struct platform_device *pdev);
+int nvmap_init(struct platform_device *pdev);
+
+int nvmap_create_carveout(const struct nvmap_platform_carveout *co);
+int nvmap_co_setup(struct reserved_mem *rmem);
+
+struct device *dma_dev_from_handle(unsigned long type);
+struct nvmap_heap_block *nvmap_carveout_alloc(struct nvmap_client *dev,
+					      struct nvmap_handle *handle,
+					      unsigned long type,
+					      phys_addr_t *start);
+
+struct nvmap_carveout_node;
+
+void nvmap_handle_put(struct nvmap_handle *h);
+
+struct nvmap_handle_ref *__nvmap_validate_locked(struct nvmap_client *priv,
+						 struct nvmap_handle *h);
+
+struct nvmap_handle *nvmap_validate_get(struct nvmap_handle *h);
+
+struct nvmap_handle_ref *nvmap_create_handle(struct nvmap_client *client,
+					     size_t size, bool ro_buf);
+
+struct nvmap_handle_ref *nvmap_create_handle_from_va(struct nvmap_client *client,
+						     ulong addr, size_t size,
+						     unsigned int access_flags);
+
+struct nvmap_handle_ref *nvmap_duplicate_handle(struct nvmap_client *client,
+					struct nvmap_handle *h, bool skip_val);
+
+struct nvmap_handle_ref *nvmap_try_duplicate_by_ivmid(
+			struct nvmap_client *client, u64 ivm_id,
+			struct nvmap_heap_block **block);
+
+struct nvmap_handle_ref *nvmap_create_handle_from_fd(
+			struct nvmap_client *client, int fd);
+
+void inner_cache_maint(unsigned int op, void *vaddr, size_t size);
+void outer_cache_maint(unsigned int op, phys_addr_t paddr, size_t size);
+
+int nvmap_alloc_handle(struct nvmap_client *client,
+		       struct nvmap_handle *h, unsigned int heap_mask,
+		       size_t align, u8 kind,
+		       unsigned int flags, int peer);
+
+int nvmap_alloc_handle_from_va(struct nvmap_client *client,
+			       struct nvmap_handle *h,
+			       ulong addr,
+			       unsigned int flags);
+
+void nvmap_free_handle(struct nvmap_client *c, struct nvmap_handle *h);
+
+void nvmap_free_handle_fd(struct nvmap_client *c, int fd);
+
+int nvmap_handle_remove(struct nvmap_device *dev, struct nvmap_handle *h);
+
+void nvmap_handle_add(struct nvmap_device *dev, struct nvmap_handle *h);
+
+int is_nvmap_vma(struct vm_area_struct *vma);
+
+int nvmap_get_dmabuf_fd(struct nvmap_client *client, struct nvmap_handle *h);
+struct nvmap_handle *nvmap_handle_get_from_dmabuf_fd(
+				struct nvmap_client *client, int fd);
+int nvmap_dmabuf_duplicate_gen_fd(struct nvmap_client *client,
+		struct dma_buf *dmabuf);
+struct sg_table *nvmap_dmabuf_map_dma_buf(
+	struct dma_buf_attachment *attach, enum dma_data_direction dir);
+struct sg_table *_nvmap_dmabuf_map_dma_buf(
+	struct dma_buf_attachment *attach, enum dma_data_direction dir);
+void nvmap_dmabuf_unmap_dma_buf(struct dma_buf_attachment *attach,
+				       struct sg_table *sgt,
+				       enum dma_data_direction dir);
+void _nvmap_dmabuf_unmap_dma_buf(struct dma_buf_attachment *attach,
+				       struct sg_table *sgt,
+				       enum dma_data_direction dir);
+
+int nvmap_get_handle_param(struct nvmap_client *client,
+		struct nvmap_handle_ref *ref, u32 param, u64 *result);
+
+struct nvmap_handle *nvmap_handle_get_from_fd(int fd);
+
+/* MM definitions. */
+extern size_t cache_maint_inner_threshold;
+extern int nvmap_cache_maint_by_set_ways;
+
+extern void v7_flush_kern_cache_all(void);
+extern void v7_clean_kern_cache_all(void *);
+
+extern void (*inner_flush_cache_all)(void);
+extern void (*inner_clean_cache_all)(void);
+void nvmap_override_cache_ops(void);
+void nvmap_clean_cache(struct page **pages, int numpages);
+void nvmap_clean_cache_page(struct page *page);
+void nvmap_flush_cache(struct page **pages, int numpages);
+int nvmap_cache_maint_phys_range(unsigned int op, phys_addr_t pstart,
+		phys_addr_t pend, int inner, int outer);
+
+int nvmap_do_cache_maint_list(struct nvmap_handle **handles, u64 *offsets,
+			      u64 *sizes, int op, u32 nr_ops, bool is_32);
+int __nvmap_cache_maint(struct nvmap_client *client,
+			       struct nvmap_cache_op_64 *op);
+int nvmap_cache_debugfs_init(struct dentry *nvmap_root);
+
+/* Internal API to support dmabuf */
+struct dma_buf *__nvmap_dmabuf_export(struct nvmap_client *client,
+				 struct nvmap_handle *handle);
+struct dma_buf *__nvmap_make_dmabuf(struct nvmap_client *client,
+				    struct nvmap_handle *handle, bool ro_buf);
+struct sg_table *__nvmap_sg_table(struct nvmap_client *client,
+				  struct nvmap_handle *h);
+void __nvmap_free_sg_table(struct nvmap_client *client,
+			   struct nvmap_handle *h, struct sg_table *sgt);
+void *__nvmap_kmap(struct nvmap_handle *h, unsigned int pagenum);
+void __nvmap_kunmap(struct nvmap_handle *h, unsigned int pagenum, void *addr);
+void *__nvmap_mmap(struct nvmap_handle *h);
+void __nvmap_munmap(struct nvmap_handle *h, void *addr);
+int __nvmap_map(struct nvmap_handle *h, struct vm_area_struct *vma);
+int __nvmap_do_cache_maint(struct nvmap_client *client, struct nvmap_handle *h,
+			   unsigned long start, unsigned long end,
+			   unsigned int op, bool clean_only_dirty);
+struct nvmap_client *__nvmap_create_client(struct nvmap_device *dev,
+					   const char *name);
+int __nvmap_dmabuf_fd(struct nvmap_client *client,
+		      struct dma_buf *dmabuf, int flags);
+
+int nvmap_dmabuf_stash_init(void);
+
+void *nvmap_altalloc(size_t len);
+void nvmap_altfree(void *ptr, size_t len);
+
+void do_set_pte(struct vm_area_struct *vma, unsigned long address,
+		struct page *page, pte_t *pte, bool write, bool anon);
+
+static inline struct page *nvmap_to_page(struct page *page)
+{
+	return (struct page *)((unsigned long)page & ~3UL);
+}
+
+static inline bool nvmap_page_dirty(struct page *page)
+{
+	return (unsigned long)page & 1UL;
+}
+
+static inline bool nvmap_page_mkdirty(struct page **page)
+{
+	if (nvmap_page_dirty(*page))
+		return false;
+	*page = (struct page *)((unsigned long)*page | 1UL);
+	return true;
+}
+
+static inline bool nvmap_page_mkclean(struct page **page)
+{
+	if (!nvmap_page_dirty(*page))
+		return false;
+	*page = (struct page *)((unsigned long)*page & ~1UL);
+	return true;
+}
+
+/*
+ * FIXME: assume user space requests for reserve operations
+ * are page aligned
+ */
+static inline int nvmap_handle_mk(struct nvmap_handle *h,
+				  u32 offset, u32 size,
+				  bool (*fn)(struct page **),
+				  bool locked)
+{
+	int i, nchanged = 0;
+	u32 start_page = offset >> PAGE_SHIFT;
+	u32 end_page = PAGE_ALIGN(offset + size) >> PAGE_SHIFT;
+
+	if (!locked)
+		mutex_lock(&h->lock);
+	if (h->heap_pgalloc &&
+		(offset < h->size) &&
+		(size <= h->size) &&
+		(offset <= (h->size - size))) {
+		for (i = start_page; i < end_page; i++)
+			nchanged += fn(&h->pgalloc.pages[i]) ? 1 : 0;
+	}
+	if (!locked)
+		mutex_unlock(&h->lock);
+	return nchanged;
+}
+
+static inline void nvmap_handle_mkclean(struct nvmap_handle *h,
+					u32 offset, u32 size)
+{
+	int nchanged;
+
+	if (h->heap_pgalloc && !atomic_read(&h->pgalloc.ndirty))
+		return;
+	if (size == 0)
+		size = h->size;
+
+	nchanged = nvmap_handle_mk(h, offset, size, nvmap_page_mkclean, false);
+	if (h->heap_pgalloc)
+		atomic_sub(nchanged, &h->pgalloc.ndirty);
+}
+
+static inline void _nvmap_handle_mkdirty(struct nvmap_handle *h,
+					u32 offset, u32 size)
+{
+	int nchanged;
+
+	if (h->heap_pgalloc &&
+		(atomic_read(&h->pgalloc.ndirty) == (h->size >> PAGE_SHIFT)))
+		return;
+
+	nchanged = nvmap_handle_mk(h, offset, size, nvmap_page_mkdirty, true);
+	if (h->heap_pgalloc)
+		atomic_add(nchanged, &h->pgalloc.ndirty);
+}
+
+static inline struct page **nvmap_pages(struct page **pg_pages, u32 nr_pages)
+{
+	struct page **pages;
+	int i;
+
+	pages = nvmap_altalloc(sizeof(*pages) * nr_pages);
+	if (!pages)
+		return NULL;
+
+	for (i = 0; i < nr_pages; i++)
+		pages[i] = nvmap_to_page(pg_pages[i]);
+
+	return pages;
+}
+
+void nvmap_zap_handle(struct nvmap_handle *handle, u64 offset, u64 size);
+
+void nvmap_vma_open(struct vm_area_struct *vma);
+
+int nvmap_reserve_pages(struct nvmap_handle **handles, u64 *offsets,
+			u64 *sizes, u32 nr, u32 op, bool is_32);
+
+static inline void nvmap_kmaps_inc(struct nvmap_handle *h)
+{
+	mutex_lock(&h->lock);
+	atomic_inc(&h->kmap_count);
+	mutex_unlock(&h->lock);
+}
+
+static inline void nvmap_kmaps_inc_no_lock(struct nvmap_handle *h)
+{
+	atomic_inc(&h->kmap_count);
+}
+
+static inline void nvmap_kmaps_dec(struct nvmap_handle *h)
+{
+	atomic_dec(&h->kmap_count);
+}
+
+static inline void nvmap_umaps_inc(struct nvmap_handle *h)
+{
+	mutex_lock(&h->lock);
+	atomic_inc(&h->umap_count);
+	mutex_unlock(&h->lock);
+}
+
+static inline void nvmap_umaps_dec(struct nvmap_handle *h)
+{
+	atomic_dec(&h->umap_count);
+}
+
+static inline void nvmap_lru_add(struct nvmap_handle *h)
+{
+	spin_lock(&nvmap_dev->lru_lock);
+	BUG_ON(!list_empty(&h->lru));
+	list_add_tail(&h->lru, &nvmap_dev->lru_handles);
+	spin_unlock(&nvmap_dev->lru_lock);
+}
+
+static inline void nvmap_lru_del(struct nvmap_handle *h)
+{
+	spin_lock(&nvmap_dev->lru_lock);
+	list_del(&h->lru);
+	INIT_LIST_HEAD(&h->lru);
+	spin_unlock(&nvmap_dev->lru_lock);
+}
+
+static inline void nvmap_lru_reset(struct nvmap_handle *h)
+{
+	spin_lock(&nvmap_dev->lru_lock);
+	BUG_ON(list_empty(&h->lru));
+	list_del(&h->lru);
+	list_add_tail(&h->lru, &nvmap_dev->lru_handles);
+	spin_unlock(&nvmap_dev->lru_lock);
+}
+
+static inline bool nvmap_handle_track_dirty(struct nvmap_handle *h)
+{
+	if (!h->heap_pgalloc)
+		return false;
+
+	return h->userflags & (NVMAP_HANDLE_CACHE_SYNC |
+			       NVMAP_HANDLE_CACHE_SYNC_AT_RESERVE);
+}
+
+struct nvmap_tag_entry *nvmap_search_tag_entry(struct rb_root *root, u32 tag);
+
+int nvmap_define_tag(struct nvmap_device *dev, u32 tag,
+	const char __user *name, u32 len);
+
+int nvmap_remove_tag(struct nvmap_device *dev, u32 tag);
+
+/* must hold tag_lock */
+static inline char *__nvmap_tag_name(struct nvmap_device *dev, u32 tag)
+{
+	struct nvmap_tag_entry *entry;
+
+	entry = nvmap_search_tag_entry(&dev->tags, tag);
+	return entry ? (char *)(entry + 1) : "";
+}
+static inline pid_t nvmap_client_pid(struct nvmap_client *client)
+{
+	return client->task ? client->task->pid : 0;
+}
+
+static inline int nvmap_get_user_pages(ulong vaddr,
+				size_t nr_page, struct page **pages,
+				bool is_user_flags, u32 user_foll_flags)
+{
+	u32 foll_flags = FOLL_FORCE;
+	struct vm_area_struct *vma;
+	vm_flags_t vm_flags;
+	long user_pages = 0;
+	int ret = 0;
+
+	down_read(&current->mm->mmap_lock);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 9, 0)
+        user_pages = get_user_pages(current, current->mm,
+			      vaddr & PAGE_MASK, nr_page,
+			      1/*write*/, 1, /* force */
+			      pages, NULL);
+#else
+	vma = find_vma(current->mm, vaddr);
+	if (vma) {
+		if (is_user_flags) {
+			foll_flags |= user_foll_flags;
+		} else {
+			vm_flags = vma->vm_flags;
+			/*
+			 * If the vaddr points to writable page then only
+			 * pass FOLL_WRITE flag
+			 */
+			if (vm_flags & VM_WRITE)
+				foll_flags |= FOLL_WRITE;
+		}
+		pr_debug("vaddr %lu is_user_flags %d user_foll_flags %x foll_flags %x.\n",
+			vaddr, is_user_flags?1:0, user_foll_flags, foll_flags);
+		user_pages = get_user_pages(vaddr & PAGE_MASK, nr_page,
+					    foll_flags, pages);
+	}
+#endif
+	up_read(&current->mm->mmap_lock);
+	if (user_pages != nr_page) {
+		ret = user_pages < 0 ? user_pages : -ENOMEM;
+		pr_err("get_user_pages requested/got: %zu/%ld]\n", nr_page,
+				user_pages);
+		while (--user_pages >= 0)
+			put_page(pages[user_pages]);
+	}
+	return ret;
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 9, 0)
+#define device_node_from_iter(iter) \
+	iter.node
+#else
+#define device_node_from_iter(iter) \
+	iter.out_args.np
+#endif
+
+#define CALL_CLEAN_CACHE_ON_INIT 1
+#define CALL_FLUSH_CACHE_ON_INIT 2
+
+struct nvmap_chip_cache_op {
+	void (*inner_clean_cache_all)(void);
+	void (*inner_flush_cache_all)(void);
+	void (*nvmap_get_cacheability)(struct nvmap_handle *h,
+		bool *inner, bool *outer);
+	const char *name;
+	int flags;
+};
+
+void nvmap_select_cache_ops(struct device *dev);
+
+typedef void (*nvmap_setup_chip_cache_fn)(struct nvmap_chip_cache_op *);
+
+extern struct of_device_id __nvmapcache_of_table;
+
+#define NVMAP_CACHE_OF_DECLARE(compat, fn) \
+	_OF_DECLARE(nvmapcache, nvmapcache_of, compat, fn, \
+			nvmap_setup_chip_cache_fn)
+
+bool nvmap_memory_available(size_t size);
+
+#endif /* __VIDEO_TEGRA_NVMAP_NVMAP_H */
diff --git a/drivers/video/tegra/nvmap/nvmap_stats.c b/drivers/video/tegra/nvmap/nvmap_stats.c
new file mode 100644
index 000000000000..7be8aeefec21
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nvmap_stats.c
@@ -0,0 +1,106 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_stats.c
+ *
+ * Nvmap Stats keeping
+ *
+ * Copyright (c) 2011-2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#include <linux/debugfs.h>
+
+#include "nvmap_priv.h"
+
+struct nvmap_stats nvmap_stats;
+
+static int nvmap_stats_reset(void *data, u64 val)
+{
+	int i;
+
+	if (val) {
+		atomic64_set(&nvmap_stats.collect, 0);
+		for (i = 0; i < NS_NUM; i++) {
+			if (i == NS_TOTAL)
+				continue;
+			atomic64_set(&nvmap_stats.stats[i], 0);
+		}
+	}
+	return 0;
+}
+
+static int nvmap_stats_get(void *data, u64 *val)
+{
+	atomic64_t *ptr = data;
+
+	*val = atomic64_read(ptr);
+	return 0;
+}
+
+static int nvmap_stats_set(void *data, u64 val)
+{
+	atomic64_t *ptr = data;
+
+	atomic64_set(ptr, val);
+	return 0;
+}
+
+DEFINE_SIMPLE_ATTRIBUTE(reset_stats_fops, NULL, nvmap_stats_reset, "%llu\n");
+DEFINE_SIMPLE_ATTRIBUTE(stats_fops, nvmap_stats_get, nvmap_stats_set, "%llu\n");
+
+void nvmap_stats_init(struct dentry *nvmap_debug_root)
+{
+	struct dentry *stats_root;
+
+#define CREATE_DF(x, y) \
+	debugfs_create_file(#x, S_IRUGO, stats_root, &y, &stats_fops);
+
+	stats_root = debugfs_create_dir("stats", nvmap_debug_root);
+	if (!IS_ERR_OR_NULL(stats_root)) {
+		CREATE_DF(alloc, nvmap_stats.stats[NS_ALLOC]);
+		CREATE_DF(release, nvmap_stats.stats[NS_RELEASE]);
+		CREATE_DF(ualloc, nvmap_stats.stats[NS_UALLOC]);
+		CREATE_DF(urelease, nvmap_stats.stats[NS_URELEASE]);
+		CREATE_DF(kalloc, nvmap_stats.stats[NS_KALLOC]);
+		CREATE_DF(krelease, nvmap_stats.stats[NS_KRELEASE]);
+		CREATE_DF(cflush_rq, nvmap_stats.stats[NS_CFLUSH_RQ]);
+		CREATE_DF(cflush_done, nvmap_stats.stats[NS_CFLUSH_DONE]);
+		CREATE_DF(ucflush_rq, nvmap_stats.stats[NS_UCFLUSH_RQ]);
+		CREATE_DF(ucflush_done, nvmap_stats.stats[NS_UCFLUSH_DONE]);
+		CREATE_DF(kcflush_rq, nvmap_stats.stats[NS_KCFLUSH_RQ]);
+		CREATE_DF(kcflush_done, nvmap_stats.stats[NS_KCFLUSH_DONE]);
+		CREATE_DF(total_memory, nvmap_stats.stats[NS_TOTAL]);
+
+		debugfs_create_file("collect", S_IRUGO | S_IWUSR,
+			stats_root, &nvmap_stats.collect, &stats_fops);
+		debugfs_create_file("reset", S_IWUSR,
+			stats_root, NULL, &reset_stats_fops);
+	}
+
+#undef CREATE_DF
+}
+
+void nvmap_stats_inc(enum nvmap_stats_t stat, size_t size)
+{
+	if (atomic64_read(&nvmap_stats.collect) || stat == NS_TOTAL)
+		atomic64_add(size, &nvmap_stats.stats[stat]);
+}
+
+void nvmap_stats_dec(enum nvmap_stats_t stat, size_t size)
+{
+	if (atomic64_read(&nvmap_stats.collect) || stat == NS_TOTAL)
+		atomic64_sub(size, &nvmap_stats.stats[stat]);
+}
+
+u64 nvmap_stats_read(enum nvmap_stats_t stat)
+{
+	return atomic64_read(&nvmap_stats.stats[stat]);
+}
+
diff --git a/drivers/video/tegra/nvmap/nvmap_stats.h b/drivers/video/tegra/nvmap/nvmap_stats.h
new file mode 100644
index 000000000000..3e2d3e806c87
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nvmap_stats.h
@@ -0,0 +1,45 @@
+/*
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __VIDEO_TEGRA_NVMAP_STATS_H
+#define __VIDEO_TEGRA_NVMAP_STATS_H
+
+enum nvmap_stats_t {
+	NS_ALLOC = 0,
+	NS_RELEASE,
+	NS_UALLOC,
+	NS_URELEASE,
+	NS_KALLOC,
+	NS_KRELEASE,
+	NS_CFLUSH_RQ,
+	NS_CFLUSH_DONE,
+	NS_UCFLUSH_RQ,
+	NS_UCFLUSH_DONE,
+	NS_KCFLUSH_RQ,
+	NS_KCFLUSH_DONE,
+	NS_TOTAL,
+	NS_NUM,
+};
+
+struct nvmap_stats {
+	atomic64_t stats[NS_NUM];
+	atomic64_t collect;
+};
+
+extern struct nvmap_stats nvmap_stats;
+
+void nvmap_stats_init(struct dentry *nvmap_debug_root);
+void nvmap_stats_inc(enum nvmap_stats_t, size_t size);
+void nvmap_stats_dec(enum nvmap_stats_t, size_t size);
+u64 nvmap_stats_read(enum nvmap_stats_t);
+#endif /* __VIDEO_TEGRA_NVMAP_STATS_H */
diff --git a/drivers/video/tegra/nvmap/nvmap_tag.c b/drivers/video/tegra/nvmap/nvmap_tag.c
new file mode 100644
index 000000000000..0efd8ecc17f7
--- /dev/null
+++ b/drivers/video/tegra/nvmap/nvmap_tag.c
@@ -0,0 +1,113 @@
+/*
+ * drivers/video/tegra/nvmap/nvmap_tag.c
+ *
+ * Allocation tag routines for nvmap
+ *
+ * Copyright (c) 2016-2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"%s: " fmt, __func__
+
+#include <linux/moduleparam.h>
+
+#include <trace/events/nvmap.h>
+
+#include "nvmap_priv.h"
+
+struct nvmap_tag_entry *nvmap_search_tag_entry(struct rb_root *root, u32 tag)
+{
+	struct rb_node *node = root->rb_node;  /* top of the tree */
+	struct nvmap_tag_entry *entry;
+
+	while (node) {
+		entry = rb_entry(node, struct nvmap_tag_entry, node);
+
+		if (entry->tag > tag)
+			node = node->rb_left;
+		else if (entry->tag < tag)
+			node = node->rb_right;
+		else
+			return entry;  /* Found it */
+	}
+	return NULL;
+}
+
+
+static void nvmap_insert_tag_entry(struct rb_root *root,
+		struct nvmap_tag_entry *new)
+{
+	struct rb_node **link = &root->rb_node;
+	struct rb_node *parent = NULL;
+	struct nvmap_tag_entry *entry;
+	u32 tag = new->tag;
+
+	/* Go to the bottom of the tree */
+	while (*link) {
+	    parent = *link;
+	    entry = rb_entry(parent, struct nvmap_tag_entry, node);
+
+	    if (entry->tag > tag)
+		link = &parent->rb_left;
+	    else
+		link = &parent->rb_right;
+	}
+
+	/* Put the new node there */
+	rb_link_node(&new->node, parent, link);
+	rb_insert_color(&new->node, root);
+}
+
+
+int nvmap_define_tag(struct nvmap_device *dev, u32 tag,
+		const char __user *name, u32 len)
+{
+	struct nvmap_tag_entry *new;
+	struct nvmap_tag_entry *old;
+
+	new = kzalloc(sizeof(struct nvmap_tag_entry) + len + 1, GFP_KERNEL);
+	if (!new)
+		return -ENOMEM;
+
+	if (copy_from_user(new + 1, name, len)) {
+		kfree(new);
+		return -EFAULT;
+	}
+
+	new->tag = tag;
+
+	mutex_lock(&dev->tags_lock);
+	old = nvmap_search_tag_entry(&dev->tags, tag);
+	if (old) {
+		rb_replace_node(&old->node, &new->node, &dev->tags);
+		kfree(old);
+	} else {
+		nvmap_insert_tag_entry(&dev->tags, new);
+	}
+	mutex_unlock(&dev->tags_lock);
+
+	return 0;
+}
+
+int nvmap_remove_tag(struct nvmap_device *dev, u32 tag)
+{
+	struct nvmap_tag_entry *old;
+
+	mutex_lock(&dev->tags_lock);
+	old = nvmap_search_tag_entry(&dev->tags, tag);
+	if (old){
+		rb_erase(&old->node, &dev->tags);
+		kfree(old);
+	}
+	mutex_unlock(&dev->tags_lock);
+
+	return 0;
+}
diff --git a/include/linux/nvmap.h b/include/linux/nvmap.h
new file mode 100644
index 000000000000..bbecb27485ed
--- /dev/null
+++ b/include/linux/nvmap.h
@@ -0,0 +1,110 @@
+/*
+ * include/linux/nvmap.h
+ *
+ * structure declarations for nvmem and nvmap user-space ioctls
+ *
+ * Copyright (c) 2009-2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef _LINUX_NVMAP_H
+#define _LINUX_NVMAP_H
+
+#include <linux/rbtree.h>
+#include <linux/file.h>
+#include <linux/dma-buf.h>
+#include <linux/device.h>
+#include <uapi/linux/nvmap.h>
+
+#define NVMAP_HEAP_IOVMM   (1ul<<30)
+
+/* common carveout heaps */
+#define NVMAP_HEAP_CARVEOUT_IRAM    (1ul<<29)
+#define NVMAP_HEAP_CARVEOUT_VPR     (1ul<<28)
+#define NVMAP_HEAP_CARVEOUT_TSEC    (1ul<<27)
+#define NVMAP_HEAP_CARVEOUT_VIDMEM  (1ul<<26)
+#define NVMAP_HEAP_CARVEOUT_IVM_VPR (1ul<<2)
+#define NVMAP_HEAP_CARVEOUT_IVM     (1ul<<1)
+#define NVMAP_HEAP_CARVEOUT_GENERIC (1ul<<0)
+
+#define NVMAP_HEAP_CARVEOUT_MASK    (NVMAP_HEAP_IOVMM - 1)
+
+/* allocation flags */
+#define NVMAP_HANDLE_UNCACHEABLE     (0x0ul << 0)
+#define NVMAP_HANDLE_WRITE_COMBINE   (0x1ul << 0)
+#define NVMAP_HANDLE_INNER_CACHEABLE (0x2ul << 0)
+#define NVMAP_HANDLE_CACHEABLE       (0x3ul << 0)
+#define NVMAP_HANDLE_CACHE_FLAG      (0x3ul << 0)
+
+#define NVMAP_HANDLE_SECURE          (0x1ul << 2)
+#define NVMAP_HANDLE_KIND_SPECIFIED  (0x1ul << 3)
+#define NVMAP_HANDLE_COMPR_SPECIFIED (0x1ul << 4)
+#define NVMAP_HANDLE_ZEROED_PAGES    (0x1ul << 5)
+#define NVMAP_HANDLE_PHYS_CONTIG     (0x1ul << 6)
+#define NVMAP_HANDLE_CACHE_SYNC      (0x1ul << 7)
+#define NVMAP_HANDLE_CACHE_SYNC_AT_RESERVE      (0x1ul << 8)
+#define NVMAP_HANDLE_RO	             (0x1ul << 9)
+
+#ifdef CONFIG_NVMAP_PAGE_POOLS
+ulong nvmap_page_pool_get_unused_pages(void);
+#else
+static inline ulong nvmap_page_pool_get_unused_pages(void)
+{
+	return 0;
+}
+#endif
+
+ulong nvmap_iovmm_get_used_pages(void);
+int nvmap_register_vidmem_carveout(struct device *dma_dev,
+		phys_addr_t base, size_t size);
+
+/*
+ * A heap can be mapped to memory other than DRAM.
+ * The HW, controls the memory, can be power gated/ungated
+ * based upon the clients using the memory.
+ * if no client/alloc happens from the memory, the HW needs
+ * to be power gated. Similarly it should power ungated if
+ * alloc happens from the memory.
+ * int (*busy)(void) - trigger runtime power ungate
+ * int (*idle)(void) - trigger runtime power gate
+ */
+struct nvmap_pm_ops {
+	int (*busy)(void);
+	int (*idle)(void);
+};
+
+struct nvmap_platform_carveout {
+	const char *name;
+	unsigned int usage_mask;
+	phys_addr_t base;
+	size_t size;
+	struct device *cma_dev;
+	bool resize;
+	struct device *dma_dev;
+	struct device dev;
+	struct dma_declare_info *dma_info;
+	bool is_ivm;
+	int peer;
+	int vmid;
+	int can_alloc;
+	bool enable_static_dma_map;
+	bool disable_dynamic_dma_map;
+	bool no_cpu_access; /* carveout can't be accessed from cpu at all */
+	bool init_done;	/* FIXME: remove once all caveouts use reserved-memory */
+	struct nvmap_pm_ops pm_ops;
+};
+
+struct nvmap_platform_data {
+	const struct nvmap_platform_carveout *carveouts;
+	unsigned int nr_carveouts;
+};
+
+#endif /* _LINUX_NVMAP_H */
diff --git a/include/linux/platform/tegra/tegra_fd.h b/include/linux/platform/tegra/tegra_fd.h
new file mode 100644
index 000000000000..c30daa03c928
--- /dev/null
+++ b/include/linux/platform/tegra/tegra_fd.h
@@ -0,0 +1,23 @@
+/*
+ * Copyright (C) 2017-2018, NVIDIA Corporation.  All rights reserved.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#include <linux/fdtable.h>
+
+#ifndef __TEGRA_FD_H
+#define __TEGRA_FD_H
+
+int tegra_alloc_fd(struct files_struct *files, unsigned int start,
+		   unsigned int flags);
+
+#endif
diff --git a/include/linux/tegra-mce.h b/include/linux/tegra-mce.h
new file mode 100644
index 000000000000..047ea68f73cb
--- /dev/null
+++ b/include/linux/tegra-mce.h
@@ -0,0 +1,128 @@
+/*
+ * Copyright (c) 2014-2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#ifndef _LINUX_TEGRA_MCE_H
+#define _LINUX_TEGRA_MCE_H
+
+/* NOTE:
+ * For correct version validation, below two defines need to be
+ * updated whenever there is a new ARI implementation.
+ */
+#define CUR_ARI_VER_MAJOR	1
+#define CUR_ARI_VER_MINOR	2
+
+enum {
+	TEGRA_MCE_XOVER_C1_C6, /* Only valid for Denver */
+	TEGRA_MCE_XOVER_CC1_CC6,
+	TEGRA_MCE_XOVER_CC1_CC7,
+
+	TEGRA_MCE_XOVER_MAX = TEGRA_MCE_XOVER_CC1_CC7
+};
+
+enum {
+	TEGRA_MCE_CSTATS_CLEAR,
+
+	TEGRA_MCE_CSTATS_ENTRIES_SC7,
+	TEGRA_MCE_CSTATS_ENTRIES_SC4,
+	TEGRA_MCE_CSTATS_ENTRIES_SC3,
+	TEGRA_MCE_CSTATS_ENTRIES_SC2,
+	TEGRA_MCE_CSTATS_ENTRIES_CCP3,
+	TEGRA_MCE_CSTATS_ENTRIES_A57_CC6,
+	TEGRA_MCE_CSTATS_ENTRIES_A57_CC7,
+	TEGRA_MCE_CSTATS_ENTRIES_D15_CC6,
+	TEGRA_MCE_CSTATS_ENTRIES_D15_CC7,
+	TEGRA_MCE_CSTATS_ENTRIES_D15_CORE0_C6,
+	TEGRA_MCE_CSTATS_ENTRIES_D15_CORE1_C6,
+	/* RESV: 12-13 */
+	TEGRA_MCE_CSTATS_ENTRIES_D15_CORE0_C7 = 14,
+	TEGRA_MCE_CSTATS_ENTRIES_D15_CORE1_C7,
+	/* RESV: 16-17 */
+	TEGRA_MCE_CSTATS_ENTRIES_A57_CORE0_C7 = 18,
+	TEGRA_MCE_CSTATS_ENTRIES_A57_CORE1_C7,
+	TEGRA_MCE_CSTATS_ENTRIES_A57_CORE2_C7,
+	TEGRA_MCE_CSTATS_ENTRIES_A57_CORE3_C7,
+	TEGRA_MCE_CSTATS_LAST_ENTRY_D15_CORE0,
+	TEGRA_MCE_CSTATS_LAST_ENTRY_D15_CORE1,
+	/* RESV: 24-25 */
+	TEGRA_MCE_CSTATS_LAST_ENTRY_A57_CORE0,
+	TEGRA_MCE_CSTATS_LAST_ENTRY_A57_CORE1,
+	TEGRA_MCE_CSTATS_LAST_ENTRY_A57_CORE2,
+	TEGRA_MCE_CSTATS_LAST_ENTRY_A57_CORE3,
+
+	TEGRA_MCE_CSTATS_MAX = TEGRA_MCE_CSTATS_LAST_ENTRY_A57_CORE3,
+};
+
+enum {
+	TEGRA_MCE_ENUM_D15_CORE0,
+	TEGRA_MCE_D15_CORE1,
+	/* RESV: 2-3 */
+	TEGRA_MCE_ENUM_A57_0 = 4,
+	TEGRA_MCE_ENUM_A57_1,
+	TEGRA_MCE_ENUM_A57_2,
+	TEGRA_MCE_ENUM_A57_3,
+
+	TEGRA_MCE_ENUM_MAX = TEGRA_MCE_ENUM_A57_3,
+};
+
+enum {
+	TEGRA_MCE_FEATURE_CCP3,
+};
+
+/* MCA support */
+typedef union {
+	struct {
+		u8 cmd;
+		u8 subidx;
+		u8 idx;
+		u8 inst;
+	};
+	struct {
+		u32 low;
+		u32 high;
+	};
+	u64 data;
+} mca_cmd_t;
+
+/* NOTE: These functions will return -ENOTSUPP if no implementation */
+int tegra_mce_enter_cstate(u32 state, u32 wake_time);
+int tegra_mce_update_cstate_info(u32 cluster, u32 ccplex, u32 system,
+				 u8 force, u32 wake_mask, bool valid);
+int tegra_mce_update_crossover_time(u32 type, u32 time);
+int tegra_mce_read_cstate_stats(u32 state, u64 *stats);
+int tegra_mce_write_cstate_stats(u32 state, u32 stats);
+int tegra_mce_is_sc7_allowed(u32 state, u32 wake, u32 *allowed);
+int tegra_mce_online_core(int cpu);
+int tegra_mce_cc3_ctrl(u32 ndiv, u32 vindex, u8 enable);
+int tegra_mce_echo_data(u32 data, int *matched);
+int tegra_mce_read_versions(u32 *major, u32 *minor);
+int tegra_mce_enum_features(u64 *features);
+int tegra_mce_read_uncore_mca(mca_cmd_t cmd, u64 *data, u32 *error);
+int tegra_mce_write_uncore_mca(mca_cmd_t cmd, u64 data, u32 *error);
+int tegra_mce_read_uncore_perfmon(u32 req, u32 *data);
+int tegra_mce_write_uncore_perfmon(u32 req, u32 data);
+int tegra_mce_enable_latic(void);
+int tegra_mce_write_dda_ctrl(u32 index, u64 value);
+int tegra_mce_read_dda_ctrl(u32 index, u64 *value);
+
+/* Tegra cache functions */
+int tegra_flush_cache_all(void);
+int tegra_flush_dcache_all(void *__maybe_unused unused);
+int tegra_clean_dcache_all(void *__maybe_unused unused);
+/* L3 cache ways read/write functions */
+int tegra_mce_read_l3_cache_ways(u64 *value);
+int tegra_mce_write_l3_cache_ways(u64 data, u64 *value);
+
+#endif /* _LINUX_TEGRA_MCE_H */
diff --git a/include/trace/events/nvmap.h b/include/trace/events/nvmap.h
new file mode 100644
index 000000000000..b9e913deda6f
--- /dev/null
+++ b/include/trace/events/nvmap.h
@@ -0,0 +1,798 @@
+/*
+ * include/trace/events/nvmap.h
+ *
+ * NvMap event logging to ftrace.
+ *
+ * Copyright (c) 2012-2017, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM nvmap
+
+#if !defined(_TRACE_NVMAP_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _TRACE_NVMAP_H
+
+#include <linux/nvmap.h>
+#include <linux/dma-buf.h>
+#include <linux/types.h>
+#include <linux/tracepoint.h>
+
+struct nvmap_handle;
+struct nvmap_handle_ref;
+struct nvmap_client;
+
+DECLARE_EVENT_CLASS(nvmap,
+	TP_PROTO(struct nvmap_client *client, const char *name),
+	TP_ARGS(client, name),
+	TP_STRUCT__entry(
+		__field(struct nvmap_client *, client)
+		__string(sname, name)
+	),
+	TP_fast_assign(
+		__entry->client = client;
+		__assign_str(sname, name)
+	),
+	TP_printk("client=%p, name=%s",
+		__entry->client, __get_str(sname))
+);
+
+DEFINE_EVENT(nvmap, nvmap_open,
+	TP_PROTO(struct nvmap_client *client, const char *name),
+	TP_ARGS(client, name)
+);
+
+DEFINE_EVENT(nvmap, nvmap_release,
+	TP_PROTO(struct nvmap_client *client, const char *name),
+	TP_ARGS(client, name)
+);
+
+TRACE_EVENT(nvmap_create_handle,
+	TP_PROTO(struct nvmap_client *client,
+		 const char *name,
+		 struct nvmap_handle *h,
+		 u32 size,
+		 struct nvmap_handle_ref *ref
+	),
+
+	TP_ARGS(client, name, h, size, ref),
+
+	TP_STRUCT__entry(
+		__field(struct nvmap_client *, client)
+		__string(sname, name)
+		__field(struct nvmap_handle *, h)
+		__field(u32, size)
+		__field(struct nvmap_handle_ref *, ref)
+	),
+
+	TP_fast_assign(
+		__entry->client = client;
+		__assign_str(sname, name)
+		__entry->h = h;
+		__entry->size = size;
+		__entry->ref = ref;
+	),
+
+	TP_printk("client=%p, name=%s, handle=%p, size=%d, ref=%p",
+		__entry->client, __get_str(sname),
+		__entry->h, __entry->size, __entry->ref)
+);
+
+TRACE_EVENT(nvmap_alloc_handle,
+	TP_PROTO(struct nvmap_client *client,
+		 struct nvmap_handle *handle,
+		 size_t size,
+		 u32 heap_mask,
+		 u32 align,
+		 u32 flags,
+		 u64 total,
+		 u64 alloc
+	),
+
+	TP_ARGS(client, handle, size, heap_mask, align, flags, total, alloc),
+
+	TP_STRUCT__entry(
+		__field(struct nvmap_client *, client)
+		__field(struct nvmap_handle *, handle)
+		__field(size_t, size)
+		__field(u32, heap_mask)
+		__field(u32, align)
+		__field(u32, flags)
+		__field(u64, total)
+		__field(u64, alloc)
+	),
+
+	TP_fast_assign(
+		__entry->client = client;
+		__entry->handle = handle;
+		__entry->size = size;
+		__entry->heap_mask = heap_mask;
+		__entry->align = align;
+		__entry->flags = flags;
+		__entry->total = total;
+		__entry->alloc = alloc;
+	),
+
+	TP_printk("client=%p, id=0x%p, size=%zu, heap_mask=0x%x, align=%d, flags=0x%x, total=%llu, alloc=%llu",
+		__entry->client, __entry->handle, __entry->size,
+		__entry->heap_mask, __entry->align, __entry->flags,
+		(unsigned long long)__entry->total,
+		(unsigned long long)__entry->alloc)
+);
+
+
+DECLARE_EVENT_CLASS(nvmap_handle_summary,
+	TP_PROTO(struct nvmap_client *client, pid_t pid, u32 dupes,
+		 struct nvmap_handle *handle, u32 share, u64 base,
+		 size_t size, u32 flags, u32 tag, const char *tag_name),
+	TP_ARGS(client, pid, dupes, handle, share, base, size, flags, tag, tag_name),
+
+	TP_STRUCT__entry(
+		__field(struct nvmap_client *, client)
+		__field(pid_t, pid)
+		__field(u32, dupes)
+		__field(struct nvmap_handle *, handle)
+		__field(u32, share)
+		__field(u64, base)
+		__field(size_t, size)
+		__field(u32, flags)
+		__field(u32, tag)
+		__string(tag_name, tag_name)
+	),
+
+	TP_fast_assign(
+		__entry->client = client;
+		__entry->pid = pid;
+		__entry->dupes = dupes;
+		__entry->handle = handle;
+		__entry->share = share;
+		__entry->base = base;
+		__entry->size = size;
+		__entry->flags = flags;
+		__entry->tag = tag;
+		__assign_str(tag_name, tag_name)
+	),
+
+	TP_printk("client=0x%p pid=%d dupes=%u handle=0x%p share=%u base=%llx size=%zu flags=0x%x tag=0x%x %s",
+		__entry->client,
+		__entry->pid,
+		__entry->dupes,
+		__entry->handle,
+		__entry->share,
+		__entry->base,
+		__entry->size,
+		__entry->flags,
+		__entry->tag,
+		__get_str(tag_name)
+	)
+);
+
+DEFINE_EVENT(nvmap_handle_summary,
+	nvmap_alloc_handle_done,
+	TP_PROTO(struct nvmap_client *client, pid_t pid, u32 dupes,
+		 struct nvmap_handle *handle, u32 share, u64 base,
+		 size_t size, u32 flags, u32 tag, const char *tag_name),
+	TP_ARGS(client, pid, dupes, handle, share, base, size, flags, tag, tag_name)
+);
+
+
+DEFINE_EVENT(nvmap_handle_summary,
+	nvmap_duplicate_handle,
+	TP_PROTO(struct nvmap_client *client, pid_t pid, u32 dupes,
+		 struct nvmap_handle *handle, u32 share, u64 base,
+		 size_t size, u32 flags, u32 tag, const char *tag_name),
+	TP_ARGS(client, pid, dupes, handle, share, base, size, flags, tag, tag_name)
+);
+
+DEFINE_EVENT(nvmap_handle_summary,
+	nvmap_free_handle,
+	TP_PROTO(struct nvmap_client *client, pid_t pid, u32 dupes,
+		 struct nvmap_handle *handle, u32 share, u64 base,
+		 size_t size, u32 flags, u32 tag, const char *tag_name),
+	TP_ARGS(client, pid, dupes, handle, share, base, size, flags, tag, tag_name)
+);
+
+DEFINE_EVENT(nvmap_handle_summary,
+	nvmap_destroy_handle,
+	TP_PROTO(struct nvmap_client *client, pid_t pid, u32 dupes,
+		 struct nvmap_handle *handle, u32 share, u64 base,
+		 size_t size, u32 flags, u32 tag, const char *tag_name),
+	TP_ARGS(client, pid, dupes, handle, share, base, size, flags, tag, tag_name)
+);
+
+TRACE_EVENT(nvmap_cache_maint,
+	TP_PROTO(struct nvmap_client *client,
+		 struct nvmap_handle *h,
+		 ulong start,
+		 ulong end,
+		 u32 op,
+		 size_t size
+	),
+
+	TP_ARGS(client, h, start, end, op, size),
+
+	TP_STRUCT__entry(
+		__field(struct nvmap_client *, client)
+		__field(struct nvmap_handle *, h)
+		__field(ulong, start)
+		__field(ulong, end)
+		__field(u32, op)
+		__field(size_t, size)
+	),
+
+	TP_fast_assign(
+		__entry->client = client;
+		__entry->h = h;
+		__entry->start = start;
+		__entry->end = end;
+		__entry->op = op;
+		__entry->size = size;
+	),
+
+	TP_printk("client=%p, h=%p, start=0x%lx, end=0x%lx, op=%d, size=%zu",
+		__entry->client, __entry->h, __entry->start,
+		__entry->end, __entry->op, __entry->size)
+);
+
+TRACE_EVENT(nvmap_cache_flush,
+	TP_PROTO(size_t size,
+		 u64 alloc_rq,
+		 u64 total_rq,
+		 u64 total_done
+	),
+
+	TP_ARGS(size, alloc_rq, total_rq, total_done),
+
+	TP_STRUCT__entry(
+		__field(size_t, size)
+		__field(u64, alloc_rq)
+		__field(u64, total_rq)
+		__field(u64, total_done)
+	),
+
+	TP_fast_assign(
+		__entry->size = size;
+		__entry->alloc_rq = alloc_rq;
+		__entry->total_rq = total_rq;
+		__entry->total_done = total_done;
+	),
+
+	TP_printk("size=%zu, alloc_rq=%llu, total_rq=%llu, total_done=%llu",
+		__entry->size,
+		(unsigned long long)__entry->alloc_rq,
+		(unsigned long long)__entry->total_rq,
+		(unsigned long long)__entry->total_done)
+);
+
+TRACE_EVENT(nvmap_map_into_caller_ptr,
+	TP_PROTO(struct nvmap_client *client,
+		 struct nvmap_handle *h,
+		 u32 offset,
+		 u32 length,
+		 u32 flags
+	),
+
+	TP_ARGS(client, h, offset, length, flags),
+
+	TP_STRUCT__entry(
+		__field(struct nvmap_client *, client)
+		__field(struct nvmap_handle *, h)
+		__field(u32, offset)
+		__field(u32, length)
+		__field(u32, flags)
+	),
+
+	TP_fast_assign(
+		__entry->client = client;
+		__entry->h = h;
+		__entry->offset = offset;
+		__entry->length = length;
+		__entry->flags = flags;
+	),
+
+	TP_printk("client=%p, h=%p, offset=%d, length=%d, flags=0x%x",
+		__entry->client, __entry->h, __entry->offset,
+		__entry->length, __entry->flags)
+);
+
+TRACE_EVENT(nvmap_ioctl_rw_handle,
+	TP_PROTO(struct nvmap_client *client,
+		 struct nvmap_handle *h,
+		 u32 is_read,
+		 u32 offset,
+		 unsigned long addr,
+		 u32 mem_stride,
+		 u32 user_stride,
+		 u32 elem_size,
+		 u32 count
+	),
+
+	TP_ARGS(client, h, is_read, offset, addr, mem_stride,
+		user_stride, elem_size, count),
+
+	TP_STRUCT__entry(
+		__field(struct nvmap_client *, client)
+		__field(struct nvmap_handle *, h)
+		__field(u32, is_read)
+		__field(u32, offset)
+		__field(unsigned long, addr)
+		__field(u32, mem_stride)
+		__field(u32, user_stride)
+		__field(u32, elem_size)
+		__field(u32, count)
+	),
+
+	TP_fast_assign(
+		__entry->client = client;
+		__entry->h = h;
+		__entry->is_read = is_read;
+		__entry->offset = offset;
+		__entry->addr = addr;
+		__entry->mem_stride = mem_stride;
+		__entry->user_stride = user_stride;
+		__entry->elem_size = elem_size;
+		__entry->count = count;
+	),
+
+	TP_printk("client=%p, h=%p, is_read=%d, offset=%d, addr=0x%lx,"
+		"mem_stride=%d, user_stride=%d, elem_size=%d, count=%d",
+		__entry->client, __entry->h, __entry->is_read, __entry->offset,
+		__entry->addr, __entry->mem_stride, __entry->user_stride,
+		__entry->elem_size, __entry->count)
+);
+
+TRACE_EVENT(nvmap_ioctl_pinop,
+	TP_PROTO(struct nvmap_client *client,
+		 u32 is_pin,
+		 u32 count,
+		 struct nvmap_handle **ids
+	),
+
+	TP_ARGS(client, is_pin, count, ids),
+
+	TP_STRUCT__entry(
+		__field(struct nvmap_client *, client)
+		__field(u32, is_pin)
+		__field(u32, count)
+		__field(struct nvmap_handle **, ids)
+		__dynamic_array(struct nvmap_handle *, ids, count)
+	),
+
+	TP_fast_assign(
+		__entry->client = client;
+		__entry->is_pin = is_pin;
+		__entry->count = count;
+		__entry->ids = ids;
+		memcpy(__get_dynamic_array(ids), ids,
+		    sizeof(struct nvmap_handle *) * count);
+	),
+
+	TP_printk("client=%p, is_pin=%d, count=%d, ids=[%s]",
+		__entry->client, __entry->is_pin, __entry->count,
+		__print_hex(__get_dynamic_array(ids), __entry->ids ?
+			    sizeof(struct nvmap_handle *) * __entry->count : 0))
+);
+
+DECLARE_EVENT_CLASS(handle_get_put,
+	TP_PROTO(struct nvmap_handle *handle, u32 ref_count),
+
+	TP_ARGS(handle, ref_count),
+
+	TP_STRUCT__entry(
+		__field(struct nvmap_handle *, handle)
+		__field(u32, ref_count)
+	),
+
+	TP_fast_assign(
+		__entry->handle = handle;
+		__entry->ref_count = ref_count;
+	),
+
+	TP_printk("ref=%u handle=%p",
+		__entry->ref_count,
+		__entry->handle
+	)
+);
+
+DEFINE_EVENT(handle_get_put, nvmap_handle_get,
+	TP_PROTO(struct nvmap_handle *handle, u32 ref_count),
+	TP_ARGS(handle, ref_count)
+);
+
+DEFINE_EVENT(handle_get_put, nvmap_handle_put,
+	TP_PROTO(struct nvmap_handle *handle, u32 ref_count),
+	TP_ARGS(handle, ref_count)
+);
+
+DECLARE_EVENT_CLASS(pin_unpin,
+	TP_PROTO(struct nvmap_client *client,
+		 const char *name,
+		 struct nvmap_handle *h,
+		 u32 pin_count
+	),
+
+	TP_ARGS(client, name, h, pin_count),
+
+	TP_STRUCT__entry(
+		__field(struct nvmap_client *, client)
+		__string(sname, name)
+		__field(struct nvmap_handle *, h)
+		__field(u32, pin_count)
+	),
+
+	TP_fast_assign(
+		__entry->client = client;
+		__assign_str(sname, name)
+		__entry->h = h;
+		__entry->pin_count = pin_count;
+	),
+
+	TP_printk("client=%p, name=%s, h=%p, pin_count=%d",
+		__entry->client, __get_str(sname),
+		__entry->h, __entry->pin_count)
+);
+
+DEFINE_EVENT(pin_unpin, nvmap_pin,
+	TP_PROTO(struct nvmap_client *client,
+		 const char *name,
+		 struct nvmap_handle *h,
+		 u32 pin_count
+	),
+	TP_ARGS(client, name, h, pin_count)
+);
+
+DEFINE_EVENT(pin_unpin, nvmap_unpin,
+	TP_PROTO(struct nvmap_client *client,
+		 const char *name,
+		 struct nvmap_handle *h,
+		 u32 pin_count
+	),
+	TP_ARGS(client, name, h, pin_count)
+);
+
+DEFINE_EVENT(pin_unpin, handle_unpin_error,
+	TP_PROTO(struct nvmap_client *client,
+		 const char *name,
+		 struct nvmap_handle *h,
+		 u32 pin_count
+	),
+	TP_ARGS(client, name, h, pin_count)
+);
+
+/*
+ * Nvmap dmabuf events
+ */
+DECLARE_EVENT_CLASS(nvmap_dmabuf_2,
+	TP_PROTO(struct dma_buf *dbuf,
+		 struct device *dev
+	),
+
+	TP_ARGS(dbuf, dev),
+
+	TP_STRUCT__entry(
+		__field(struct dma_buf *, dbuf)
+		__string(name, dev_name(dev))
+	),
+
+	TP_fast_assign(
+		__entry->dbuf = dbuf;
+		__assign_str(name, dev_name(dev));
+	),
+
+	TP_printk("dmabuf=%p, device=%s",
+		__entry->dbuf, __get_str(name)
+	)
+);
+
+DECLARE_EVENT_CLASS(nvmap_dmabuf_1,
+	TP_PROTO(struct dma_buf *dbuf),
+
+	TP_ARGS(dbuf),
+
+	TP_STRUCT__entry(
+		__field(struct dma_buf *, dbuf)
+	),
+
+	TP_fast_assign(
+		__entry->dbuf = dbuf;
+	),
+
+	TP_printk("dmabuf=%p",
+		__entry->dbuf
+	)
+);
+
+DEFINE_EVENT(nvmap_dmabuf_2, nvmap_dmabuf_attach,
+	TP_PROTO(struct dma_buf *dbuf,
+		 struct device *dev
+	),
+	TP_ARGS(dbuf, dev)
+);
+
+DEFINE_EVENT(nvmap_dmabuf_2, nvmap_dmabuf_detach,
+	TP_PROTO(struct dma_buf *dbuf,
+		 struct device *dev
+	),
+	TP_ARGS(dbuf, dev)
+);
+
+DEFINE_EVENT(nvmap_dmabuf_2, nvmap_dmabuf_map_dma_buf,
+	TP_PROTO(struct dma_buf *dbuf,
+		 struct device *dev
+	),
+	TP_ARGS(dbuf, dev)
+);
+
+DEFINE_EVENT(nvmap_dmabuf_2, nvmap_dmabuf_unmap_dma_buf,
+	TP_PROTO(struct dma_buf *dbuf,
+		 struct device *dev
+	),
+	TP_ARGS(dbuf, dev)
+);
+
+DEFINE_EVENT(nvmap_dmabuf_1, nvmap_dmabuf_mmap,
+	TP_PROTO(struct dma_buf *dbuf),
+	TP_ARGS(dbuf)
+);
+
+DEFINE_EVENT(nvmap_dmabuf_1, nvmap_dmabuf_vmap,
+	TP_PROTO(struct dma_buf *dbuf),
+	TP_ARGS(dbuf)
+);
+
+DEFINE_EVENT(nvmap_dmabuf_1, nvmap_dmabuf_vunmap,
+	TP_PROTO(struct dma_buf *dbuf),
+	TP_ARGS(dbuf)
+);
+
+DEFINE_EVENT(nvmap_dmabuf_1, nvmap_dmabuf_kmap,
+	TP_PROTO(struct dma_buf *dbuf),
+	TP_ARGS(dbuf)
+);
+
+DEFINE_EVENT(nvmap_dmabuf_1, nvmap_dmabuf_kunmap,
+	TP_PROTO(struct dma_buf *dbuf),
+	TP_ARGS(dbuf)
+);
+
+DECLARE_EVENT_CLASS(nvmap_dmabuf_cpu_access,
+	TP_PROTO(struct dma_buf *dbuf,
+		 size_t start,
+		 size_t len),
+
+	TP_ARGS(dbuf, start, len),
+
+	TP_STRUCT__entry(
+		__field(struct dma_buf *, dbuf)
+		__field(size_t, start)
+		__field(size_t, len)
+	),
+
+	TP_fast_assign(
+		__entry->dbuf = dbuf;
+		__entry->start = start;
+		__entry->len = len;
+	),
+
+	TP_printk("dmabuf=%p, start=%zd len=%zd",
+		  __entry->dbuf, __entry->start, __entry->len
+	)
+);
+
+DEFINE_EVENT(nvmap_dmabuf_cpu_access, nvmap_dmabuf_begin_cpu_access,
+	TP_PROTO(struct dma_buf *dbuf,
+		 size_t start,
+		 size_t len),
+	TP_ARGS(dbuf, start, len)
+);
+
+DEFINE_EVENT(nvmap_dmabuf_cpu_access, nvmap_dmabuf_end_cpu_access,
+	TP_PROTO(struct dma_buf *dbuf,
+		 size_t start,
+		 size_t len),
+	TP_ARGS(dbuf, start, len)
+);
+
+DECLARE_EVENT_CLASS(nvmap_dmabuf_make_release,
+	TP_PROTO(const char *cli,
+		 struct nvmap_handle *h,
+		 struct dma_buf *dbuf
+	),
+
+	TP_ARGS(cli, h, dbuf),
+
+	TP_STRUCT__entry(
+		__field(const char *, cli)
+		__field(struct nvmap_handle *, h)
+		__field(struct dma_buf *, dbuf)
+	),
+
+	TP_fast_assign(
+		__entry->cli = cli;
+		__entry->h = h;
+		__entry->dbuf = dbuf;
+	),
+
+	TP_printk("cli=%s handle=%p dmabuf=%p",
+		  __entry->cli, __entry->h, __entry->dbuf
+	)
+);
+
+DEFINE_EVENT(nvmap_dmabuf_make_release, nvmap_make_dmabuf,
+	TP_PROTO(const char *cli,
+		 struct nvmap_handle *h,
+		 struct dma_buf *dbuf
+	),
+	TP_ARGS(cli, h, dbuf)
+);
+
+DEFINE_EVENT(nvmap_dmabuf_make_release, nvmap_dmabuf_release,
+	TP_PROTO(const char *cli,
+		 struct nvmap_handle *h,
+		 struct dma_buf *dbuf
+	),
+	TP_ARGS(cli, h, dbuf)
+);
+
+TRACE_EVENT(pp_clean_cache,
+	TP_PROTO(u32 dirty_pages,
+		 size_t cache_maint_th,
+		 int cache_maint_by_set_ways
+	),
+
+	TP_ARGS(dirty_pages, cache_maint_th, cache_maint_by_set_ways),
+
+	TP_STRUCT__entry(
+		__field(u32, dirty_pages)
+		__field(size_t, cache_maint_th)
+		__field(int, cache_maint_by_set_ways)
+	),
+
+	TP_fast_assign(
+		__entry->dirty_pages = dirty_pages;
+		__entry->cache_maint_th = cache_maint_th >> PAGE_SHIFT;
+		__entry->cache_maint_by_set_ways = cache_maint_by_set_ways;
+	),
+
+	TP_printk("dirty_pages=%u, cache_maint_th=%zu, cache_maint_by_set_ways=%d",
+		__entry->dirty_pages, __entry->cache_maint_th,
+		__entry->cache_maint_by_set_ways)
+);
+
+DECLARE_EVENT_CLASS(nvmap_get_list_page,
+	TP_PROTO(u32 count),
+
+	TP_ARGS(count),
+
+	TP_STRUCT__entry(
+		__field(u32, count)
+	),
+
+	TP_fast_assign(
+		__entry->count = count;
+	),
+
+	TP_printk("pages left in list=%u", __entry->count)
+);
+
+DEFINE_EVENT(nvmap_get_list_page, get_zero_list_page,
+	TP_PROTO(u32 count),
+	TP_ARGS(count)
+);
+
+DEFINE_EVENT(nvmap_get_list_page, get_page_list_page,
+	TP_PROTO(u32 count),
+	TP_ARGS(count)
+);
+
+TRACE_EVENT(nvmap_pp_zero_pages,
+	TP_PROTO(u32 count),
+
+	TP_ARGS(count),
+
+	TP_STRUCT__entry(
+		__field(u32, count)
+	),
+
+	TP_fast_assign(
+		__entry->count = count;
+	),
+
+	TP_printk("no. of pages zeroed=%u", __entry->count)
+);
+
+TRACE_EVENT(nvmap_pp_do_background_zero_pages,
+	TP_PROTO(u32 inserted, u32 zeroed),
+
+	TP_ARGS(inserted, zeroed),
+
+	TP_STRUCT__entry(
+		__field(u32, inserted)
+		__field(u32, zeroed)
+	),
+
+	TP_fast_assign(
+		__entry->inserted = inserted;
+		__entry->zeroed = zeroed;
+	),
+
+	TP_printk("failed to insert %u no. of zeroed pages to page_list",
+		__entry->zeroed - __entry->inserted)
+);
+
+TRACE_EVENT(nvmap_pp_alloc_locked,
+	TP_PROTO(int force_alloc),
+
+	TP_ARGS(force_alloc),
+
+	TP_STRUCT__entry(
+		__field(int, force_alloc)
+	),
+
+	TP_fast_assign(
+		__entry->force_alloc = force_alloc;
+	),
+
+	TP_printk("allocated one page with force_alloc:%d", __entry->force_alloc)
+);
+
+TRACE_EVENT(nvmap_pp_alloc_lots,
+	TP_PROTO(u32 alloced, u32 requested),
+
+	TP_ARGS(alloced, requested),
+
+	TP_STRUCT__entry(
+		__field(u32, alloced)
+		__field(u32, requested)
+	),
+
+	TP_fast_assign(
+		__entry->alloced = alloced;
+		__entry->requested = requested;
+	),
+
+	TP_printk("requested:%u alloced:%u\n",
+		__entry->requested, __entry->alloced)
+);
+
+TRACE_EVENT(nvmap_pp_fill_zero_lots,
+	TP_PROTO(u32 save_to_zero,
+		 u32 to_zero,
+		 u32 ret,
+		 u32 nr
+	),
+
+	TP_ARGS(save_to_zero, to_zero, ret, nr),
+
+	TP_STRUCT__entry(
+		__field(u32, save_to_zero)
+		__field(u32, to_zero)
+		__field(u32, ret)
+		__field(u32, nr)
+	),
+
+	TP_fast_assign(
+		__entry->save_to_zero = save_to_zero;
+		__entry->to_zero = to_zero;
+		__entry->ret = ret;
+		__entry->nr = nr;
+	),
+
+	TP_printk("inserted %u pages to zero list, freed %u pages, did not process %u pages",
+		__entry->to_zero - __entry->save_to_zero,
+		__entry->ret - (__entry->to_zero - __entry->save_to_zero),
+		__entry->nr - __entry->ret)
+);
+
+#endif /* _TRACE_NVMAP_H */
+
+/* This part must be outside protection */
+#include <trace/define_trace.h>
diff --git a/include/uapi/linux/nvmap.h b/include/uapi/linux/nvmap.h
new file mode 100644
index 000000000000..2d59c42f0e13
--- /dev/null
+++ b/include/uapi/linux/nvmap.h
@@ -0,0 +1,418 @@
+/*
+ * include/uapi/linux/nvmap.h
+ *
+ * structure declarations for nvmem and nvmap user-space ioctls
+ *
+ * Copyright (c) 2009-2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#include <linux/ioctl.h>
+#include <linux/types.h>
+
+#ifndef __UAPI_LINUX_NVMAP_H
+#define __UAPI_LINUX_NVMAP_H
+
+/*
+ * DOC: NvMap Userspace API
+ *
+ * create a client by opening /dev/nvmap
+ * most operations handled via following ioctls
+ *
+ */
+enum {
+	NVMAP_HANDLE_PARAM_SIZE = 1,
+	NVMAP_HANDLE_PARAM_ALIGNMENT,
+	NVMAP_HANDLE_PARAM_BASE,
+	NVMAP_HANDLE_PARAM_HEAP,
+	NVMAP_HANDLE_PARAM_KIND,
+	NVMAP_HANDLE_PARAM_COMPR, /* ignored, to be removed */
+};
+
+enum {
+	NVMAP_CACHE_OP_WB = 0,
+	NVMAP_CACHE_OP_INV,
+	NVMAP_CACHE_OP_WB_INV,
+};
+
+enum {
+	NVMAP_PAGES_UNRESERVE = 0,
+	NVMAP_PAGES_RESERVE,
+	NVMAP_INSERT_PAGES_ON_UNRESERVE,
+	NVMAP_PAGES_PROT_AND_CLEAN,
+};
+
+#define NVMAP_ELEM_SIZE_U64 (1 << 31)
+
+struct nvmap_create_handle {
+	union {
+		struct {
+			union {
+				/* size will be overwritten */
+				__u32 size;	/* CreateHandle */
+				__s32 fd;	/* DmaBufFd or FromFd */
+			};
+			__u32 handle;		/* returns nvmap handle */
+		};
+		struct {
+			/* one is input parameter, and other is output parameter
+			 * since its a union please note that input parameter
+			 * will be overwritten once ioctl returns
+			 */
+			union {
+				__u64 ivm_id;	 /* CreateHandle from ivm*/
+				__s32 ivm_handle;/* Get ivm_id from handle */
+			};
+		};
+		struct {
+			union {
+				/* size64 will be overwritten */
+				__u64 size64; /* CreateHandle */
+				__u32 handle64; /* returns nvmap handle */
+			};
+		};
+	};
+};
+
+struct nvmap_create_handle_from_va {
+	__u64 va;		/* FromVA*/
+	__u32 size;		/* non-zero for partial memory VMA. zero for end of VMA */
+	__u32 flags;		/* wb/wc/uc/iwb, tag etc. */
+	union {
+		__u32 handle;		/* returns nvmap handle */
+		__u64 size64;		/* used when size is 0 */
+	};
+};
+
+struct nvmap_gup_test {
+	__u64 va;		/* FromVA*/
+	__u32 handle;		/* returns nvmap handle */
+	__u32 result;		/* result=1 for pass, result=-err for failure */
+};
+
+struct nvmap_alloc_handle {
+	__u32 handle;		/* nvmap handle */
+	__u32 heap_mask;	/* heaps to allocate from */
+	__u32 flags;		/* wb/wc/uc/iwb etc. */
+	__u32 align;		/* min alignment necessary */
+};
+
+struct nvmap_alloc_ivm_handle {
+	__u32 handle;		/* nvmap handle */
+	__u32 heap_mask;	/* heaps to allocate from */
+	__u32 flags;		/* wb/wc/uc/iwb etc. */
+	__u32 align;		/* min alignment necessary */
+	__u32 peer;		/* peer with whom handle must be shared. Used
+				 *  only for NVMAP_HEAP_CARVEOUT_IVM
+				 */
+};
+
+struct nvmap_alloc_kind_handle {
+	__u32 handle;		/* nvmap handle */
+	__u32 heap_mask;
+	__u32 flags;
+	__u32 align;
+	__u8  kind;
+	__u8  comp_tags;
+};
+
+struct nvmap_map_caller {
+	__u32 handle;		/* nvmap handle */
+	__u32 offset;		/* offset into hmem; should be page-aligned */
+	__u32 length;		/* number of bytes to map */
+	__u32 flags;		/* maps as wb/iwb etc. */
+	unsigned long addr;	/* user pointer */
+};
+
+#ifdef CONFIG_COMPAT
+struct nvmap_map_caller_32 {
+	__u32 handle;		/* nvmap handle */
+	__u32 offset;		/* offset into hmem; should be page-aligned */
+	__u32 length;		/* number of bytes to map */
+	__u32 flags;		/* maps as wb/iwb etc. */
+	__u32 addr;		/* user pointer*/
+};
+#endif
+
+struct nvmap_rw_handle {
+	unsigned long addr;	/* user pointer*/
+	__u32 handle;		/* nvmap handle */
+	__u32 offset;		/* offset into hmem */
+	__u32 elem_size;	/* individual atom size */
+	__u32 hmem_stride;	/* delta in bytes between atoms in hmem */
+	__u32 user_stride;	/* delta in bytes between atoms in user */
+	__u32 count;		/* number of atoms to copy */
+};
+
+struct nvmap_rw_handle_64 {
+	unsigned long addr;	/* user pointer*/
+	__u32 handle;		/* nvmap handle */
+	__u64 offset;		/* offset into hmem */
+	__u64 elem_size;	/* individual atom size */
+	__u64 hmem_stride;	/* delta in bytes between atoms in hmem */
+	__u64 user_stride;	/* delta in bytes between atoms in user */
+	__u64 count;		/* number of atoms to copy */
+};
+
+#ifdef CONFIG_COMPAT
+struct nvmap_rw_handle_32 {
+	__u32 addr;		/* user pointer */
+	__u32 handle;		/* nvmap handle */
+	__u32 offset;		/* offset into hmem */
+	__u32 elem_size;	/* individual atom size */
+	__u32 hmem_stride;	/* delta in bytes between atoms in hmem */
+	__u32 user_stride;	/* delta in bytes between atoms in user */
+	__u32 count;		/* number of atoms to copy */
+};
+#endif
+
+struct nvmap_pin_handle {
+	__u32 *handles;		/* array of handles to pin/unpin */
+	unsigned long *addr;	/* array of addresses to return */
+	__u32 count;		/* number of entries in handles */
+};
+
+#ifdef CONFIG_COMPAT
+struct nvmap_pin_handle_32 {
+	__u32 handles;		/* array of handles to pin/unpin */
+	__u32 addr;		/*  array of addresses to return */
+	__u32 count;		/* number of entries in handles */
+};
+#endif
+
+struct nvmap_handle_param {
+	__u32 handle;		/* nvmap handle */
+	__u32 param;		/* size/align/base/heap etc. */
+	unsigned long result;	/* returns requested info*/
+};
+
+#ifdef CONFIG_COMPAT
+struct nvmap_handle_param_32 {
+	__u32 handle;		/* nvmap handle */
+	__u32 param;		/* size/align/base/heap etc. */
+	__u32 result;		/* returns requested info*/
+};
+#endif
+
+struct nvmap_cache_op {
+	unsigned long addr;	/* user pointer*/
+	__u32 handle;		/* nvmap handle */
+	__u32 len;		/* bytes to flush */
+	__s32 op;		/* wb/wb_inv/inv */
+};
+
+struct nvmap_cache_op_64 {
+	unsigned long addr;	/* user pointer*/
+	__u32 handle;		/* nvmap handle */
+	__u64 len;		/* bytes to flush */
+	__s32 op;		/* wb/wb_inv/inv */
+};
+
+#ifdef CONFIG_COMPAT
+struct nvmap_cache_op_32 {
+	__u32 addr;		/* user pointer*/
+	__u32 handle;		/* nvmap handle */
+	__u32 len;		/* bytes to flush */
+	__s32 op;		/* wb/wb_inv/inv */
+};
+#endif
+
+struct nvmap_cache_op_list {
+	__u64 handles;		/* Ptr to u32 type array, holding handles */
+	__u64 offsets;		/* Ptr to u32 type array, holding offsets
+				 * into handle mem */
+	__u64 sizes;		/* Ptr to u32 type array, holindg sizes of memory
+				 * regions within each handle */
+	__u32 nr;		/* Number of handles */
+	__s32 op;		/* wb/wb_inv/inv */
+};
+
+struct nvmap_debugfs_handles_header {
+	__u8 version;
+};
+
+struct nvmap_debugfs_handles_entry {
+	__u64 base;
+	__u64 size;
+	__u32 flags;
+	__u32 share_count;
+	__u64 mapped_size;
+};
+
+struct nvmap_set_tag_label {
+	__u32 tag;
+	__u32 len;		/* in: label length
+				   out: number of characters copied */
+	__u64 addr;		/* in: pointer to label or NULL to remove */
+};
+
+struct nvmap_available_ivm_heaps {
+	/* One is input parameter, and other is output parameter. Since it is a
+	 * union please note that input parameter will be overwritten once the
+	 * ioctl returns.
+	 */
+	union {
+		__u32 requested_heap_type;	/* Input */
+		__u32 heap_mask;		/* Output */
+	};
+};
+
+struct nvmap_available_heaps {
+	__u64 heaps;		/* heaps bitmask */
+};
+
+struct nvmap_heap_size {
+	__u32 heap;
+	__u64 size;
+};
+
+/**
+ * Struct used while querying heap parameters
+ */
+struct nvmap_query_heap_params {
+	__u32 heap_mask;
+	__u32 flags;
+	__u8 contig;
+	__u64 total;
+	__u64 free;
+	__u64 largest_free_block;
+};
+
+struct nvmap_handle_parameters {
+    __u8 contig;
+    __u32 import_id;
+    __u32 handle;
+    __u32 heap_number;
+    __u32 access_flags;
+    __u64 heap;
+    __u64 align;
+    __u64 coherency;
+    __u64 size;
+};
+
+#define NVMAP_IOC_MAGIC 'N'
+
+/* Creates a new memory handle. On input, the argument is the size of the new
+ * handle; on return, the argument is the name of the new handle
+ */
+#define NVMAP_IOC_CREATE  _IOWR(NVMAP_IOC_MAGIC, 0, struct nvmap_create_handle)
+#define NVMAP_IOC_CREATE_64 \
+	_IOWR(NVMAP_IOC_MAGIC, 1, struct nvmap_create_handle)
+#define NVMAP_IOC_FROM_ID _IOWR(NVMAP_IOC_MAGIC, 2, struct nvmap_create_handle)
+
+/* Actually allocates memory for the specified handle */
+#define NVMAP_IOC_ALLOC    _IOW(NVMAP_IOC_MAGIC, 3, struct nvmap_alloc_handle)
+
+/* Frees a memory handle, unpinning any pinned pages and unmapping any mappings
+ */
+#define NVMAP_IOC_FREE       _IO(NVMAP_IOC_MAGIC, 4)
+
+/* Maps the region of the specified handle into a user-provided virtual address
+ * that was previously created via an mmap syscall on this fd */
+#define NVMAP_IOC_MMAP       _IOWR(NVMAP_IOC_MAGIC, 5, struct nvmap_map_caller)
+#ifdef CONFIG_COMPAT
+#define NVMAP_IOC_MMAP_32    _IOWR(NVMAP_IOC_MAGIC, 5, struct nvmap_map_caller_32)
+#endif
+
+/* Reads/writes data (possibly strided) from a user-provided buffer into the
+ * hmem at the specified offset */
+#define NVMAP_IOC_WRITE      _IOW(NVMAP_IOC_MAGIC, 6, struct nvmap_rw_handle)
+#define NVMAP_IOC_READ       _IOW(NVMAP_IOC_MAGIC, 7, struct nvmap_rw_handle)
+#ifdef CONFIG_COMPAT
+#define NVMAP_IOC_WRITE_32   _IOW(NVMAP_IOC_MAGIC, 6, struct nvmap_rw_handle_32)
+#define NVMAP_IOC_READ_32    _IOW(NVMAP_IOC_MAGIC, 7, struct nvmap_rw_handle_32)
+#endif
+#define NVMAP_IOC_WRITE_64 \
+	_IOW(NVMAP_IOC_MAGIC, 6, struct nvmap_rw_handle_64)
+#define NVMAP_IOC_READ_64 \
+	_IOW(NVMAP_IOC_MAGIC, 7, struct nvmap_rw_handle_64)
+
+#define NVMAP_IOC_PARAM _IOWR(NVMAP_IOC_MAGIC, 8, struct nvmap_handle_param)
+#ifdef CONFIG_COMPAT
+#define NVMAP_IOC_PARAM_32 _IOWR(NVMAP_IOC_MAGIC, 8, struct nvmap_handle_param_32)
+#endif
+
+/* Pins a list of memory handles into IO-addressable memory (either IOVMM
+ * space or physical memory, depending on the allocation), and returns the
+ * address. Handles may be pinned recursively. */
+#define NVMAP_IOC_PIN_MULT      _IOWR(NVMAP_IOC_MAGIC, 10, struct nvmap_pin_handle)
+#define NVMAP_IOC_UNPIN_MULT    _IOW(NVMAP_IOC_MAGIC, 11, struct nvmap_pin_handle)
+#ifdef CONFIG_COMPAT
+#define NVMAP_IOC_PIN_MULT_32   _IOWR(NVMAP_IOC_MAGIC, 10, struct nvmap_pin_handle_32)
+#define NVMAP_IOC_UNPIN_MULT_32 _IOW(NVMAP_IOC_MAGIC, 11, struct nvmap_pin_handle_32)
+#endif
+
+#define NVMAP_IOC_CACHE      _IOW(NVMAP_IOC_MAGIC, 12, struct nvmap_cache_op)
+#define NVMAP_IOC_CACHE_64   _IOW(NVMAP_IOC_MAGIC, 12, struct nvmap_cache_op_64)
+#ifdef CONFIG_COMPAT
+#define NVMAP_IOC_CACHE_32  _IOW(NVMAP_IOC_MAGIC, 12, struct nvmap_cache_op_32)
+#endif
+
+/* Returns a global ID usable to allow a remote process to create a handle
+ * reference to the same handle */
+#define NVMAP_IOC_GET_ID  _IOWR(NVMAP_IOC_MAGIC, 13, struct nvmap_create_handle)
+
+/* Returns a dma-buf fd usable to allow a remote process to create a handle
+ * reference to the same handle */
+#define NVMAP_IOC_SHARE  _IOWR(NVMAP_IOC_MAGIC, 14, struct nvmap_create_handle)
+
+/* Returns a file id that allows a remote process to create a handle
+ * reference to the same handle */
+#define NVMAP_IOC_GET_FD  _IOWR(NVMAP_IOC_MAGIC, 15, struct nvmap_create_handle)
+
+/* Create a new memory handle from file id passed */
+#define NVMAP_IOC_FROM_FD _IOWR(NVMAP_IOC_MAGIC, 16, struct nvmap_create_handle)
+
+/* Perform cache maintenance on a list of handles. */
+#define NVMAP_IOC_CACHE_LIST _IOW(NVMAP_IOC_MAGIC, 17,	\
+				  struct nvmap_cache_op_list)
+/* Perform reserve operation on a list of handles. */
+#define NVMAP_IOC_RESERVE _IOW(NVMAP_IOC_MAGIC, 18,	\
+				  struct nvmap_cache_op_list)
+
+#define NVMAP_IOC_FROM_IVC_ID _IOWR(NVMAP_IOC_MAGIC, 19, struct nvmap_create_handle)
+#define NVMAP_IOC_GET_IVC_ID _IOWR(NVMAP_IOC_MAGIC, 20, struct nvmap_create_handle)
+#define NVMAP_IOC_GET_IVM_HEAPS \
+	_IOWR(NVMAP_IOC_MAGIC, 21, struct nvmap_available_ivm_heaps)
+
+/* Create a new memory handle from VA passed */
+#define NVMAP_IOC_FROM_VA _IOWR(NVMAP_IOC_MAGIC, 22, struct nvmap_create_handle_from_va)
+
+#define NVMAP_IOC_GUP_TEST _IOWR(NVMAP_IOC_MAGIC, 23, struct nvmap_gup_test)
+
+/* Define a label for allocation tag */
+#define NVMAP_IOC_SET_TAG_LABEL	_IOW(NVMAP_IOC_MAGIC, 24, struct nvmap_set_tag_label)
+
+#define NVMAP_IOC_GET_AVAILABLE_HEAPS \
+	_IOR(NVMAP_IOC_MAGIC, 25, struct nvmap_available_heaps)
+
+#define NVMAP_IOC_GET_HEAP_SIZE \
+	_IOR(NVMAP_IOC_MAGIC, 26, struct nvmap_heap_size)
+
+#define NVMAP_IOC_PARAMETERS \
+	_IOR(NVMAP_IOC_MAGIC, 27, struct nvmap_handle_parameters)
+/* START of T124 IOCTLS */
+/* Actually allocates memory for the specified handle, with kind */
+#define NVMAP_IOC_ALLOC_KIND _IOW(NVMAP_IOC_MAGIC, 100, struct nvmap_alloc_kind_handle)
+
+/* Actually allocates memory from IVM heaps */
+#define NVMAP_IOC_ALLOC_IVM _IOW(NVMAP_IOC_MAGIC, 101, struct nvmap_alloc_ivm_handle)
+
+/* Allocate seperate memory for VPR */
+#define NVMAP_IOC_VPR_FLOOR_SIZE _IOW(NVMAP_IOC_MAGIC, 102, __u32)
+
+/* Get heap parameters such as total and frre size */
+#define NVMAP_IOC_QUERY_HEAP_PARAMS _IOR(NVMAP_IOC_MAGIC, 105, \
+		struct nvmap_query_heap_params)
+
+#define NVMAP_IOC_MAXNR (_IOC_NR(NVMAP_IOC_QUERY_HEAP_PARAMS))
+
+#endif /* __UAPI_LINUX_NVMAP_H */
-- 
2.34.1

