From 6c91868ad18b1b3ae77448b4f9af720cb0f6841e Mon Sep 17 00:00:00 2001
From: Thomas Epperson <thomas.epperson@snapon.com>
Date: Tue, 17 Sep 2024 13:41:25 -0500
Subject: [PATCH] Nvgpu compiles now.
Upstream-Status: Pending

---
 arch/arm/mach-tegra/iomap.h                   |    9 +
 arch/arm64/boot/dts/nvidia/tegra210.dtsi      |    2 +-
 arch/arm64/boot/dts/uglyoldbob/xjrad.dts      |    9 +
 drivers/clk/clk.c                             |   98 +
 drivers/clk/tegra/clk-dfll.c                  |  153 +-
 drivers/clk/tegra/clk-dfll.h                  |    5 +
 drivers/platform/Kconfig                      |    5 +
 drivers/platform/tegra/mc/emc_bwmgr-t19x.c    |    2 +-
 drivers/platform/tegra/mc/emc_bwmgr.c         |    8 +-
 drivers/platform/tegra/mc/mcerr.c             |    2 +-
 .../platform/tegra/mc/pmqos_bwmgr_client.c    |    4 +-
 drivers/regulator/core.c                      |   31 +
 drivers/soc/tegra/Makefile                    |    3 +
 drivers/soc/tegra/cvb.c                       |  175 ++
 drivers/soc/tegra/pmc.c                       |  109 +-
 drivers/soc/tegra/tegra-dvfs.c                | 2449 +++++++++++++++++
 drivers/soc/tegra/tegra210-dvfs.c             | 2358 ++++++++++++++++
 drivers/thermal/tegra/soctherm.c              |   28 +
 drivers/thermal/tegra/soctherm.h              |    5 +
 drivers/video/Kconfig                         |   15 +
 drivers/video/tegra/nvmap/nvmap_ioctl.c       |    3 +-
 include/linux/clk-provider.h                  |   16 +
 include/linux/clk.h                           |   10 +
 include/linux/platform/tegra/emc_bwmgr.h      |  396 +++
 include/linux/platform/tegra/isomgr.h         |  215 ++
 include/linux/pm_qos.h                        |   78 +
 include/linux/regulator/consumer.h            |    2 +
 include/soc/tegra/cvb.h                       |  127 +
 include/soc/tegra/pmc.h                       |    3 +
 include/soc/tegra/tegra-dfll.h                |   46 +
 include/soc/tegra/tegra-dvfs.h                |  401 +++
 include/trace/events/bwmgr.h                  |   80 +
 kernel/power/qos.c                            |  268 ++
 33 files changed, 7072 insertions(+), 43 deletions(-)
 create mode 100644 drivers/soc/tegra/cvb.c
 create mode 100644 drivers/soc/tegra/tegra-dvfs.c
 create mode 100644 drivers/soc/tegra/tegra210-dvfs.c
 create mode 100644 include/linux/platform/tegra/emc_bwmgr.h
 create mode 100644 include/linux/platform/tegra/isomgr.h
 create mode 100644 include/soc/tegra/cvb.h
 create mode 100644 include/soc/tegra/tegra-dfll.h
 create mode 100644 include/soc/tegra/tegra-dvfs.h
 create mode 100644 include/trace/events/bwmgr.h

diff --git a/arch/arm/mach-tegra/iomap.h b/arch/arm/mach-tegra/iomap.h
index 4cb7e5fee137..9dfab95704fa 100644
--- a/arch/arm/mach-tegra/iomap.h
+++ b/arch/arm/mach-tegra/iomap.h
@@ -79,6 +79,15 @@
 #define TEGRA_EMC1_BASE			0x7001A800
 #define TEGRA_EMC1_SIZE			SZ_2K
 
+#define TEGRA_T210_EMC_BASE		0x7001B000
+#define TEGRA_T210_EMC_SIZE		SZ_4K
+
+#define TEGRA_T210_EMC0_BASE		0x7001E000
+#define TEGRA_T210_EMC0_SIZE		SZ_4K
+
+#define TEGRA_T210_EMC1_BASE		0x7001F000
+#define TEGRA_T210_EMC1_SIZE		SZ_4K
+
 #define TEGRA124_EMC_BASE		0x7001B000
 #define TEGRA124_EMC_SIZE		SZ_2K
 
diff --git a/arch/arm64/boot/dts/nvidia/tegra210.dtsi b/arch/arm64/boot/dts/nvidia/tegra210.dtsi
index 47f8268e46bf..25308142ca2b 100644
--- a/arch/arm64/boot/dts/nvidia/tegra210.dtsi
+++ b/arch/arm64/boot/dts/nvidia/tegra210.dtsi
@@ -85,7 +85,7 @@ pci@2,0 {
 		};
 	};
 
-	host1x@50000000 {
+	host1x: host1x@50000000 {
 		compatible = "nvidia,tegra210-host1x";
 		reg = <0x0 0x50000000 0x0 0x00034000>;
 		interrupts = <GIC_SPI 65 IRQ_TYPE_LEVEL_HIGH>, /* syncpt */
diff --git a/arch/arm64/boot/dts/uglyoldbob/xjrad.dts b/arch/arm64/boot/dts/uglyoldbob/xjrad.dts
index 7f18d4158d7c..658aece8228f 100644
--- a/arch/arm64/boot/dts/uglyoldbob/xjrad.dts
+++ b/arch/arm64/boot/dts/uglyoldbob/xjrad.dts
@@ -32,6 +32,13 @@ memory@80000000 {
 		reg = <0x0 0x80000000 0x1 0x0>;
 	};
 
+	bwmgr {
+		compatible = "nvidia,bwmgr";
+		clocks = <&tegra_car TEGRA210_CLK_EMC>;
+		clock-names = "emc";
+		status = "okay";
+	};
+
 	pcie@1003000 {
 		status = "okay";
 
@@ -110,6 +117,8 @@ i2c@546c0000 {
 
 	gpu@57000000 {
 		vdd-supply = <&vdd_gpu>;
+		nvidia,host1x = <&host1x>;
+		access-vpr-phys;
 		status = "okay";
 	};
 
diff --git a/drivers/clk/clk.c b/drivers/clk/clk.c
index f8776065ad1f..4627712825f3 100644
--- a/drivers/clk/clk.c
+++ b/drivers/clk/clk.c
@@ -92,6 +92,7 @@ struct clk_core {
 #ifdef CONFIG_DEBUG_FS
 	struct dentry		*dentry;
 	struct hlist_node	debug_node;
+	struct hlist_head	debug_extra_nodes;
 #endif
 	struct kref		ref;
 };
@@ -110,6 +111,16 @@ struct clk {
 	struct hlist_node clks_node;
 };
 
+#ifdef CONFIG_DEBUG_FS
+struct debug_extra_node {
+	struct hlist_node	debug_node;
+	char 			*name;
+	umode_t 		mode;
+	void			*data;
+	const struct file_operations *fops;
+};
+#endif
+
 /***           runtime pm          ***/
 static int clk_pm_runtime_get(struct clk_core *core)
 {
@@ -629,6 +640,14 @@ bool __clk_is_enabled(struct clk *clk)
 }
 EXPORT_SYMBOL_GPL(__clk_is_enabled);
 
+bool __clk_is_prepared(struct clk *clk)
+{
+	if (!clk)
+		return false;
+
+	return clk_core_is_prepared(clk->core);
+}
+
 static bool mux_is_better_rate(unsigned long rate, unsigned long now,
 			   unsigned long best, unsigned long flags)
 {
@@ -2777,6 +2796,32 @@ int clk_set_max_rate(struct clk *clk, unsigned long rate)
 }
 EXPORT_SYMBOL_GPL(clk_set_max_rate);
 
+/**
+ * clk_set_rate_refresh - re-set a clocks rate, including triggering any
+ *			  notifiers.
+ *
+ * @clk: clock source
+ *
+ * Returns success (0) or negative errno.
+ */
+int clk_set_rate_refresh(struct clk *clk)
+{
+	int ret;
+
+	if (!clk)
+		return 0;
+
+        /* prevent racing with updates to the clock topology */
+	clk_prepare_lock();
+
+	ret = clk_core_set_rate_nolock(clk->core, clk->core->req_rate);
+
+	clk_prepare_unlock();
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(clk_set_rate_refresh);
+
 /**
  * clk_get_parent - return the parent of a clk
  * @clk: the clk whose parent gets returned
@@ -3742,6 +3787,59 @@ static void clk_debug_unregister(struct clk_core *core)
 	mutex_unlock(&clk_debug_lock);
 }
 
+static void clk_debug_add_debugfs_extra_node(struct clk_core *core, char *name,
+		umode_t mode, void *data, const struct file_operations *fops)
+{
+	struct debug_extra_node *debugfs_extra;
+
+	debugfs_extra = kzalloc(sizeof(*debugfs_extra), GFP_KERNEL);
+	if (!debugfs_extra)
+		return;
+
+	debugfs_extra->name = name;
+	debugfs_extra->mode = mode;
+	debugfs_extra->data = data;
+	debugfs_extra->fops = fops;
+
+	hlist_add_head(&debugfs_extra->debug_node, &core->debug_extra_nodes);
+}
+
+struct dentry *__clk_debugfs_add_file(struct clk *clk, char *name,
+	umode_t mode, void *data, const struct file_operations *fops)
+{
+	struct dentry *d = NULL;
+
+	if (clk->core->dentry)
+		d = debugfs_create_file(name, mode, clk->core->dentry, data,
+			fops);
+	else {
+		clk_debug_add_debugfs_extra_node(clk->core, name, mode, data,
+						 fops);
+		return ERR_PTR(-EAGAIN);
+	}
+
+	return d;
+}
+EXPORT_SYMBOL_GPL(__clk_debugfs_add_file);
+
+struct dentry *clk_debugfs_add_file(struct clk_hw *hw, char *name, umode_t mode,
+				void *data, const struct file_operations *fops)
+{
+	struct dentry *d = NULL;
+
+	if (hw->core->dentry)
+		d = debugfs_create_file(name, mode, hw->core->dentry, data,
+					fops);
+	else {
+		clk_debug_add_debugfs_extra_node(hw->core, name, mode, data,
+						 fops);
+		return ERR_PTR(-EAGAIN);
+	}
+
+	return d;
+}
+EXPORT_SYMBOL_GPL(clk_debugfs_add_file);
+
 /**
  * clk_debug_init - lazily populate the debugfs clk directory
  *
diff --git a/drivers/clk/tegra/clk-dfll.c b/drivers/clk/tegra/clk-dfll.c
index 58fa5a59e0c7..a64f134f4ac7 100644
--- a/drivers/clk/tegra/clk-dfll.c
+++ b/drivers/clk/tegra/clk-dfll.c
@@ -45,6 +45,7 @@
 #include <linux/regulator/consumer.h>
 #include <linux/reset.h>
 #include <linux/seq_file.h>
+#include <soc/tegra/cvb.h>
 
 #include "clk-dfll.h"
 #include "cvb.h"
@@ -312,8 +313,62 @@ struct tegra_dfll {
 	struct pinctrl_state		*pwm_enable_state;
 	struct pinctrl_state		*pwm_disable_state;
 	u32				reg_init_uV;
+	
+	/* spinlock protecting register accesses */
+	spinlock_t			lock;
+
+	/* Vmin set from external rail connected to dfll */
+	unsigned int			external_floor_output;
+	
+	/* Thermal parameters */
+	unsigned int			thermal_floor_output;
+	unsigned int			thermal_floor_index;
+	unsigned int			thermal_cap_output;
+	unsigned int			thermal_cap_index;
+	unsigned long			*dvco_rate_floors;
+	bool				dvco_cold_floor_done;
 };
 
+static u8 find_mv_out_cap(struct tegra_dfll *td, int mv);
+static struct tegra_dfll *tegra_dfll_dev;
+
+/**
+ * find_mv_out_cap - find the out_map index with voltage >= @mv
+ * @td: DFLL instance
+ * @mv: millivolts
+ *
+ * Find the lut index with voltage greater than or equal to @mv,
+ * and return it. If all of the voltages in out_map are less than
+ * @mv, then return the lut index * corresponding to the highest
+ * possible voltage, even though it's less than @mv.
+ */
+static u8 find_mv_out_cap(struct tegra_dfll *td, int mv)
+{
+	u8 i;
+
+	for (i = td->lut_bottom; i < td->lut_size; i++) {
+		if (td->lut_uv[i] >= mv * 1000)
+			return i;
+	}
+
+	return i - 1;	/* maximum possible output */
+}
+
+
+unsigned long dfll_request_get(struct tegra_dfll *td)
+{
+	/*
+	 * If running below dvco minimum rate with skipper resolution:
+	 * dvco min rate / 256 - return last requested rate rounded to 1kHz.
+	 * If running above dvco minimum, with closed loop resolution:
+	 * ref rate / 2 - return cl_dvfs target rate.
+	 */
+	if ((td->last_req.scale_bits + 1) < DFLL_FREQ_REQ_SCALE_MAX)
+		return (td->last_req.rate / 1000) * 1000;
+
+	return td->last_req.dvco_target_rate;
+}
+
 #define clk_hw_to_dfll(_hw) container_of(_hw, struct tegra_dfll, dfll_clk_hw)
 
 /* mode_name: map numeric DFLL modes to names for friendly console messages */
@@ -452,7 +507,7 @@ static void dfll_tune_low(struct tegra_dfll *td)
 	td->tune_range = DFLL_TUNE_LOW;
 
 	dfll_writel(td, td->soc->cvb->cpu_dfll_data.tune0_low, DFLL_TUNE0);
-	dfll_writel(td, td->soc->cvb->cpu_dfll_data.tune1, DFLL_TUNE1);
+	dfll_writel(td, td->soc->cvb->cpu_dfll_data.tune1_low, DFLL_TUNE1);
 	dfll_wmb(td);
 
 	if (td->soc->set_clock_trimmers_low)
@@ -2106,3 +2161,99 @@ struct tegra_dfll_soc_data *tegra_dfll_unregister(struct platform_device *pdev)
 	return td->soc;
 }
 EXPORT_SYMBOL(tegra_dfll_unregister);
+
+/**
+ * tegra_dfll_get_thermal_cap_mv - return millivolts of thermal cap
+ */
+u32 tegra_dfll_get_thermal_cap_mv(void)
+{
+	return tegra_dfll_dev->lut_uv[tegra_dfll_dev->thermal_cap_output]
+		/ 1000;
+}
+EXPORT_SYMBOL(tegra_dfll_get_thermal_cap_mv);
+
+/*
+ * External floor interface
+ */
+
+/**
+ * tegra_dfll_set_external_floor_mv - get Vmin setting from external
+ * rail, which is physically connected to cpu dfll rail
+ * @external_floor_mv: Vmin requested by connected external rail
+ */
+int tegra_dfll_set_external_floor_mv(int external_floor_mv)
+{
+	unsigned long flags;
+	unsigned int max;
+	u8 new_output;
+
+	if (!tegra_dfll_dev) {
+		pr_err("%s: null tegra dfll dev.\n", __func__);
+		return -EINVAL;
+	}
+
+	max = tegra_dfll_dev->lut_uv[tegra_dfll_dev->lut_max] / 1000;
+	if (external_floor_mv < 0 || external_floor_mv > max) {
+		pr_err("%s: invalid external vmin requested %d\n",
+				__func__, external_floor_mv);
+		return -EINVAL;
+	}
+
+	spin_lock_irqsave(&tegra_dfll_dev->lock, flags);
+
+	new_output = find_mv_out_cap(tegra_dfll_dev, external_floor_mv);
+	if (tegra_dfll_dev->external_floor_output != new_output) {
+		tegra_dfll_dev->external_floor_output = new_output;
+		if (tegra_dfll_dev->mode == DFLL_CLOSED_LOOP)
+			dfll_request_rate(tegra_dfll_dev,
+					dfll_request_get(tegra_dfll_dev));
+	}
+
+	spin_unlock_irqrestore(&tegra_dfll_dev->lock, flags);
+
+	/* Add delay to ensure new Vmin delivery is finished before return */
+	udelay(2 * DIV_ROUND_UP(1000000, tegra_dfll_dev->sample_rate));
+
+	return 0;
+}
+
+/**
+ * tegra_dfll_get_alignment - return DFLL alignment
+ */
+struct rail_alignment *tegra_dfll_get_alignment(void)
+{
+	if (!tegra_dfll_dev)
+		return ERR_PTR(-EPROBE_DEFER);
+	return &tegra_dfll_dev->soc->alignment;
+}
+EXPORT_SYMBOL(tegra_dfll_get_alignment);
+
+/**
+ * tegra_dfll_get_peak_thermal_floor_mv - get millivolts of peak thermal floor
+ */
+u32 tegra_dfll_get_peak_thermal_floor_mv(void)
+{
+	int mv = tegra_dfll_dev->soc->thermal_floor_table[0].millivolts;
+
+       return tegra_round_voltage(mv, &tegra_dfll_dev->soc->alignment, 1);
+}
+EXPORT_SYMBOL(tegra_dfll_get_peak_thermal_floor_mv);
+
+/**
+ * tegra_dfll_get_thermal_floor_mv - return millivolts of thermal floor
+ */
+u32 tegra_dfll_get_thermal_floor_mv(void)
+{
+	return tegra_dfll_dev->lut_uv[tegra_dfll_dev->thermal_floor_output]
+		/ 1000;
+}
+EXPORT_SYMBOL(tegra_dfll_get_thermal_floor_mv);
+
+/**
+ * tegra_dfll_get_min_millivolts - return DFLL min millivolts
+ */
+u32 tegra_dfll_get_min_millivolts(void)
+{
+	return tegra_dfll_dev->soc->min_millivolts;
+}
+EXPORT_SYMBOL(tegra_dfll_get_min_millivolts);
diff --git a/drivers/clk/tegra/clk-dfll.h b/drivers/clk/tegra/clk-dfll.h
index fb209eb5f365..3f006b361cfc 100644
--- a/drivers/clk/tegra/clk-dfll.h
+++ b/drivers/clk/tegra/clk-dfll.h
@@ -31,10 +31,15 @@ struct tegra_dfll_soc_data {
 	unsigned long max_freq;
 	const struct cvb_table *cvb;
 	struct rail_alignment alignment;
+	unsigned int min_millivolts;
 
 	void (*init_clock_trimmers)(void);
 	void (*set_clock_trimmers_high)(void);
 	void (*set_clock_trimmers_low)(void);
+	const struct thermal_tv *thermal_floor_table;
+	const struct thermal_tv *thermal_cap_table;
+	unsigned int thermal_floor_table_size;
+	unsigned int thermal_cap_table_size;
 };
 
 int tegra_dfll_register(struct platform_device *pdev,
diff --git a/drivers/platform/Kconfig b/drivers/platform/Kconfig
index 868b20361769..bdb4ee63fd6b 100644
--- a/drivers/platform/Kconfig
+++ b/drivers/platform/Kconfig
@@ -14,3 +14,8 @@ source "drivers/platform/olpc/Kconfig"
 source "drivers/platform/surface/Kconfig"
 
 source "drivers/platform/x86/Kconfig"
+
+config TEGRA_BWMGR
+	bool "TEGRA_BWMGR"
+	help
+	  The TEGRA_BWMGR
diff --git a/drivers/platform/tegra/mc/emc_bwmgr-t19x.c b/drivers/platform/tegra/mc/emc_bwmgr-t19x.c
index bcd550410f0c..d8ef0ca8cf4e 100644
--- a/drivers/platform/tegra/mc/emc_bwmgr-t19x.c
+++ b/drivers/platform/tegra/mc/emc_bwmgr-t19x.c
@@ -16,7 +16,7 @@
 #include <linux/io.h>
 #include <soc/tegra/bpmp_abi.h>
 #include <soc/tegra/tegra_bpmp.h>
-#include <soc/tegra/chip-id.h>
+#include <soc/tegra/fuse.h>
 
 /* T194 dram freq table */
 static u32 bwmgr_t194_dram_freq_table[] = { /* MHz */
diff --git a/drivers/platform/tegra/mc/emc_bwmgr.c b/drivers/platform/tegra/mc/emc_bwmgr.c
index c1941a55d157..8a9850036a8b 100644
--- a/drivers/platform/tegra/mc/emc_bwmgr.c
+++ b/drivers/platform/tegra/mc/emc_bwmgr.c
@@ -21,7 +21,7 @@
 #include <linux/debugfs.h>
 #include <linux/thermal.h>
 #include <linux/version.h>
-#include <soc/tegra/chip-id.h>
+#include <soc/tegra/fuse.h>
 
 #define CREATE_TRACE_POINTS
 #include <trace/events/bwmgr.h>
@@ -939,8 +939,6 @@ static struct dentry *debugfs_node_iso_cap;
 static struct dentry *debugfs_node_bw;
 static struct dentry *debugfs_node_iso_bw;
 static struct dentry *debugfs_node_emc_rate;
-static struct dentry *debugfs_node_emc_min;
-static struct dentry *debugfs_node_emc_max;
 static struct dentry *debugfs_node_core_emc_rate;
 static struct dentry *debugfs_node_clients_info;
 static struct dentry *debugfs_node_dram_channels;
@@ -1150,10 +1148,10 @@ static void bwmgr_debugfs_init(void)
 		debugfs_create_bool(
 			"clk_update_disabled", S_IRWXU, debugfs_dir,
 			&clk_update_disabled);
-		debugfs_node_emc_min = debugfs_create_u64(
+		debugfs_create_u64(
 			"emc_min_rate", S_IRUSR, debugfs_dir,
 			(u64 *) &bwmgr.emc_min_rate);
-		debugfs_node_emc_max = debugfs_create_u64(
+		debugfs_create_u64(
 			"emc_max_rate", S_IRUSR, debugfs_dir,
 			(u64 *) &bwmgr.emc_max_rate);
 		debugfs_node_core_emc_rate = debugfs_create_file(
diff --git a/drivers/platform/tegra/mc/mcerr.c b/drivers/platform/tegra/mc/mcerr.c
index c3a0d30b7625..bce30be4b144 100644
--- a/drivers/platform/tegra/mc/mcerr.c
+++ b/drivers/platform/tegra/mc/mcerr.c
@@ -42,7 +42,7 @@
 #include <linux/platform/tegra/mc-regs-t18x.h>
 
 static const struct of_device_id __mcerr_of_table_sentinel
-	__used __section("__mcerr_of_table_end");
+	__used __section("__mcerr_of_table");
 extern struct of_device_id __mcerr_of_table;
 
 static bool mcerr_throttle_enabled = true;
diff --git a/drivers/platform/tegra/mc/pmqos_bwmgr_client.c b/drivers/platform/tegra/mc/pmqos_bwmgr_client.c
index fae3df503754..b1f6a53175aa 100644
--- a/drivers/platform/tegra/mc/pmqos_bwmgr_client.c
+++ b/drivers/platform/tegra/mc/pmqos_bwmgr_client.c
@@ -41,8 +41,8 @@ static int pmqos_emc_freq_min_notify(struct notifier_block *b,
 	int ret = 0;
 	unsigned long floor_freq;
 
-	floor_freq = (unsigned long)pm_qos_request(PM_QOS_EMC_FREQ_MIN);
-	/* pm_qos_request() returns frequency in Khz */
+	floor_freq = (unsigned long)pm_qos_request_func(PM_QOS_EMC_FREQ_MIN);
+	/* pm_qos_request_func() returns frequency in Khz */
 	floor_freq *= 1000;
 	ret = tegra_bwmgr_set_emc(pmqos_bwmgr_handle, floor_freq,
 						TEGRA_BWMGR_SET_EMC_FLOOR);
diff --git a/drivers/regulator/core.c b/drivers/regulator/core.c
index c96bf095695f..d20de900625f 100644
--- a/drivers/regulator/core.c
+++ b/drivers/regulator/core.c
@@ -4505,6 +4505,37 @@ int regulator_get_voltage(struct regulator *regulator)
 }
 EXPORT_SYMBOL_GPL(regulator_get_voltage);
 
+/**
+ * regulator_get_constraint_voltages - get platform specific constraint voltage,
+ * @regulator: regulator source
+ * @min_uV: Minimum microvolts.
+ * @max_uV: Maximum microvolts.
+ *
+ * This returns the current regulator voltage in uV.
+ *
+ * NOTE: If the regulator is disabled it will return the voltage value. This
+ * function should not be used to determine regulator state.
+ */
+
+int regulator_get_constraint_voltages(struct regulator *regulator,
+	int *min_uV, int *max_uV)
+{
+	struct regulator_dev *rdev = regulator->rdev;
+
+	if (rdev->desc && rdev->desc->fixed_uV && rdev->desc->n_voltages == 1) {
+		*min_uV = rdev->desc->fixed_uV;
+		*max_uV = rdev->desc->fixed_uV;
+		return 0;
+	}
+	if (rdev->constraints) {
+		*min_uV = rdev->constraints->min_uV;
+		*max_uV = rdev->constraints->max_uV;
+		return 0;
+	}
+	return -EINVAL;
+}
+EXPORT_SYMBOL_GPL(regulator_get_constraint_voltages);
+
 /**
  * regulator_set_current_limit - set regulator output current limit
  * @regulator: regulator source
diff --git a/drivers/soc/tegra/Makefile b/drivers/soc/tegra/Makefile
index 6144eeff0cdc..a778a73af18e 100644
--- a/drivers/soc/tegra/Makefile
+++ b/drivers/soc/tegra/Makefile
@@ -4,8 +4,11 @@ obj-y += cbb/
 
 obj-y += common.o
 obj-y += tegra-bpmp-dvfs.o
+obj-y += cvb.o
 obj-$(CONFIG_SOC_TEGRA_FLOWCTRL) += flowctrl.o
 obj-$(CONFIG_SOC_TEGRA_PMC) += pmc.o
 obj-$(CONFIG_SOC_TEGRA20_VOLTAGE_COUPLER) += regulators-tegra20.o
 obj-$(CONFIG_SOC_TEGRA30_VOLTAGE_COUPLER) += regulators-tegra30.o
 obj-$(CONFIG_ARCH_TEGRA_186_SOC) += ari-tegra186.o
+obj-$(CONFIG_TEGRA_DVFS)       += tegra-dvfs.o
+obj-$(CONFIG_TEGRA_210_DVFS)   += tegra210-dvfs.o
diff --git a/drivers/soc/tegra/cvb.c b/drivers/soc/tegra/cvb.c
new file mode 100644
index 000000000000..74aa0c880e66
--- /dev/null
+++ b/drivers/soc/tegra/cvb.c
@@ -0,0 +1,175 @@
+/*
+ * Utility functions for parsing Tegra CVB voltage tables
+ *
+ * Copyright (C) 2012-2014 NVIDIA Corporation.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ */
+#include <linux/err.h>
+#include <linux/kernel.h>
+#include <linux/pm_opp.h>
+#include <soc/tegra/cvb.h>
+
+/* cvb_mv = ((c2 * speedo / s_scale + c1) * speedo / s_scale + c0) */
+int tegra_get_cvb_voltage(int speedo, int s_scale,
+			  const struct cvb_coefficients *cvb)
+{
+	int mv;
+
+	/* apply only speedo scale: output mv = cvb_mv * v_scale */
+	mv = DIV_ROUND_CLOSEST(cvb->c2 * speedo, s_scale);
+	mv = DIV_ROUND_CLOSEST((mv + cvb->c1) * speedo, s_scale) + cvb->c0;
+	return mv;
+}
+
+/* cvb_t_mv =
+   ((c3 * speedo / s_scale + c4 + c5 * T / t_scale) * T / t_scale) / v_scale */
+int tegra_get_cvb_t_voltage(int speedo, int s_scale, int t, int t_scale,
+			    struct cvb_coefficients *cvb)
+{
+	/* apply speedo & temperature scales: output mv = cvb_t_mv * v_scale */
+	int mv;
+	mv = DIV_ROUND_CLOSEST(cvb->c3 * speedo, s_scale) + cvb->c4 +
+		DIV_ROUND_CLOSEST(cvb->c5 * t, t_scale);
+	mv = DIV_ROUND_CLOSEST(mv * t, t_scale);
+	return mv;
+}
+
+int tegra_round_cvb_voltage(int mv, int v_scale,
+			    const struct rail_alignment *align)
+{
+	/* combined: apply voltage scale and round to cvb alignment step */
+	int uv;
+	int step = (align->step_uv ? : 1000) * v_scale;
+	int offset = align->offset_uv * v_scale;
+
+	uv = max(mv * 1000, offset) - offset;
+	uv = DIV_ROUND_UP(uv, step) * align->step_uv + align->offset_uv;
+	return uv / 1000;
+}
+
+enum {
+	DOWN,
+	UP
+};
+
+int tegra_round_voltage(int mv, const struct rail_alignment *align, int up)
+{
+	if (align->step_uv) {
+		int uv;
+
+		uv = max(mv * 1000, align->offset_uv) - align->offset_uv;
+		uv = (uv + (up ? align->step_uv - 1 : 0)) / align->step_uv;
+		return (uv * align->step_uv + align->offset_uv) / 1000;
+	}
+	return mv;
+}
+
+/**
+ * cvb_t_mv =
+ * ((c2 * speedo / s_scale + c1) * speedo / s_scale + c0) +
+ * ((c3 * speedo / s_scale + c4 + c5 * T / t_scale) * T / t_scale)
+ */
+static inline int get_cvb_thermal_floor(int speedo, int temp,
+					int s_scale, int t_scale,
+					const struct thermal_coefficients *coef)
+{
+	int cvb_mv, mv;
+
+	cvb_mv = tegra_get_cvb_voltage(speedo, s_scale, &coef->cvb_coef);
+
+	mv = DIV_ROUND_CLOSEST(coef->c3 * speedo, s_scale) + coef->c4 +
+		DIV_ROUND_CLOSEST(coef->c5 * temp, t_scale);
+	mv = DIV_ROUND_CLOSEST(mv * temp, t_scale) + cvb_mv;
+	return mv;
+}
+
+static int build_opp_table(struct device *dev, const struct cvb_table *table,
+			   struct rail_alignment *align,
+			   int speedo_value, unsigned long max_freq, int *vmin)
+{
+	int i, ret, dfll_mv, min_mv, max_mv;
+
+	if (!align->step_uv)
+		align->step_uv = table->alignment.step_uv;
+	if (!align->step_uv)
+		return -EINVAL;
+
+	if (!align->offset_uv)
+		align->offset_uv = table->alignment.offset_uv;
+
+	min_mv = tegra_round_voltage(table->min_millivolts, align, UP);
+	max_mv = tegra_round_voltage(table->max_millivolts, align, DOWN);
+
+	dfll_mv = tegra_get_cvb_voltage(
+		speedo_value, table->speedo_scale, &table->vmin_coefficients);
+	dfll_mv = tegra_round_cvb_voltage(dfll_mv, table->voltage_scale, align);
+	min_mv = max(min_mv, dfll_mv);
+
+	for (i = 0; i < MAX_DVFS_FREQS; i++) {
+		const struct cvb_table_freq_entry *entry = &table->entries[i];
+
+		if (!entry->freq || (entry->freq > max_freq))
+			break;
+
+		dfll_mv = tegra_get_cvb_voltage(
+			speedo_value, table->speedo_scale, &entry->coefficients);
+		dfll_mv = tegra_round_cvb_voltage(dfll_mv, table->voltage_scale, align);
+		dfll_mv = clamp(dfll_mv, min_mv, max_mv);
+
+		ret = dev_pm_opp_add(dev, entry->freq, dfll_mv * 1000);
+		if (ret)
+			return ret;
+	}
+
+	if (vmin)
+		*vmin = min_mv;
+
+	return 0;
+}
+
+/**
+ * tegra_cvb_build_thermal_table - build thermal table from Tegra CVB tables
+ * @table: the hardware characterization thermal table
+ * @speedo_value: speedo value of the HW module
+ * @soc_min_mv: minimum voltage applied across all temperature ranges
+ *
+ * The minimum voltage for the IP blocks inside Tegra SoCs might depend on
+ * the current temperature. This function calculates the voltage-thermal
+ * relations according to the given coefficients.   Note that if the
+ * coefficients are not defined, the fixed thermal floors in the @table will
+ * be used.  Returns 0 on success or a negative error code on failure.
+ */
+int tegra_cvb_build_thermal_table(const struct thermal_table *table,
+		int speedo_value, unsigned int soc_min_mv)
+{
+	int i;
+
+	if (!table)
+		return -EINVAL;
+
+	/* The vmin for the lowest trip point is fixed */
+	for (i = 1; i < table->thermal_floor_table_size; i++) {
+		unsigned int mv;
+
+		mv = get_cvb_thermal_floor(speedo_value,
+				table->thermal_floor_table[i-1].temp,
+				table->speedo_scale,
+				table->temp_scale,
+				&table->coefficients);
+		mv = DIV_ROUND_UP(mv, table->voltage_scale);
+		mv = max(mv, soc_min_mv);
+		table->thermal_floor_table[i].millivolts = max(mv,
+				table->thermal_floor_table[i].millivolts);
+	}
+
+	return 0;
+}
diff --git a/drivers/soc/tegra/pmc.c b/drivers/soc/tegra/pmc.c
index 162f52456f65..9b7553ae1e3f 100644
--- a/drivers/soc/tegra/pmc.c
+++ b/drivers/soc/tegra/pmc.c
@@ -480,7 +480,7 @@ to_powergate(struct generic_pm_domain *domain)
 	return container_of(domain, struct tegra_powergate, genpd);
 }
 
-static u32 tegra_pmc_readl(struct tegra_pmc *pmc, unsigned long offset)
+u32 tegra_pmc_readl(unsigned long offset)
 {
 	struct arm_smccc_res res;
 
@@ -501,6 +501,7 @@ static u32 tegra_pmc_readl(struct tegra_pmc *pmc, unsigned long offset)
 
 	return readl(pmc->base + offset);
 }
+EXPORT_SYMBOL(tegra_pmc_readl);
 
 static void tegra_pmc_writel(struct tegra_pmc *pmc, u32 value,
 			     unsigned long offset)
@@ -526,7 +527,7 @@ static void tegra_pmc_writel(struct tegra_pmc *pmc, u32 value,
 static u32 tegra_pmc_scratch_readl(struct tegra_pmc *pmc, unsigned long offset)
 {
 	if (pmc->tz_only)
-		return tegra_pmc_readl(pmc, offset);
+		return tegra_pmc_readl(offset);
 
 	return readl(pmc->scratch + offset);
 }
@@ -548,9 +549,9 @@ static void tegra_pmc_scratch_writel(struct tegra_pmc *pmc, u32 value,
 static inline bool tegra_powergate_state(int id)
 {
 	if (id == TEGRA_POWERGATE_3D && pmc->soc->has_gpu_clamps)
-		return (tegra_pmc_readl(pmc, GPU_RG_CNTRL) & 0x1) == 0;
+		return (tegra_pmc_readl(GPU_RG_CNTRL) & 0x1) == 0;
 	else
-		return (tegra_pmc_readl(pmc, PWRGATE_STATUS) & BIT(id)) != 0;
+		return (tegra_pmc_readl(PWRGATE_STATUS) & BIT(id)) != 0;
 }
 
 static inline bool tegra_powergate_is_valid(struct tegra_pmc *pmc, int id)
@@ -606,7 +607,7 @@ static int tegra20_powergate_set(struct tegra_pmc *pmc, unsigned int id,
 
 static inline bool tegra_powergate_toggle_ready(struct tegra_pmc *pmc)
 {
-	return !(tegra_pmc_readl(pmc, PWRGATE_TOGGLE) & PWRGATE_TOGGLE_START);
+	return !(tegra_pmc_readl(PWRGATE_TOGGLE) & PWRGATE_TOGGLE_START);
 }
 
 static int tegra114_powergate_set(struct tegra_pmc *pmc, unsigned int id,
@@ -1137,7 +1138,7 @@ static void tegra_pmc_restart(void)
 	u32 value;
 
 	/* reset everything but PMC_SCRATCH0 and PMC_RST_STATUS */
-	value = tegra_pmc_readl(pmc, PMC_CNTRL);
+	value = tegra_pmc_readl(PMC_CNTRL);
 	value |= PMC_CNTRL_MAIN_RST;
 	tegra_pmc_writel(pmc, value, PMC_CNTRL);
 }
@@ -1589,7 +1590,7 @@ static int tegra_io_pad_poll(struct tegra_pmc *pmc, unsigned long offset,
 	timeout = jiffies + msecs_to_jiffies(timeout);
 
 	while (time_after(timeout, jiffies)) {
-		value = tegra_pmc_readl(pmc, offset);
+		value = tegra_pmc_readl(offset);
 		if ((value & mask) == val)
 			return 0;
 
@@ -1709,7 +1710,7 @@ static int tegra_io_pad_is_powered(struct tegra_pmc *pmc, enum tegra_io_pad id)
 	status = pad->status;
 	mask = BIT(pad->dpd);
 
-	value = tegra_pmc_readl(pmc, status);
+	value = tegra_pmc_readl(status);
 
 	return !(value & mask);
 }
@@ -1730,7 +1731,7 @@ static int tegra_io_pad_set_voltage(struct tegra_pmc *pmc, enum tegra_io_pad id,
 	mutex_lock(&pmc->powergates_lock);
 
 	if (pmc->soc->has_impl_33v_pwr) {
-		value = tegra_pmc_readl(pmc, PMC_IMPL_E_33V_PWR);
+		value = tegra_pmc_readl(PMC_IMPL_E_33V_PWR);
 
 		if (voltage == TEGRA_IO_PAD_VOLTAGE_1V8)
 			value &= ~BIT(pad->voltage);
@@ -1740,12 +1741,12 @@ static int tegra_io_pad_set_voltage(struct tegra_pmc *pmc, enum tegra_io_pad id,
 		tegra_pmc_writel(pmc, value, PMC_IMPL_E_33V_PWR);
 	} else {
 		/* write-enable PMC_PWR_DET_VALUE[pad->voltage] */
-		value = tegra_pmc_readl(pmc, PMC_PWR_DET);
+		value = tegra_pmc_readl(PMC_PWR_DET);
 		value |= BIT(pad->voltage);
 		tegra_pmc_writel(pmc, value, PMC_PWR_DET);
 
 		/* update I/O voltage */
-		value = tegra_pmc_readl(pmc, PMC_PWR_DET_VALUE);
+		value = tegra_pmc_readl(PMC_PWR_DET_VALUE);
 
 		if (voltage == TEGRA_IO_PAD_VOLTAGE_1V8)
 			value &= ~BIT(pad->voltage);
@@ -1775,9 +1776,9 @@ static int tegra_io_pad_get_voltage(struct tegra_pmc *pmc, enum tegra_io_pad id)
 		return -ENOTSUPP;
 
 	if (pmc->soc->has_impl_33v_pwr)
-		value = tegra_pmc_readl(pmc, PMC_IMPL_E_33V_PWR);
+		value = tegra_pmc_readl(PMC_IMPL_E_33V_PWR);
 	else
-		value = tegra_pmc_readl(pmc, PMC_PWR_DET_VALUE);
+		value = tegra_pmc_readl(PMC_PWR_DET_VALUE);
 
 	if ((value & BIT(pad->voltage)) == 0)
 		return TEGRA_IO_PAD_VOLTAGE_1V8;
@@ -1853,7 +1854,7 @@ void tegra_pmc_enter_suspend_mode(enum tegra_suspend_mode mode)
 	do_div(ticks, USEC_PER_SEC);
 	tegra_pmc_writel(pmc, ticks, PMC_CPUPWROFF_TIMER);
 
-	value = tegra_pmc_readl(pmc, PMC_CNTRL);
+	value = tegra_pmc_readl(PMC_CNTRL);
 	value &= ~PMC_CNTRL_SIDE_EFFECT_LP0;
 	value |= PMC_CNTRL_CPU_PWRREQ_OE;
 	tegra_pmc_writel(pmc, value, PMC_CNTRL);
@@ -1999,7 +2000,7 @@ static void tegra_pmc_init_tsense_reset(struct tegra_pmc *pmc)
 	if (of_property_read_u32(np, "nvidia,pinmux-id", &pinmux))
 		pinmux = 0;
 
-	value = tegra_pmc_readl(pmc, PMC_SENSOR_CTRL);
+	value = tegra_pmc_readl(PMC_SENSOR_CTRL);
 	value |= PMC_SENSOR_CTRL_SCRATCH_WRITE;
 	tegra_pmc_writel(pmc, value, PMC_SENSOR_CTRL);
 
@@ -2025,7 +2026,7 @@ static void tegra_pmc_init_tsense_reset(struct tegra_pmc *pmc)
 
 	tegra_pmc_writel(pmc, value, PMC_SCRATCH55);
 
-	value = tegra_pmc_readl(pmc, PMC_SENSOR_CTRL);
+	value = tegra_pmc_readl(PMC_SENSOR_CTRL);
 	value |= PMC_SENSOR_CTRL_ENABLE_RST;
 	tegra_pmc_writel(pmc, value, PMC_SENSOR_CTRL);
 
@@ -2193,7 +2194,7 @@ static ssize_t reset_reason_show(struct device *dev,
 {
 	u32 value;
 
-	value = tegra_pmc_readl(pmc, pmc->soc->regs->rst_status);
+	value = tegra_pmc_readl(pmc->soc->regs->rst_status);
 	value &= pmc->soc->regs->rst_source_mask;
 	value >>= pmc->soc->regs->rst_source_shift;
 
@@ -2210,7 +2211,7 @@ static ssize_t reset_level_show(struct device *dev,
 {
 	u32 value;
 
-	value = tegra_pmc_readl(pmc, pmc->soc->regs->rst_status);
+	value = tegra_pmc_readl(pmc->soc->regs->rst_status);
 	value &= pmc->soc->regs->rst_level_mask;
 	value >>= pmc->soc->regs->rst_level_shift;
 
@@ -2356,7 +2357,7 @@ static int tegra210_pmc_irq_set_wake(struct irq_data *data, unsigned int on)
 	else
 		offset = PMC_WAKE_MASK;
 
-	value = tegra_pmc_readl(pmc, offset);
+	value = tegra_pmc_readl(offset);
 
 	if (on)
 		value |= BIT(bit);
@@ -2382,7 +2383,7 @@ static int tegra210_pmc_irq_set_type(struct irq_data *data, unsigned int type)
 	else
 		offset = PMC_WAKE_LEVEL;
 
-	value = tegra_pmc_readl(pmc, offset);
+	value = tegra_pmc_readl(offset);
 
 	switch (type) {
 	case IRQ_TYPE_EDGE_RISING:
@@ -2573,7 +2574,7 @@ static int tegra_pmc_clk_notify_cb(struct notifier_block *nb,
 
 static void pmc_clk_fence_udelay(u32 offset)
 {
-	tegra_pmc_readl(pmc, offset);
+	tegra_pmc_readl(offset);
 	/* pmc clk propagation delay 2 us */
 	udelay(2);
 }
@@ -2583,7 +2584,7 @@ static u8 pmc_clk_mux_get_parent(struct clk_hw *hw)
 	struct pmc_clk *clk = to_pmc_clk(hw);
 	u32 val;
 
-	val = tegra_pmc_readl(pmc, clk->offs) >> clk->mux_shift;
+	val = tegra_pmc_readl(clk->offs) >> clk->mux_shift;
 	val &= PMC_CLK_OUT_MUX_MASK;
 
 	return val;
@@ -2594,7 +2595,7 @@ static int pmc_clk_mux_set_parent(struct clk_hw *hw, u8 index)
 	struct pmc_clk *clk = to_pmc_clk(hw);
 	u32 val;
 
-	val = tegra_pmc_readl(pmc, clk->offs);
+	val = tegra_pmc_readl(clk->offs);
 	val &= ~(PMC_CLK_OUT_MUX_MASK << clk->mux_shift);
 	val |= index << clk->mux_shift;
 	tegra_pmc_writel(pmc, val, clk->offs);
@@ -2608,7 +2609,7 @@ static int pmc_clk_is_enabled(struct clk_hw *hw)
 	struct pmc_clk *clk = to_pmc_clk(hw);
 	u32 val;
 
-	val = tegra_pmc_readl(pmc, clk->offs) & BIT(clk->force_en_shift);
+	val = tegra_pmc_readl(clk->offs) & BIT(clk->force_en_shift);
 
 	return val ? 1 : 0;
 }
@@ -2617,7 +2618,7 @@ static void pmc_clk_set_state(unsigned long offs, u32 shift, int state)
 {
 	u32 val;
 
-	val = tegra_pmc_readl(pmc, offs);
+	val = tegra_pmc_readl(offs);
 	val = state ? (val | BIT(shift)) : (val & ~BIT(shift));
 	tegra_pmc_writel(pmc, val, offs);
 	pmc_clk_fence_udelay(offs);
@@ -2679,7 +2680,7 @@ static int pmc_clk_gate_is_enabled(struct clk_hw *hw)
 {
 	struct pmc_clk_gate *gate = to_pmc_clk_gate(hw);
 
-	return tegra_pmc_readl(pmc, gate->offs) & BIT(gate->shift) ? 1 : 0;
+	return tegra_pmc_readl(gate->offs) & BIT(gate->shift) ? 1 : 0;
 }
 
 static int pmc_clk_gate_enable(struct clk_hw *hw)
@@ -2839,11 +2840,22 @@ static const struct regmap_access_table pmc_usb_sleepwalk_table = {
 	.n_yes_ranges = ARRAY_SIZE(pmc_usb_sleepwalk_ranges),
 };
 
+static bool get_secure_pmc_setting(void);
+
+void tegra_pmc_writel_relaxed(u32 value, unsigned long offset)
+{
+	if (get_secure_pmc_setting())
+		tegra_pmc_writel(pmc, value, offset);
+	else
+		writel_relaxed(value, pmc->base + offset);
+}
+EXPORT_SYMBOL(tegra_pmc_writel_relaxed);
+
 static int tegra_pmc_regmap_readl(void *context, unsigned int offset, unsigned int *value)
 {
 	struct tegra_pmc *pmc = context;
 
-	*value = tegra_pmc_readl(pmc, offset);
+	*value = tegra_pmc_readl(offset);
 	return 0;
 }
 
@@ -3254,11 +3266,11 @@ static void tegra20_pmc_init(struct tegra_pmc *pmc)
 	u32 value, osc, pmu, off;
 
 	/* Always enable CPU power request */
-	value = tegra_pmc_readl(pmc, PMC_CNTRL);
+	value = tegra_pmc_readl(PMC_CNTRL);
 	value |= PMC_CNTRL_CPU_PWRREQ_OE;
 	tegra_pmc_writel(pmc, value, PMC_CNTRL);
 
-	value = tegra_pmc_readl(pmc, PMC_CNTRL);
+	value = tegra_pmc_readl(PMC_CNTRL);
 
 	if (pmc->sysclkreq_high)
 		value &= ~PMC_CNTRL_SYSCLK_POLARITY;
@@ -3274,7 +3286,7 @@ static void tegra20_pmc_init(struct tegra_pmc *pmc)
 	tegra_pmc_writel(pmc, value, PMC_CNTRL);
 
 	/* now enable the request */
-	value = tegra_pmc_readl(pmc, PMC_CNTRL);
+	value = tegra_pmc_readl(PMC_CNTRL);
 	value |= PMC_CNTRL_SYSCLK_OE;
 	tegra_pmc_writel(pmc, value, PMC_CNTRL);
 
@@ -3295,7 +3307,7 @@ static void tegra20_pmc_setup_irq_polarity(struct tegra_pmc *pmc,
 {
 	u32 value;
 
-	value = tegra_pmc_readl(pmc, PMC_CNTRL);
+	value = tegra_pmc_readl(PMC_CNTRL);
 
 	if (invert)
 		value |= PMC_CNTRL_INTR_POLARITY;
@@ -4282,6 +4294,41 @@ static const struct of_device_id tegra_pmc_match[] = {
 	{ }
 };
 
+static bool get_secure_pmc_setting(void)
+{
+	struct device_node *np;
+	static bool secure_pmc;
+	static bool initialized;
+	struct resource regs;
+
+	if (!initialized) {
+		initialized = true;
+		np = of_find_matching_node(NULL, tegra_pmc_match);
+		if (!np) {
+			pr_err("%s: compatible node not found\n", __func__);
+			secure_pmc = false;
+			return secure_pmc;
+		}
+		secure_pmc = of_find_property(np,"nvidia,secure-pmc",NULL);
+		pr_info("%s: done secure_pmc=%d\n", __func__, secure_pmc);
+
+		/*
+		 * Initialize pmc->base for calls which are performed much
+		 * before tegra_pmc_probe() or tegra_pmc_early_init().
+		 */
+		if (!secure_pmc) {
+			if (!(of_address_to_resource(np, 0, &regs) < 0))
+				pmc->base = ioremap(regs.start,
+						resource_size(&regs)); //ioremap_nocache
+			else {
+				pr_err("failed to set pmc-base\n");
+				WARN_ON(1);
+			}
+		}
+	}
+	return secure_pmc;
+}
+
 static void tegra_pmc_sync_state(struct device *dev)
 {
 	int err;
diff --git a/drivers/soc/tegra/tegra-dvfs.c b/drivers/soc/tegra/tegra-dvfs.c
new file mode 100644
index 000000000000..f5b941de6f63
--- /dev/null
+++ b/drivers/soc/tegra/tegra-dvfs.c
@@ -0,0 +1,2449 @@
+/*
+ * Copyright (C) 2010 Google, Inc.
+ *
+ * Author:
+ *	Colin Cross <ccross@google.com>
+ *
+ * Copyright (c) 2014-2018, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include <linux/kernel.h>
+#include <linux/clk.h>
+#include <linux/clkdev.h>
+#include <linux/clk-provider.h>
+#include <linux/debugfs.h>
+#include <linux/init.h>
+#include <linux/list.h>
+#include <linux/list_sort.h>
+#include <linux/module.h>
+#include <linux/regulator/consumer.h>
+#include <linux/suspend.h>
+#include <linux/thermal.h>
+#include <linux/reboot.h>
+#include <linux/platform_device.h>
+#include <linux/of.h>
+#include <linux/pm_opp.h>
+#include <linux/cpu.h>
+
+#include <soc/tegra/tegra-dfll.h>
+#include <soc/tegra/tegra-dvfs.h>
+
+struct dvfs_rail *tegra_cpu_rail;
+struct dvfs_rail *tegra_core_rail;
+static struct dvfs_rail *tegra_gpu_rail;
+
+bool core_dvfs_started;
+
+static LIST_HEAD(dvfs_rail_list);
+static DEFINE_MUTEX(dvfs_lock);
+
+static int dvfs_rail_update(struct dvfs_rail *rail);
+
+static inline int tegra_dvfs_rail_get_disable_level(struct dvfs_rail *rail)
+{
+	return rail->disable_millivolts ? : rail->nominal_millivolts;
+}
+
+static inline int tegra_dvfs_rail_get_suspend_level(struct dvfs_rail *rail)
+{
+	return rail->suspend_millivolts ? : rail->nominal_millivolts;
+}
+
+void tegra_dvfs_add_relationships(struct dvfs_relationship *rels, int n)
+{
+	int i;
+	struct dvfs_relationship *rel;
+
+	mutex_lock(&dvfs_lock);
+
+	for (i = 0; i < n; i++) {
+		rel = &rels[i];
+		list_add_tail(&rel->from_node, &rel->to->relationships_from);
+		list_add_tail(&rel->to_node, &rel->from->relationships_to);
+	}
+
+	mutex_unlock(&dvfs_lock);
+}
+
+static void init_rails_lists(struct dvfs_rail *rails[], int n)
+{
+	int i;
+	static bool initialized;
+
+	if (initialized)
+		return;
+
+	initialized = true;
+
+	for (i = 0; i < n; i++) {
+		INIT_LIST_HEAD(&rails[i]->dvfs);
+		INIT_LIST_HEAD(&rails[i]->relationships_from);
+		INIT_LIST_HEAD(&rails[i]->relationships_to);
+
+		list_add_tail(&rails[i]->node, &dvfs_rail_list);
+	}
+}
+
+void tegra_dvfs_init_rails_lists(struct dvfs_rail *rails[], int n)
+{
+	mutex_lock(&dvfs_lock);
+	init_rails_lists(rails, n);
+	mutex_unlock(&dvfs_lock);
+}
+
+int tegra_dvfs_init_rails(struct dvfs_rail *rails[], int n)
+{
+	int i, mv;
+
+	mutex_lock(&dvfs_lock);
+
+	init_rails_lists(rails, n);
+
+	for (i = 0; i < n; i++) {
+		mv = rails[i]->nominal_millivolts;
+		if (rails[i]->disable_millivolts > mv)
+			rails[i]->disable_millivolts = mv;
+		if (rails[i]->suspend_millivolts > mv)
+			rails[i]->suspend_millivolts = mv;
+
+		rails[i]->millivolts = mv;
+		rails[i]->new_millivolts = mv;
+		if (!rails[i]->step)
+			rails[i]->step = rails[i]->max_millivolts;
+		if (!rails[i]->step_up)
+			rails[i]->step_up = rails[i]->step;
+
+		if (!strcmp("vdd-cpu", rails[i]->reg_id))
+			tegra_cpu_rail = rails[i];
+		else if (!strcmp("vdd-core", rails[i]->reg_id))
+			tegra_core_rail = rails[i];
+		else if (!strcmp("vdd-gpu", rails[i]->reg_id))
+			tegra_gpu_rail = rails[i];
+	}
+
+	mutex_unlock(&dvfs_lock);
+
+	return 0;
+}
+
+static int dvfs_solve_relationship(struct dvfs_relationship *rel)
+{
+	return rel->solve(rel->from, rel->to);
+}
+
+static void dvfs_rail_stats_init(struct dvfs_rail *rail, int millivolts)
+{
+	int dvfs_rail_stats_range;
+
+	if (!rail->stats.bin_uv)
+		rail->stats.bin_uv = DVFS_RAIL_STATS_BIN;
+
+	dvfs_rail_stats_range =
+		(DVFS_RAIL_STATS_TOP_BIN - 1) * rail->stats.bin_uv / 1000;
+
+	rail->stats.last_update = ktime_get();
+	if (millivolts >= rail->min_millivolts) {
+		int i = 1 + (2 * (millivolts - rail->min_millivolts) * 1000 +
+			     rail->stats.bin_uv) / (2 * rail->stats.bin_uv);
+		rail->stats.last_index = min(i, DVFS_RAIL_STATS_TOP_BIN);
+	}
+
+	if (rail->max_millivolts >
+	    rail->min_millivolts + dvfs_rail_stats_range)
+		pr_warn("tegra_dvfs: %s: stats above %d mV will be squashed\n",
+			rail->reg_id,
+			rail->min_millivolts + dvfs_rail_stats_range);
+}
+
+static void dvfs_rail_stats_update(
+	struct dvfs_rail *rail, int millivolts, ktime_t now)
+{
+	rail->stats.time_at_mv[rail->stats.last_index] = ktime_add(
+		rail->stats.time_at_mv[rail->stats.last_index], ktime_sub(
+			now, rail->stats.last_update));
+	rail->stats.last_update = now;
+
+	if (rail->stats.off)
+		return;
+
+	if (millivolts >= rail->min_millivolts) {
+		int i = 1 + (2 * (millivolts - rail->min_millivolts) * 1000 +
+			     rail->stats.bin_uv) / (2 * rail->stats.bin_uv);
+		rail->stats.last_index = min(i, DVFS_RAIL_STATS_TOP_BIN);
+	} else if (millivolts == 0)
+			rail->stats.last_index = 0;
+}
+
+static int dvfs_rail_set_voltage_reg(struct dvfs_rail *rail, int millivolts)
+{
+	int ret;
+
+	if (rail->joint_rail_with_dfll)
+		tegra_dfll_set_external_floor_mv(rail->new_millivolts);
+
+	ret = regulator_set_voltage(rail->reg,
+		millivolts * 1000,
+		rail->max_millivolts * 1000);
+
+	return ret;
+}
+
+/**
+ * dvfs_rail_set_voltage - set voltage in millivolts to specific rail
+ *
+ * @rail: struct dvfs_rail * power rail context
+ * @millivolts: voltage in millivolts to be set to regulator
+ *
+ * Sets the voltage on a dvfs rail to a specific value, and updates any
+ * rails that depend on this rail.
+ */
+static int dvfs_rail_set_voltage(struct dvfs_rail *rail, int millivolts)
+{
+	int ret = 0;
+	struct dvfs_relationship *rel;
+	int step, offset;
+	int i;
+	int steps;
+	bool jmp_to_zero;
+
+	if (!rail->reg) {
+		if (millivolts == rail->millivolts)
+			return 0;
+		else
+			return -EINVAL;
+	}
+
+	if (millivolts > rail->millivolts) {
+		step = rail->step_up;
+		offset = step;
+	} else {
+		step = rail->step;
+		offset = -step;
+	}
+
+	if (rail->dfll_mode) {
+		rail->millivolts = millivolts;
+		rail->new_millivolts = millivolts;
+		dvfs_rail_stats_update(rail, millivolts, ktime_get());
+		return 0;
+	}
+
+	if (rail->disabled)
+		return 0;
+
+	rail->resolving_to = true;
+	jmp_to_zero = rail->jmp_to_zero &&
+			((millivolts == 0) || (rail->millivolts == 0));
+	if (jmp_to_zero || (rail->in_band_pm && rail->stats.off))
+		steps = 1;
+	else
+		steps = DIV_ROUND_UP(abs(millivolts - rail->millivolts), step);
+
+	for (i = 0; i < steps; i++) {
+		if ((i + 1) < steps)
+			rail->new_millivolts = rail->millivolts + offset;
+		else
+			rail->new_millivolts = millivolts;
+
+		/*
+		 * Before changing the voltage, tell each rail that depends
+		 * on this rail that the voltage will change.
+		 * This rail will be the "from" rail in the relationship,
+		 * the rail that depends on this rail will be the "to" rail.
+		 * from->millivolts will be the old voltage
+		 * from->new_millivolts will be the new voltage
+		 */
+		list_for_each_entry(rel, &rail->relationships_to, to_node) {
+			ret = dvfs_rail_update(rel->to);
+			if (ret)
+				goto out;
+		}
+
+		ret = dvfs_rail_set_voltage_reg(rail, rail->new_millivolts);
+		if (ret) {
+			pr_err("Failed to set dvfs regulator %s\n",
+					rail->reg_id);
+			goto out;
+		}
+
+		rail->millivolts = rail->new_millivolts;
+		dvfs_rail_stats_update(rail, rail->millivolts, ktime_get());
+
+		/*
+		 * After changing the voltage, tell each rail that depends
+		 * on this rail that the voltage has changed.
+		 * from->millivolts and from->new_millivolts will be the
+		 * new voltage
+		 */
+		list_for_each_entry(rel, &rail->relationships_to, to_node) {
+			ret = dvfs_rail_update(rel->to);
+			if (ret)
+				goto out;
+		}
+	}
+
+	if (unlikely(rail->millivolts != millivolts)) {
+		pr_err("%s: rail didn't reach target %d in %d steps (%d)\n",
+			__func__, millivolts, steps, rail->millivolts);
+		ret = -EINVAL;
+	}
+
+out:
+	rail->resolving_to = false;
+	return ret;
+}
+
+static int dvfs_rail_apply_limits(struct dvfs_rail *rail, int millivolts,
+				  bool warn_on_cap)
+{
+	int min_mv = rail->min_millivolts;
+	int max_mv = rail->max_millivolts;
+
+	if (rail->therm_floors) {
+		int i = rail->therm_floor_idx;
+
+		if (i < rail->therm_floors_size)
+			min_mv = rail->therm_floors[i].mv;
+	}
+
+	if (rail->therm_caps && warn_on_cap) {
+		int i = rail->therm_cap_idx;
+
+		if ((i > 0) && (millivolts > rail->therm_caps[i - 1].mv)) {
+			WARN(!rail->therm_cap_warned,
+			     "tegra_dvfs: %s set to %dmV above cap %dmV\n",
+			     rail->reg_id, millivolts,
+			     rail->therm_caps[i - 1].mv);
+			rail->therm_cap_warned = true;
+		} else {
+			rail->therm_cap_warned = false;
+		}
+	}
+
+	if (rail->override_millivolts) {
+		millivolts = rail->override_millivolts;
+		return millivolts;
+	} else if (rail->dbg_mv_offs) {
+		/* apply offset and ignore limits */
+		millivolts += rail->dbg_mv_offs;
+		return millivolts;
+	}
+
+	millivolts = clamp_val(millivolts, min_mv, max_mv);
+
+	return millivolts;
+}
+
+/**
+ * dvfs_rail_update - update rail voltage
+ *
+ * @rail: struct dvfs_rail * power rail context
+ *
+ * Determine the minimum valid voltage for a rail, taking into account
+ * the dvfs clocks and any rails that this rail depends on.  Calls
+ * dvfs_rail_set_voltage with the new voltage, which will call
+ * dvfs_rail_update on any rails that depend on this rail.
+ */
+static int dvfs_rail_update(struct dvfs_rail *rail)
+{
+	int millivolts = 0;
+	struct dvfs *d;
+	struct dvfs_relationship *rel;
+	int ret = 0;
+	int steps;
+
+	if (rail->disabled)
+		return 0;
+
+	/* if dvfs is suspended, return and handle it during resume */
+	if (rail->suspended)
+		return 0;
+
+	/* if regulators are not connected yet, return and handle it later */
+	if (!rail->reg)
+		return 0;
+
+	/* if rail update is entered while resolving circular dependencies,
+	   abort recursion */
+	if (rail->resolving_to)
+		return 0;
+
+	/* Find the maximum voltage requested by any clock */
+	list_for_each_entry(d, &rail->dvfs, reg_node)
+		millivolts = max(d->cur_millivolts, millivolts);
+
+	/* Apply offset and min/max limits if any clock is requesting voltage */
+	if (millivolts)
+		millivolts = dvfs_rail_apply_limits(rail, millivolts, true);
+	/* Keep current voltage if regulator is to be disabled via explicitly */
+	else if (rail->in_band_pm)
+		return 0;
+	/* Keep current voltage if regulator must not be disabled at run time */
+	else if (!rail->jmp_to_zero) {
+		WARN(1, "%s cannot be turned off by dvfs\n", rail->reg_id);
+		return 0;
+	}
+
+	/*
+	 * retry update if limited by from-relationship to account for
+	 * circular dependencies
+	 */
+	steps = DIV_ROUND_UP(abs(millivolts - rail->millivolts), rail->step);
+	for (; steps >= 0; steps--) {
+		rail->new_millivolts = millivolts;
+
+		/* Check any rails that this rail depends on */
+		list_for_each_entry(rel, &rail->relationships_from, from_node)
+			rail->new_millivolts = dvfs_solve_relationship(rel);
+
+		if (rail->new_millivolts == rail->millivolts)
+			break;
+
+		ret = dvfs_rail_set_voltage(rail, rail->new_millivolts);
+	}
+
+	return ret;
+}
+
+static int dvfs_rail_connect_to_regulator(struct device *dev,
+					  struct dvfs_rail *rail)
+{
+	struct regulator *reg;
+	int v;
+
+	if (!rail->reg) {
+		mutex_unlock(&dvfs_lock);
+		reg = regulator_get(dev, rail->reg_id);
+		mutex_lock(&dvfs_lock);
+		if (IS_ERR(reg)) {
+			pr_err("tegra_dvfs: failed to connect %s rail\n",
+			       rail->reg_id);
+			return PTR_ERR(reg);
+		}
+		rail->reg = reg;
+	}
+
+	if (!rail->in_band_pm) {
+		v = regulator_enable(rail->reg);
+		if (v < 0) {
+			pr_err("tegra_dvfs: failed on enabling regulator %s\n, err %d",
+				rail->reg_id, v);
+			return v;
+		}
+	}
+
+	v = regulator_get_voltage(rail->reg);
+	if (v < 0) {
+		pr_err("tegra_dvfs: failed initial get %s voltage\n",
+		       rail->reg_id);
+		return v;
+	}
+
+	if (!rail->min_millivolts) {
+		int min_uv, max_uv;
+
+		if (!regulator_get_constraint_voltages(rail->reg, &min_uv,
+						       &max_uv))
+			rail->min_millivolts = min_uv / 1000;
+	}
+
+	if (v > rail->nominal_millivolts * 1000) {
+		if (dvfs_rail_set_voltage_reg(rail, rail->nominal_millivolts)) {
+			pr_err("tegra_dvfs: failed lower %s voltage %d to %d\n",
+			       rail->reg_id, v, rail->nominal_millivolts);
+			return -EINVAL;
+		}
+		v = rail->nominal_millivolts * 1000;
+	}
+
+	rail->millivolts = v / 1000;
+	rail->new_millivolts = rail->millivolts;
+	dvfs_rail_stats_init(rail, rail->millivolts);
+
+	return 0;
+}
+
+static inline const int *dvfs_get_millivolts_pll(struct dvfs *d)
+{
+	if (d->therm_dvfs) {
+		int therm_idx = d->dvfs_rail->therm_scale_idx;
+
+		return d->millivolts + therm_idx * MAX_DVFS_FREQS;
+	}
+
+	return d->millivolts;
+}
+
+static inline const int *dvfs_get_millivolts(struct dvfs *d, unsigned long rate)
+{
+	if (tegra_dvfs_is_dfll_scale(d, rate))
+		return d->dfll_millivolts;
+
+	return dvfs_get_millivolts_pll(d);
+}
+
+static inline const int *dvfs_get_peak_millivolts(struct dvfs *d, unsigned long rate)
+{
+	if (tegra_dvfs_is_dfll_scale(d, rate))
+		return d->dfll_millivolts;
+
+	if (d->peak_millivolts)
+		return d->peak_millivolts;
+
+	return dvfs_get_millivolts_pll(d);
+}
+
+static unsigned long *dvfs_get_freqs(struct dvfs *d)
+{
+	if (d->use_alt_freqs)
+		return &d->alt_freqs[0];
+	else
+		return &d->freqs[0];
+}
+
+static int __tegra_dvfs_set_rate(struct dvfs *d, unsigned long rate)
+{
+	int i = 0;
+	int ret, mv;
+	unsigned long *freqs = dvfs_get_freqs(d);
+	const int *millivolts = dvfs_get_millivolts(d, rate);
+
+	if (freqs == NULL || millivolts == NULL)
+		return -ENODEV;
+
+	/*
+	 * On entry to dfll range limit 1st step to range bottom (full ramp of
+	 * voltage/rate is completed automatically in dfll mode)
+	 */
+	if (tegra_dvfs_is_dfll_range_entry(d, rate))
+		rate = d->use_dfll_rate_min;
+
+	if (rate > freqs[d->num_freqs - 1]) {
+		pr_warn("tegra-dvfs: rate %lu too high for dvfs on %s\n", rate,
+			d->clk_name);
+		return -EINVAL;
+	}
+
+	if (rate == 0) {
+		d->cur_millivolts = 0;
+	} else {
+		while (i < d->num_freqs && rate > freqs[i])
+			i++;
+
+		if ((d->max_millivolts) &&
+		    (millivolts[i] > d->max_millivolts)) {
+			pr_warn("tegra-dvfs: voltage %d too high for dvfs on %s\n",
+					millivolts[i], d->clk_name);
+			return -EINVAL;
+		}
+
+		mv = millivolts[i];
+		d->cur_millivolts = millivolts[i];
+	}
+
+	d->cur_rate = rate;
+
+	ret = dvfs_rail_update(d->dvfs_rail);
+	if (ret)
+		pr_err("Failed to set regulator %s for clock %s to %d mV\n",
+			d->dvfs_rail->reg_id, d->clk_name, d->cur_millivolts);
+
+	return ret;
+}
+
+static struct dvfs *tegra_clk_to_dvfs(struct clk *c)
+{
+	struct dvfs *d;
+	struct dvfs_rail *rail;
+
+	list_for_each_entry(rail, &dvfs_rail_list, node) {
+		list_for_each_entry(d, &rail->dvfs, reg_node) {
+			if (clk_is_match(c, d->clk))
+				return d;
+		}
+	}
+	return NULL;
+}
+
+/*
+ *  Using non alt frequencies always results in peak voltage
+ * (enforced by alt_freqs_validate())
+ */
+static int predict_non_alt_millivolts(struct dvfs *d, const int *millivolts,
+				      unsigned long rate)
+{
+	int i;
+	int vmin = d->dvfs_rail->min_millivolts;
+	unsigned long dvfs_unit = 1 * d->freqs_mult;
+
+	if (!millivolts)
+		return -ENODEV;
+
+	if (millivolts == d->dfll_millivolts)
+		vmin = tegra_dfll_get_min_millivolts();
+
+	for (i = 0; i < d->num_freqs; i++) {
+		unsigned long f = d->freqs[i];
+		if ((dvfs_unit < f) && (rate <= f))
+			break;
+	}
+
+	if (i == d->num_freqs)
+		i--;
+
+	return max(millivolts[i], vmin);
+}
+
+static int predict_millivolts(struct dvfs *d, const int *millivolts,
+			      unsigned long rate)
+{
+	int i;
+	unsigned long *freqs = dvfs_get_freqs(d);
+
+	if (!millivolts)
+		return -ENODEV;
+
+	for (i = 0; i < d->num_freqs; i++) {
+		if (rate <= freqs[i])
+			break;
+	}
+
+	if (i == d->num_freqs)
+		return -EINVAL;
+
+	return millivolts[i];
+}
+
+static int dvfs_rail_get_thermal_floor(struct dvfs_rail *rail)
+{
+	if (rail->therm_floors &&
+		rail->therm_floor_idx < rail->therm_floors_size)
+		return rail->therm_floors[rail->therm_floor_idx].mv;
+
+	return 0;
+}
+
+static int dvfs_dfll_get_peak_thermal_floor(void)
+{
+	struct rail_alignment *align = tegra_dfll_get_alignment();
+	int mv;
+
+	if (WARN_ON(IS_ERR(align)))
+		return 0;
+
+	mv = tegra_dfll_get_peak_thermal_floor_mv();
+	if (mv < 0)
+		return 0;
+
+	return tegra_round_voltage(mv, align, 1);
+}
+
+static int dvfs_get_peak_thermal_floor(struct dvfs *d, unsigned long rate)
+{
+	bool dfll_range = dvfs_is_dfll_range(d, rate);
+
+	if (!dfll_range && d->dvfs_rail->therm_floors)
+		return d->dvfs_rail->therm_floors[0].mv;
+	if (dfll_range)
+		return dvfs_dfll_get_peak_thermal_floor();
+	return 0;
+}
+
+static int predict_mv_at_hz_no_tfloor(struct dvfs *d, unsigned long rate)
+{
+	const int *millivolts;
+
+	millivolts = dvfs_get_millivolts(d, rate);
+
+	return predict_millivolts(d, millivolts, rate);
+}
+
+static int predict_mv_at_hz_cur_tfloor(struct dvfs *d, unsigned long rate)
+{
+	int mv;
+
+	mv = predict_mv_at_hz_no_tfloor(d, rate);
+
+	if (mv < 0)
+		return mv;
+
+	return max(mv, dvfs_rail_get_thermal_floor(d->dvfs_rail));
+}
+
+int opp_millivolts[MAX_DVFS_FREQS];
+unsigned long opp_frequencies[MAX_DVFS_FREQS];
+
+/**
+ * tegra_get_cpu_fv_table - get CPU frequencies/voltages table
+ *
+ * @num_freqs: number of frequencies
+ * @freqs: the array of frequencies
+ * @mvs: the array of voltages
+ *
+ * Get the frequency and voltage table using CPU OPP which were built by the
+ * DFLL driver.
+ */
+int tegra_get_cpu_fv_table(int *num_freqs, unsigned long **freqs, int **mvs)
+{
+	struct device *cpu_dev;
+	struct dev_pm_opp *opp;
+	unsigned long rate;
+	int i, ret = 0;
+
+	cpu_dev = get_cpu_device(0);
+	if (!cpu_dev)
+		return -EINVAL;
+
+	mutex_lock(&dvfs_lock);
+	for (i = 0, rate = 0;; rate++) {
+		rcu_read_lock();
+		opp = dev_pm_opp_find_freq_ceil(cpu_dev, &rate);
+		if (IS_ERR(opp)) {
+			rcu_read_unlock();
+			break;
+		}
+		opp_frequencies[i] = rate;
+		opp_millivolts[i++] = dev_pm_opp_get_voltage(opp);
+		rcu_read_unlock();
+	}
+	if (!i) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	*num_freqs = i;
+	*freqs = opp_frequencies;
+	*mvs = opp_millivolts;
+out:
+	mutex_unlock(&dvfs_lock);
+	return ret;
+}
+EXPORT_SYMBOL(tegra_get_cpu_fv_table);
+
+/**
+ * tegra_dvfs_predict_millivolts - return the safe voltage for running
+ *					the clock at one sepcific rate
+ *
+ * @c: struct clk * the clock which needs the voltage info
+ * @rate: the rate being predicted
+ *
+ * Extract the voltage table associated with the clock and return the safe
+ * voltage for ticking the clock at the specified rate
+ */
+int tegra_dvfs_predict_millivolts(struct clk *c, unsigned long rate)
+{
+	int mv;
+	struct dvfs *d;
+
+	d = tegra_clk_to_dvfs(c);
+	if (d == NULL)
+		return -EINVAL;
+
+	if (!rate)
+		return 0;
+
+	mutex_lock(&dvfs_lock);
+
+	mv = predict_mv_at_hz_no_tfloor(d, rate);
+
+	mutex_unlock(&dvfs_lock);
+
+	return mv;
+}
+EXPORT_SYMBOL(tegra_dvfs_predict_millivolts);
+
+int tegra_dvfs_predict_mv_at_hz_cur_tfloor(struct clk *c, unsigned long rate)
+{
+	int mv;
+	struct dvfs *d;
+
+	d = tegra_clk_to_dvfs(c);
+	if (d == NULL)
+		return -EINVAL;
+
+	if (!rate)
+		return 0;
+
+	mutex_lock(&dvfs_lock);
+
+	mv = predict_mv_at_hz_cur_tfloor(d, rate);
+
+	mutex_unlock(&dvfs_lock);
+
+	return mv;
+}
+EXPORT_SYMBOL(tegra_dvfs_predict_mv_at_hz_cur_tfloor);
+
+/*
+ * Predict minimum voltage required to run target clock at specified rate.
+ * Evaluate target clock domain V/F relation, and apply proper PLL or
+ * DFLL table depending on specified rate range. Apply maximum thermal floor
+ * across all temperature ranges.
+ */
+static int dvfs_predict_mv_at_hz_max_tfloor(struct dvfs *d, unsigned long rate)
+{
+	int mv;
+	const int *millivolts;
+
+	if (!d)
+		return -ENODATA;
+
+	millivolts = dvfs_get_peak_millivolts(d, rate);
+	mv = predict_non_alt_millivolts(d, millivolts, rate);
+	if (mv < 0)
+		return mv;
+
+	return max(mv, dvfs_get_peak_thermal_floor(d, rate));
+}
+
+int tegra_dvfs_predict_mv_at_hz_max_tfloor(struct clk *c, unsigned long rate)
+{
+	struct dvfs *d;
+	int mv;
+
+	d = tegra_clk_to_dvfs(c);
+	if (d == NULL)
+		return -EINVAL;
+
+	mutex_lock(&dvfs_lock);
+	mv = dvfs_predict_mv_at_hz_max_tfloor(d, rate);
+	mutex_unlock(&dvfs_lock);
+
+	return mv;
+}
+EXPORT_SYMBOL(tegra_dvfs_predict_mv_at_hz_max_tfloor);
+
+long tegra_dvfs_predict_hz_at_mv_max_tfloor(struct clk *c, int mv)
+{
+	int mv_at_f = 0, i;
+	struct dvfs *d;
+	const int *millivolts;
+	unsigned long rate = -EINVAL;
+
+	/* Recursively search for ancestor with DVFS */
+	do {
+		d = tegra_clk_to_dvfs(c);
+		if (!d)
+			c = clk_get_parent(c);
+	} while (!d && !IS_ERR_OR_NULL(c));
+
+	if (d == NULL)
+		return -EINVAL;
+
+	mutex_lock(&dvfs_lock);
+
+	if (d->alt_freqs)
+		goto out;
+
+	for (i = 0; i < d->num_freqs; i++) {
+		rate = d->freqs[i];
+		millivolts = dvfs_get_peak_millivolts(d, rate);
+		if (!millivolts)
+			goto out;
+
+		if (d->dvfs_rail->therm_floors)
+			mv_at_f = d->dvfs_rail->therm_floors[0].mv;
+		mv_at_f = max(millivolts[i], mv_at_f);
+		if (mv < mv_at_f)
+			break;
+	}
+
+	if (i)
+		rate = d->freqs[i - 1];
+	if (!i || (rate <= d->freqs_mult))
+		rate = -ENOENT;
+
+out:
+	mutex_unlock(&dvfs_lock);
+
+	return rate;
+}
+
+/**
+ * tegra_dvfs_set_rate - update rail voltage due to the clock rate change
+ *
+ * @c: struct clk * the clock which has changed rate
+ * @rate: the changed rate
+ *
+ * Check if the voltage of the power rail need to be updated due to the clock
+ * rate change.
+ */
+int tegra_dvfs_set_rate(struct clk *c, unsigned long rate)
+{
+	int ret = 0;
+	struct dvfs *d;
+
+	if (!core_dvfs_started)
+		return ret;
+
+	mutex_lock(&dvfs_lock);
+
+	d = tegra_clk_to_dvfs(c);
+	if (d)
+		ret = __tegra_dvfs_set_rate(d, rate);
+
+	mutex_unlock(&dvfs_lock);
+
+	return ret;
+}
+EXPORT_SYMBOL(tegra_dvfs_set_rate);
+
+/**
+ * tegra_dvfs_get_rate - get current rate used for determining rail voltage
+ *
+ * @c: struct clk * clock we want to know the rate of used for determining
+ *		    rail voltage
+ *
+ * Returns 0 if there is no dvfs for the clock.
+ */
+unsigned long tegra_dvfs_get_rate(struct clk *c)
+{
+	unsigned long rate = 0;
+	struct dvfs *d;
+
+	if (!core_dvfs_started)
+		return rate;
+
+	mutex_lock(&dvfs_lock);
+
+	d = tegra_clk_to_dvfs(c);
+	if (d)
+		rate = d->cur_rate;
+
+	mutex_unlock(&dvfs_lock);
+
+	return rate;
+}
+EXPORT_SYMBOL(tegra_dvfs_get_rate);
+
+/**
+ * tegra_dvfs_get_freqs - export dvfs frequency array associated with the clock
+ *
+ * @c: struct clk * the clock which needs the frequency table
+ * @freqs: the array of the frequencies
+ * @num_freqs: number of the frequencies
+ *
+ * Check if the voltage of the power rail need to be updated due to the clock
+ * rate change.
+ */
+int tegra_dvfs_get_freqs(struct clk *c, unsigned long **freqs, int *num_freqs)
+{
+	struct dvfs *d;
+
+	d = tegra_clk_to_dvfs(c);
+	if (d == NULL) {
+		if (core_dvfs_started) {
+			pr_err("Failed to get %s dvfs structure\n", __clk_get_name(c));
+			return -ENOSYS;
+		}
+		return -EINVAL;
+	}
+
+	*num_freqs = d->num_freqs;
+	*freqs = dvfs_get_freqs(d);
+
+	return 0;
+}
+EXPORT_SYMBOL(tegra_dvfs_get_freqs);
+
+unsigned long tegra_dvfs_get_maxrate(struct clk *c)
+{
+	unsigned long rate = 0;
+	int err, num_freqs;
+	unsigned long *freqs;
+
+	err = tegra_dvfs_get_freqs(c, &freqs, &num_freqs);
+	if (err < 0)
+		return rate;
+
+	return freqs[num_freqs - 1];
+}
+EXPORT_SYMBOL(tegra_dvfs_get_maxrate);
+
+unsigned long tegra_dvfs_round_rate(struct clk *c, unsigned long rate)
+{
+	int i, num_freqs;
+	unsigned long *freqs, ret;
+
+	mutex_lock(&dvfs_lock);
+	ret = tegra_dvfs_get_freqs(c, &freqs, &num_freqs);
+	if (IS_ERR_VALUE(ret))
+		goto out;
+
+	for (i = 0; i < num_freqs; i++) {
+		if (freqs[i] >= rate) {
+			ret = freqs[i];
+			goto out;
+		}
+	}
+	ret = freqs[i-1];
+
+out:
+	mutex_unlock(&dvfs_lock);
+	return ret;
+}
+
+int tegra_dvfs_use_alt_freqs_on_clk(struct clk *c, bool use_alt_freq)
+{
+	struct dvfs *d;
+	int err = -ENOENT;
+
+	mutex_lock(&dvfs_lock);
+
+	d = tegra_clk_to_dvfs(c);
+	if (d && d->alt_freqs) {
+		err = 0;
+		if (d->use_alt_freqs != use_alt_freq) {
+			d->use_alt_freqs = use_alt_freq;
+			if (__tegra_dvfs_set_rate(d, d->cur_rate) < 0) {
+				d->use_alt_freqs = !use_alt_freq;
+				pr_err("%s: %s: %s alt dvfs failed\n", __func__,
+					d->clk_name,
+					use_alt_freq ? "set" : "clear");
+				__tegra_dvfs_set_rate(d, d->cur_rate);
+				err = -EINVAL;
+			}
+		}
+	}
+
+	mutex_unlock(&dvfs_lock);
+
+	return err;
+}
+EXPORT_SYMBOL(tegra_dvfs_use_alt_freqs_on_clk);
+
+static int tegra_dvfs_clk_event(struct notifier_block *this,
+		unsigned long event, void *ptr)
+{
+	struct clk_notifier_data *cnd = ptr;
+	struct dvfs *d;
+	int new_mv, err = 0;
+
+	d = tegra_clk_to_dvfs(cnd->clk);
+	if (d == NULL)
+		return NOTIFY_DONE;
+
+	if (d->dvfs_rail == tegra_core_rail && !core_dvfs_started)
+		return NOTIFY_DONE;
+
+	if (!__clk_is_enabled(cnd->clk) && !__clk_is_prepared(cnd->clk) &&
+	    (d->dvfs_rail != tegra_gpu_rail))
+		return NOTIFY_DONE;
+
+	mutex_lock(&dvfs_lock);
+
+	switch (event) {
+	case PRE_RATE_CHANGE:
+		if (d->therm_dvfs) {
+			new_mv = predict_mv_at_hz_cur_tfloor(d, cnd->new_rate);
+			if (new_mv > d->cur_millivolts)
+				err = __tegra_dvfs_set_rate(d, cnd->new_rate);
+		} else if (cnd->old_rate < cnd->new_rate) {
+			err = __tegra_dvfs_set_rate(d, cnd->new_rate);
+		}
+		break;
+	case POST_RATE_CHANGE:
+		if (d->therm_dvfs) {
+			new_mv = predict_mv_at_hz_cur_tfloor(d, cnd->new_rate);
+			if (new_mv < d->cur_millivolts)
+				err = __tegra_dvfs_set_rate(d, cnd->new_rate);
+		} else if (cnd->old_rate > cnd->new_rate) {
+			err = __tegra_dvfs_set_rate(d, cnd->new_rate);
+		}
+		break;
+	case ABORT_RATE_CHANGE:
+		break;
+	}
+
+	mutex_unlock(&dvfs_lock);
+
+	if (err < 0)
+		return NOTIFY_BAD;
+
+	return NOTIFY_DONE;
+}
+
+static struct notifier_block tegra_dvfs_nb = {
+	.notifier_call = tegra_dvfs_clk_event,
+	.priority = 1,
+};
+
+static int use_alt_freq_get(void *data, u64 *val)
+{
+	struct clk *c = (struct clk *)data;
+	struct dvfs *d;
+
+	d = tegra_clk_to_dvfs(c);
+	if (!d)
+		*val = 0;
+	else
+		*val = d->use_alt_freqs;
+
+	return 0;
+}
+
+static int use_alt_freq_set(void *data, u64 val)
+{
+	struct clk *c = (struct clk *)data;
+
+	return tegra_dvfs_use_alt_freqs_on_clk(c, !!val);
+}
+DEFINE_SIMPLE_ATTRIBUTE(use_alt_freq_fops,
+			use_alt_freq_get, use_alt_freq_set, "%llu\n");
+
+static void cleanup_dvfs_table(struct dvfs *d)
+{
+	int i;
+
+	for (i = 0; i < MAX_DVFS_FREQS; i++) {
+		if (d->millivolts[i] == 0)
+			break;
+
+		if (d->freqs_mult)
+			d->freqs[i] *= d->freqs_mult;
+
+		/* If final frequencies are 0, pad with previous frequency */
+		if (d->freqs[i] == 0 && i > 1)
+			d->freqs[i] = d->freqs[i - 1];
+	}
+
+	/* Update num_freqs if not set at all, or set above cleaned max */
+	if (!d->num_freqs || (d->num_freqs > i))
+		d->num_freqs = i;
+}
+
+#ifdef CONFIG_TEGRA_CLK_DEBUG
+static int dvfs_freq_offset_get(void *data, u64 *val)
+{
+	struct dvfs *d = data;
+
+	*val = d->dbg_hz_offs;
+
+	return 0;
+}
+
+static int dvfs_freq_offset_set(void *data, u64 val)
+{
+	struct dvfs *d = data;
+	int i, ret = 0;
+	long offs = (long)val - d->dbg_hz_offs;
+	unsigned long unit_rate = 1 * d->freqs_mult;
+
+	if (!offs || !d->num_freqs)
+		return 0;
+
+	mutex_lock(&dvfs_lock);
+
+	for (i = 0; i <  d->num_freqs; i++) {
+		unsigned long rate = d->freqs[i];
+
+		if (rate <= unit_rate)
+			continue;
+
+		if ((offs < 0) && (rate <= unit_rate + (-offs))) {
+			ret = -EINVAL;
+			goto out;
+		}
+
+		d->freqs[i] = rate + offs;
+	}
+	d->dbg_hz_offs = (long)val;
+
+out:
+	mutex_unlock(&dvfs_lock);
+
+	return ret;
+}
+DEFINE_SIMPLE_ATTRIBUTE(dvfs_freq_offset_fops, dvfs_freq_offset_get,
+			dvfs_freq_offset_set, "%lld\n");
+#endif
+
+int tegra_setup_dvfs(struct clk *c, struct dvfs *d)
+{
+	cleanup_dvfs_table(d);
+
+	d->clk = c;
+
+	mutex_lock(&dvfs_lock);
+	list_add_tail(&d->reg_node, &d->dvfs_rail->dvfs);
+	mutex_unlock(&dvfs_lock);
+
+#ifdef CONFIG_TEGRA_CLK_DEBUG
+	__clk_debugfs_add_file(c, "dvfs_freq_offs", S_IRUGO | S_IWUSR, d,
+				&dvfs_freq_offset_fops);
+#endif
+	return 0;
+}
+
+int tegra_dvfs_add_alt_freqs(struct clk *c, struct dvfs *alt_d)
+{
+	struct dvfs *d;
+	int err = 0;
+
+	mutex_lock(&dvfs_lock);
+
+	d = tegra_clk_to_dvfs(c);
+	if (!d) {
+		err = -EINVAL;
+		goto out;
+	}
+
+	cleanup_dvfs_table(alt_d);
+
+	if (alt_d->num_freqs < d->num_freqs) {
+		pr_err("tegra_dvfs: %s: %d alt freqs below %d main freqs\n",
+		       d->clk_name, alt_d->num_freqs, d->num_freqs);
+		err = -EINVAL;
+		goto out;
+	}
+
+	d->alt_freqs = alt_d->freqs;
+
+	__clk_debugfs_add_file(c, "use_alt_freq", S_IRUGO | S_IWUSR, c,
+			       &use_alt_freq_fops);
+
+out:
+	mutex_unlock(&dvfs_lock);
+
+	return err;
+}
+
+static bool tegra_dvfs_all_rails_suspended(void)
+{
+	struct dvfs_rail *rail;
+
+	list_for_each_entry(rail, &dvfs_rail_list, node)
+		if (!rail->suspended && !rail->disabled)
+			return false;
+
+	return true;
+}
+
+static bool tegra_dvfs_from_rails_suspended_or_solved(struct dvfs_rail *to)
+{
+	struct dvfs_relationship *rel;
+
+	list_for_each_entry(rel, &to->relationships_from, from_node)
+		if ((!rel->from->suspended) &&
+		    (!rel->from->disabled) &&
+		    (!rel->solved_at_nominal))
+			return false;
+
+	return true;
+}
+
+static int tegra_dvfs_suspend_one(void)
+{
+	struct dvfs_rail *rail;
+	int mv;
+	int ret = 0;
+
+	list_for_each_entry(rail, &dvfs_rail_list, node) {
+		if ((rail->suspended) ||
+		    (rail->disabled) ||
+		    (!tegra_dvfs_from_rails_suspended_or_solved(rail)))
+			continue;
+
+		mv = tegra_dvfs_rail_get_suspend_level(rail);
+		mv = dvfs_rail_apply_limits(rail, mv, false);
+		/* apply suspend limit only if it is above current mv */
+		if (mv >= rail->millivolts)
+			ret = dvfs_rail_set_voltage(rail, mv);
+		if (ret) {
+			pr_err("tegra_dvfs: failed %s suspend at %d\n",
+			       rail->reg_id, rail->millivolts);
+			return ret;
+		}
+
+		rail->suspended = true;
+		return 0;
+	}
+	return -EINVAL;
+}
+
+static void tegra_dvfs_resume(void)
+{
+	struct dvfs_rail *rail;
+
+	mutex_lock(&dvfs_lock);
+
+	list_for_each_entry(rail, &dvfs_rail_list, node)
+		rail->suspended = false;
+
+	list_for_each_entry(rail, &dvfs_rail_list, node)
+		dvfs_rail_update(rail);
+
+	mutex_unlock(&dvfs_lock);
+}
+
+static int tegra_dvfs_suspend(void)
+{
+	int ret = 0;
+
+	mutex_lock(&dvfs_lock);
+
+	while (!tegra_dvfs_all_rails_suspended()) {
+		ret = tegra_dvfs_suspend_one();
+		if (ret)
+			break;
+	}
+
+	mutex_unlock(&dvfs_lock);
+
+	if (ret)
+		tegra_dvfs_resume();
+
+	return ret;
+}
+
+int tegra_dvfs_init_thermal_dvfs_voltages(int *therm_voltages,
+	int *peak_voltages, int freqs_num, int ranges_num, struct dvfs *d)
+{
+	int *millivolts;
+	int freq_idx, therm_idx;
+
+	for (therm_idx = 0; therm_idx < ranges_num; therm_idx++) {
+		millivolts = therm_voltages + therm_idx * MAX_DVFS_FREQS;
+		for (freq_idx = 0; freq_idx < freqs_num; freq_idx++) {
+			int mv = millivolts[freq_idx];
+			if ((mv > d->dvfs_rail->max_millivolts) ||
+			    (mv < d->dvfs_rail->min_millivolts) ||
+			    (freq_idx && (mv < millivolts[freq_idx - 1]))) {
+				WARN(1, "%s: invalid thermal dvfs entry %d(%d, %d)\n",
+				     d->clk_name, mv, freq_idx, therm_idx);
+				return -EINVAL;
+			}
+			if (mv > peak_voltages[freq_idx])
+				peak_voltages[freq_idx] = mv;
+		}
+	}
+
+	d->millivolts = therm_voltages;
+	d->peak_millivolts = peak_voltages;
+	d->therm_dvfs = ranges_num > 1;
+
+	return 0;
+}
+
+static int tegra_dvfs_pm_notifier_event(struct notifier_block *nb,
+				unsigned long event, void *data)
+{
+	if (event == PM_SUSPEND_PREPARE) {
+		if (tegra_dvfs_suspend())
+			return NOTIFY_STOP;
+		pr_info("tegra_dvfs: suspended\n");
+	} else if (event == PM_POST_SUSPEND) {
+		tegra_dvfs_resume();
+		pr_info("tegra_dvfs: resumed\n");
+	}
+	return NOTIFY_OK;
+};
+
+static struct notifier_block tegra_dvfs_pm_nb = {
+	.notifier_call = tegra_dvfs_pm_notifier_event,
+	.priority = -1,
+};
+
+static int tegra_dvfs_reboot_notify(struct notifier_block *nb,
+				unsigned long event, void *data)
+{
+	switch (event) {
+	case SYS_RESTART:
+	case SYS_HALT:
+	case SYS_POWER_OFF:
+		tegra_dvfs_suspend();
+		return NOTIFY_OK;
+	}
+	return NOTIFY_DONE;
+}
+
+static struct notifier_block tegra_dvfs_reboot_nb = {
+	.notifier_call = tegra_dvfs_reboot_notify,
+};
+
+static void __tegra_dvfs_rail_disable(struct dvfs_rail *rail)
+{
+	int ret = -EPERM;
+	int mv;
+
+	if (rail->dfll_mode) {
+		rail->disabled = true;
+		return;
+	}
+
+	mv = tegra_dvfs_rail_get_disable_level(rail);
+	mv = dvfs_rail_apply_limits(rail, mv, false);
+
+	if (mv >= rail->millivolts)
+		ret = dvfs_rail_set_voltage(rail, mv);
+	if (ret) {
+		pr_err("tegra_dvfs: failed to disable %s at %d\n",
+		       rail->reg_id, rail->millivolts);
+		return;
+	}
+	rail->disabled = true;
+}
+
+static void __tegra_dvfs_rail_enable(struct dvfs_rail *rail)
+{
+	rail->disabled = false;
+	dvfs_rail_update(rail);
+}
+
+void tegra_dvfs_rail_enable(struct dvfs_rail *rail)
+{
+	if (!rail)
+		return;
+
+	mutex_lock(&dvfs_lock);
+
+	if (rail->disabled)
+		__tegra_dvfs_rail_enable(rail);
+
+	mutex_unlock(&dvfs_lock);
+}
+
+void tegra_dvfs_rail_disable(struct dvfs_rail *rail)
+{
+	if (!rail)
+		return;
+
+	mutex_lock(&dvfs_lock);
+	if (rail->disabled)
+		goto out;
+
+	__tegra_dvfs_rail_disable(rail);
+out:
+	mutex_unlock(&dvfs_lock);
+}
+
+bool tegra_dvfs_is_dfll_range(struct clk *c, unsigned long rate)
+{
+	struct dvfs *d;
+
+	d = tegra_clk_to_dvfs(c);
+	if (d == NULL) {
+		pr_err("Failed to get dvfs structure\n");
+		return false;
+	}
+
+	return dvfs_is_dfll_range(d, rate);
+}
+EXPORT_SYMBOL(tegra_dvfs_is_dfll_range);
+
+int tegra_dvfs_set_dfll_range(struct clk *c, int range)
+{
+	struct dvfs *d;
+	int ret = -EINVAL;
+
+	mutex_lock(&dvfs_lock);
+	d = tegra_clk_to_dvfs(c);
+	if (d == NULL) {
+		pr_err("Failed to get dvfs structure\n");
+		goto out;
+	}
+
+	if (!d->dfll_millivolts)
+		goto out;
+
+	if ((range < DFLL_RANGE_NONE) || (range > DFLL_RANGE_HIGH_RATES))
+		goto out;
+
+	d->range = range;
+	ret = 0;
+out:
+	mutex_unlock(&dvfs_lock);
+	return ret;
+}
+EXPORT_SYMBOL(tegra_dvfs_set_dfll_range);
+
+int tegra_dvfs_dfll_mode_set(struct clk *c, unsigned long rate)
+{
+	struct dvfs *d;
+
+	mutex_lock(&dvfs_lock);
+
+	d = tegra_clk_to_dvfs(c);
+	if (d == NULL) {
+		pr_err("Failed to get dvfs structure\n");
+		mutex_unlock(&dvfs_lock);
+		return -EINVAL;
+	}
+
+	if (!d->dvfs_rail->dfll_mode) {
+		d->dvfs_rail->dfll_mode = true;
+		__tegra_dvfs_set_rate(d, rate);
+	}
+
+	mutex_unlock(&dvfs_lock);
+
+	return 0;
+}
+EXPORT_SYMBOL(tegra_dvfs_dfll_mode_set);
+
+int tegra_dvfs_dfll_mode_clear(struct clk *c, unsigned long rate)
+{
+	int ret = 0;
+	struct dvfs *d;
+
+	mutex_lock(&dvfs_lock);
+
+	d = tegra_clk_to_dvfs(c);
+	if (d == NULL) {
+		pr_err("Failed to get dvfs structure\n");
+		mutex_unlock(&dvfs_lock);
+		return -EINVAL;
+	}
+
+	if (d->dvfs_rail->dfll_mode) {
+		d->dvfs_rail->dfll_mode = false;
+		d->dvfs_rail->millivolts = regulator_get_voltage(
+				d->dvfs_rail->reg) / 1000;
+		if (d->dvfs_rail->disabled) {
+			d->dvfs_rail->disabled = false;
+			__tegra_dvfs_rail_disable(d->dvfs_rail);
+		}
+		ret = __tegra_dvfs_set_rate(d, rate);
+	}
+
+	mutex_unlock(&dvfs_lock);
+
+	return ret;
+}
+EXPORT_SYMBOL(tegra_dvfs_dfll_mode_clear);
+
+int tegra_dvfs_get_dfll_threshold(struct clk *c, unsigned long *rate)
+{
+	struct dvfs *d;
+
+	d = tegra_clk_to_dvfs(c);
+	if (d == NULL) {
+		pr_err("Failed to get dvfs structure\n");
+		return -EINVAL;
+	}
+
+	if (d->dvfs_rail && d->use_dfll_rate_min)
+		*rate = d->use_dfll_rate_min;
+
+	return 0;
+}
+EXPORT_SYMBOL(tegra_dvfs_get_dfll_threshold);
+
+int tegra_dvfs_core_count_thermal_states(enum tegra_dvfs_core_thermal_type type)
+{
+	if (IS_ERR_OR_NULL(tegra_core_rail) || !tegra_core_rail->is_ready)
+		return -EINVAL;
+
+	if (type == TEGRA_DVFS_CORE_THERMAL_FLOOR)
+		return tegra_core_rail->therm_floors_size;
+	else if (type == TEGRA_DVFS_CORE_THERMAL_CAP)
+		return tegra_core_rail->therm_caps_size;
+	else
+		return -EINVAL;
+}
+EXPORT_SYMBOL(tegra_dvfs_core_count_thermal_states);
+
+int tegra_dvfs_core_get_thermal_index(enum tegra_dvfs_core_thermal_type type)
+{
+	if (IS_ERR_OR_NULL(tegra_core_rail) || !tegra_core_rail->is_ready)
+		return -EINVAL;
+
+	if (type == TEGRA_DVFS_CORE_THERMAL_FLOOR)
+		return tegra_core_rail->therm_floor_idx;
+	else if (type == TEGRA_DVFS_CORE_THERMAL_CAP)
+		return tegra_core_rail->therm_cap_idx;
+	else
+		return -EINVAL;
+}
+EXPORT_SYMBOL(tegra_dvfs_core_get_thermal_index);
+
+int tegra_dvfs_core_update_thermal_index(enum tegra_dvfs_core_thermal_type type,
+					 unsigned long new_idx)
+{
+	struct dvfs_rail *rail = tegra_core_rail;
+	int ret = 0;
+
+	if (IS_ERR_OR_NULL(tegra_core_rail) || !tegra_core_rail->is_ready)
+		return -EINVAL;
+
+	mutex_lock(&dvfs_lock);
+	if (type == TEGRA_DVFS_CORE_THERMAL_FLOOR) {
+		if (rail->therm_floor_idx != new_idx) {
+			rail->therm_floor_idx = new_idx;
+			dvfs_rail_update(rail);
+		}
+	} else if (type == TEGRA_DVFS_CORE_THERMAL_CAP) {
+		if (rail->therm_cap_idx != new_idx) {
+			rail->therm_cap_idx = new_idx;
+			dvfs_rail_update(rail);
+		}
+	} else {
+		ret = -EINVAL;
+	}
+	mutex_unlock(&dvfs_lock);
+
+	return ret;
+}
+EXPORT_SYMBOL(tegra_dvfs_core_update_thermal_index);
+
+int tegra_dvfs_core_set_thermal_cap(struct clk *cap_clk,
+				    unsigned long thermal_index)
+{
+	int mv;
+	unsigned long rate = UINT_MAX;
+	struct dvfs_rail *rail = tegra_core_rail;
+
+	if (IS_ERR_OR_NULL(tegra_core_rail) || !tegra_core_rail->is_ready) {
+		pr_err("tegra_dvfs: not ready to set thermal cap on %s\n",
+		       cap_clk ? __clk_get_name(cap_clk) : "Unknown");
+		return -EINVAL;
+	}
+
+	if (rail->therm_caps && thermal_index) {
+		mv = rail->therm_caps[thermal_index - 1].mv;
+		rate = tegra_dvfs_predict_hz_at_mv_max_tfloor(cap_clk, mv);
+		if (IS_ERR_VALUE(rate)) {
+			pr_err("tegra_dvfs: failed to get %s rate @ %dmV\n",
+			       __clk_get_name(cap_clk), mv);
+			return -EINVAL;
+		}
+	}
+	pr_debug("tegra_dvfs: Set %lu on %s\n", rate, __clk_get_name(cap_clk));
+
+	if (clk_set_rate(cap_clk, rate)) {
+		pr_err("tegra_dvfs: failed to set cap rate %lu on %s\n",
+		       rate, __clk_get_name(cap_clk));
+		return -EINVAL;
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(tegra_dvfs_core_set_thermal_cap);
+
+struct dvfs_rail *tegra_dvfs_get_rail_by_name(char *name)
+{
+	struct dvfs_rail *rail;
+
+	list_for_each_entry(rail, &dvfs_rail_list, node) {
+		if (!strcmp(rail->reg_id, name))
+			return rail;
+	}
+
+	return NULL;
+}
+EXPORT_SYMBOL(tegra_dvfs_get_rail_by_name);
+
+bool tegra_dvfs_is_rail_up(struct dvfs_rail *rail)
+{
+	bool ret = false;
+
+	if (!rail)
+		return false;
+
+	if (!rail->in_band_pm)
+		return true;
+
+	mutex_lock(&dvfs_lock);
+	if (rail->reg)
+		ret = regulator_is_enabled(rail->reg) > 0;
+	mutex_unlock(&dvfs_lock);
+	return ret;
+}
+EXPORT_SYMBOL(tegra_dvfs_is_rail_up);
+
+int tegra_dvfs_rail_power_up(struct dvfs_rail *rail)
+{
+	int ret = -ENOENT;
+
+	if (!rail || !rail->in_band_pm)
+		return -EINVAL;
+
+	mutex_lock(&dvfs_lock);
+	if (rail->reg) {
+		ret = regulator_enable(rail->reg);
+		if (!ret && !timekeeping_suspended) {
+			rail->stats.off = false;
+			dvfs_rail_stats_update(rail, rail->millivolts,
+					       ktime_get());
+		}
+	}
+	mutex_unlock(&dvfs_lock);
+	return ret;
+}
+EXPORT_SYMBOL(tegra_dvfs_rail_power_up);
+
+int tegra_dvfs_rail_power_down(struct dvfs_rail *rail)
+{
+	int ret = -ENOENT;
+
+	if (!rail || !rail->in_band_pm)
+		return -EINVAL;
+
+	mutex_lock(&dvfs_lock);
+	if (rail->reg) {
+		ret = regulator_disable(rail->reg);
+		if (!ret && !timekeeping_suspended) {
+			dvfs_rail_stats_update(rail, 0, ktime_get());
+			rail->stats.off = true;
+
+		}
+	}
+	mutex_unlock(&dvfs_lock);
+	return ret;
+}
+EXPORT_SYMBOL(tegra_dvfs_rail_power_down);
+
+unsigned long tegra_dvfs_get_fmax_at_vmin_safe_t(struct clk *c)
+{
+	struct dvfs *d = tegra_clk_to_dvfs(c);
+
+	if (!d)
+		return 0;
+
+	return d->fmax_at_vmin_safe_t;
+}
+EXPORT_SYMBOL(tegra_dvfs_get_fmax_at_vmin_safe_t);
+
+bool tegra_dvfs_is_rail_ready(struct dvfs_rail *rail)
+{
+	return rail->is_ready;
+}
+EXPORT_SYMBOL(tegra_dvfs_is_rail_ready);
+
+/*
+ * Validate rail thermal floors/caps, and get its size.
+ * Valid floors/caps:
+ * - voltage limits are descending with temperature increasing.
+ * - the lowest limit is above rail minimum voltage in pll and
+ *   in dfll mode (if applicable).
+ * - the highest limit is below rail nominal voltage.
+ */
+static int get_thermal_limits_size(struct dvfs_rail *rail,
+				   enum tegra_dvfs_core_thermal_type type)
+{
+	const struct dvfs_therm_limits *limits;
+	int i;
+
+	if (type == TEGRA_DVFS_CORE_THERMAL_FLOOR)
+		limits = rail->therm_floors;
+	else if (type == TEGRA_DVFS_CORE_THERMAL_CAP)
+		limits = rail->therm_caps;
+	else
+		return -EINVAL;
+
+	if (!limits[0].mv) {
+		pr_warn("%s: Missing thermal limits\n", rail->reg_id);
+		return -EINVAL;
+	}
+
+	for (i = 0; i < MAX_THERMAL_LIMITS - 1; i++) {
+		if (!limits[i + 1].mv)
+			break;
+
+		if ((limits[i].temperature >= limits[i + 1].temperature) ||
+		    (limits[i].mv < limits[i + 1].mv)) {
+			pr_warn("%s: Unordered thermal limits\n",
+				rail->reg_id);
+			return -EINVAL;
+		}
+	}
+
+	if (limits[i].mv < rail->min_millivolts) {
+		pr_warn("%s: Thermal floors below minimum voltage\n",
+			rail->reg_id);
+		return -EINVAL;
+	}
+
+	return i + 1;
+}
+
+void tegra_dvfs_core_init_therm_limits(struct dvfs_rail *rail)
+{
+	int size;
+
+	size = get_thermal_limits_size(rail, TEGRA_DVFS_CORE_THERMAL_FLOOR);
+	if (size <= 0 || rail->therm_floors[0].mv > rail->nominal_millivolts) {
+		rail->therm_floors = NULL;
+		rail->therm_floors_size = 0;
+		pr_warn("%s: invalid Vmin thermal floors\n", rail->reg_id);
+	} else {
+		rail->therm_floors_size = size;
+		rail->therm_floor_idx = 0;
+	}
+
+	size = get_thermal_limits_size(rail, TEGRA_DVFS_CORE_THERMAL_CAP);
+	if (size <= 0) {
+		rail->therm_caps = NULL;
+		rail->therm_caps_size = 0;
+		pr_warn("%s: invalid Vmax thermal caps\n", rail->reg_id);
+	} else {
+		rail->therm_caps_size = size;
+		/*
+		 * Core voltage and module rates are not throttled on boot until
+		 * CORE_CDEV_TYPE_CAP is registered. In fact, core boot voltage
+		 * is allowed above high temperature cap during boot.
+		 */
+		rail->therm_cap_idx = 0;
+	}
+}
+
+static int tegra_config_dvfs(struct dvfs_rail *rail)
+{
+	int i;
+	struct dvfs *d;
+
+	list_for_each_entry(d, &rail->dvfs, reg_node) {
+		if (__clk_is_enabled(d->clk) || __clk_is_prepared(d->clk)) {
+			d->cur_rate = clk_get_rate(d->clk);
+			d->cur_millivolts = d->max_millivolts;
+
+			for (i = 0; i < d->num_freqs; i++)
+				if (d->cur_rate <= d->freqs[i])
+					break;
+
+			if (i != d->num_freqs)
+				d->cur_millivolts = d->millivolts[i];
+		}
+
+		mutex_unlock(&dvfs_lock);
+		clk_notifier_register(d->clk, &tegra_dvfs_nb);
+		mutex_lock(&dvfs_lock);
+	}
+
+	return 0;
+}
+
+static int tegra_dvfs_regulator_init(struct device *dev)
+{
+	struct dvfs_rail *rail;
+	int err;
+
+	mutex_lock(&dvfs_lock);
+
+	list_for_each_entry(rail, &dvfs_rail_list, node) {
+		err = dvfs_rail_connect_to_regulator(dev, rail);
+		if (err) {
+			if (!rail->disabled)
+				__tegra_dvfs_rail_disable(rail);
+
+			mutex_unlock(&dvfs_lock);
+			return err;
+		}
+	}
+
+	list_for_each_entry(rail, &dvfs_rail_list, node) {
+		tegra_config_dvfs(rail);
+		if (rail->disabled) {
+			/* Overwrite boot voltage with nominal */
+			rail->disabled = false;
+			__tegra_dvfs_rail_disable(rail);
+		} else {
+			__tegra_dvfs_rail_enable(rail);  /* update to clks */
+		}
+	}
+
+	core_dvfs_started = true;
+
+	mutex_unlock(&dvfs_lock);
+
+	register_pm_notifier(&tegra_dvfs_pm_nb);
+	register_reboot_notifier(&tegra_dvfs_reboot_nb);
+
+	return 0;
+}
+
+static int tegra_vts_get_max_state(struct thermal_cooling_device *cdev,
+					unsigned long *max_state)
+{
+	struct dvfs_rail *rail = cdev->devdata;
+
+	*max_state = rail->vts_number_of_trips;
+
+	return 0;
+}
+
+static int tegra_vts_get_cur_state(struct thermal_cooling_device *cdev,
+					unsigned long *cur_state)
+{
+	struct dvfs_rail *rail = cdev->devdata;
+
+	*cur_state = rail->therm_scale_idx;
+
+	return 0;
+}
+
+static int tegra_vts_set_cur_state(struct thermal_cooling_device *cdev,
+					 unsigned long cur_state)
+{
+	struct dvfs_rail *rail = cdev->devdata;
+	struct dvfs *d, *first;
+	int ret = 0;
+
+	mutex_lock(&dvfs_lock);
+
+	if (rail->therm_scale_idx == cur_state)
+		goto out;
+
+	rail->therm_scale_idx = cur_state;
+
+	first = list_first_entry(&rail->dvfs, struct dvfs, reg_node);
+	if (first->therm_dvfs && first->na_dvfs && first->cur_rate) {
+		/* only GPU thermal DVFS can be noise aware and this
+		 * rail has only a single clock. Therefor we can just
+		 * update the NA DVFS config by doing calling
+		 * clk_set_rate_refresh and leave the normal DVFS notifier
+		 * to handle the voltage update.
+		 */
+		mutex_unlock(&dvfs_lock);
+		return clk_set_rate_refresh(first->clk);
+	} else if ((!first->therm_dvfs || !first->na_dvfs) &&
+			first->dvfs_rail) {
+		list_for_each_entry(d, &rail->dvfs, reg_node) {
+			if (d->therm_dvfs)
+				d->cur_millivolts =
+				predict_mv_at_hz_cur_tfloor(d, d->cur_rate);
+		}
+		ret = dvfs_rail_update(first->dvfs_rail);
+	}
+
+out:
+	mutex_unlock(&dvfs_lock);
+
+	return ret;
+}
+
+static struct thermal_cooling_device_ops tegra_vts_cooling_ops = {
+	.get_max_state = tegra_vts_get_max_state,
+	.get_cur_state = tegra_vts_get_cur_state,
+	.set_cur_state = tegra_vts_set_cur_state,
+};
+
+#ifdef CONFIG_DEBUG_FS
+
+/* To emulate and show rail relations with 0 mV on dependent rail-to */
+static struct dvfs_rail show_to;
+static struct dvfs_relationship show_rel;
+
+static int dvfs_tree_show(struct seq_file *s, void *data)
+{
+	struct dvfs *d;
+	struct dvfs_rail *rail;
+	struct dvfs_relationship *rel;
+	int cur_max_millivolts = INT_MIN;
+	int num_clks;
+
+	seq_puts(s, "   clock           rate       mV\n");
+	seq_puts(s, "-------------------------------------\n");
+
+	mutex_lock(&dvfs_lock);
+
+	list_for_each_entry(rail, &dvfs_rail_list, node) {
+		int therm_mv = 0;
+
+		seq_printf(s, "%s %d mV%s%s:\n", rail->reg_id, rail->millivolts,
+			   rail->stats.off ? " OFF" : " ON",
+			   rail->dfll_mode ? " dfll mode" :
+				rail->disabled ? " disabled" : "");
+		list_for_each_entry(rel, &rail->relationships_from, from_node) {
+			show_rel = *rel;
+			show_rel.to = &show_to;
+			show_to = *rel->to;
+			show_to.millivolts = show_to.new_millivolts = 0;
+			seq_printf(s, "   %-10s %-7d mV %-4d mV .. %-4d mV\n",
+				rel->from->reg_id, rel->from->millivolts,
+				dvfs_solve_relationship(&show_rel),
+				dvfs_solve_relationship(rel));
+		}
+		seq_printf(s, "   %-26s %-4d mV\n", "nominal",
+			   rail->nominal_millivolts);
+		seq_printf(s, "   %-26s %-4d mV\n", "minimum",
+			   rail->min_millivolts);
+		seq_printf(s, "   %-26s %-4d mV\n", "offset", rail->dbg_mv_offs);
+		seq_printf(s, "   %-26s %-4d mV\n", "override",
+			   rail->override_millivolts);
+
+		if (rail->dfll_mode) {
+			therm_mv = tegra_dfll_get_thermal_floor_mv();
+		} else if ((rail->therm_floors) &&
+			   (rail->therm_floor_idx < rail->therm_floors_size)) {
+			therm_mv = rail->therm_floors[rail->therm_floor_idx].mv;
+		}
+		seq_printf(s, "   %-26s %-4d mV\n", "therm_floor", therm_mv);
+
+		therm_mv = 0;
+		if (rail->dfll_mode) {
+			therm_mv = tegra_dfll_get_thermal_cap_mv();
+		} else if ((rail->therm_caps) &&
+			   (rail->therm_cap_idx > 0)) {
+			therm_mv = rail->therm_caps[rail->therm_cap_idx - 1].mv;
+		}
+		seq_printf(s, "   %-26s %-4d mV\n", "therm_cap", therm_mv);
+
+		num_clks = 0;
+		list_for_each_entry(d, &rail->dvfs, reg_node) {
+			num_clks++;
+			if (d->cur_millivolts > cur_max_millivolts)
+				cur_max_millivolts = d->cur_millivolts;
+		}
+
+		while (num_clks > 0) {
+			int next_max_millivolts = INT_MIN;
+
+			list_for_each_entry(d, &rail->dvfs, reg_node) {
+				if (d->cur_millivolts > next_max_millivolts &&
+				    d->cur_millivolts < cur_max_millivolts)
+					next_max_millivolts = d->cur_millivolts;
+
+				if (d->cur_millivolts != cur_max_millivolts)
+					continue;
+
+				seq_printf(s, "   %-15s %-10lu %-4d mV\n",
+					d->clk_name, d->cur_rate,
+					d->cur_millivolts);
+				num_clks--;
+				WARN_ON(num_clks < 0);
+			}
+			cur_max_millivolts = next_max_millivolts;
+		}
+	}
+
+	mutex_unlock(&dvfs_lock);
+
+	return 0;
+}
+
+static int dvfs_tree_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, dvfs_tree_show, inode->i_private);
+}
+
+static const struct file_operations dvfs_tree_fops = {
+	.open		= dvfs_tree_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+static int dvfs_table_show(struct seq_file *s, void *data)
+{
+	int i;
+	struct dvfs *d;
+	struct dvfs_rail *rail;
+	const int *v_pll, *last_v_pll = NULL;
+	const int *v_dfll, *last_v_dfll = NULL;
+
+	seq_puts(s, "DVFS tables: units mV/MHz\n");
+
+	mutex_lock(&dvfs_lock);
+
+	list_for_each_entry(rail, &dvfs_rail_list, node) {
+		seq_printf(s, "%-8s table version: %s\n",
+			   rail->reg_id, rail->nvver ? : "N/A");
+	}
+
+	list_for_each_entry(rail, &dvfs_rail_list, node) {
+		list_for_each_entry(d, &rail->dvfs, reg_node) {
+			bool mv_done = false;
+			v_pll =  dvfs_get_millivolts_pll(d);
+			v_dfll = d->dfll_millivolts;
+
+			if (v_pll && (last_v_pll != v_pll)) {
+				if (!mv_done) {
+					seq_puts(s, "\n");
+					mv_done = true;
+				}
+				last_v_pll = v_pll;
+				seq_printf(s, "%-16s", rail->reg_id);
+				for (i = 0; i < d->num_freqs; i++)
+					seq_printf(s, "%7d", v_pll[i]);
+				seq_puts(s, "\n");
+			}
+
+			if (v_dfll && (last_v_dfll != v_dfll)) {
+				if (!mv_done) {
+					seq_puts(s, "\n");
+					mv_done = true;
+				}
+				last_v_dfll = v_dfll;
+				seq_printf(s, "%-8s (dfll) ", rail->reg_id);
+				for (i = 0; i < d->num_freqs; i++)
+					seq_printf(s, "%7d", v_dfll[i]);
+				seq_puts(s, "\n");
+			}
+
+			seq_printf(s, "%-16s", d->clk_name);
+			for (i = 0; i < d->num_freqs; i++) {
+				unsigned int f = d->freqs[i]/100000;
+				seq_printf(s, " %4u.%u", f/10, f%10);
+			}
+			if (d->alt_freqs) {
+				seq_puts(s, "\n");
+				seq_printf(s, "%-10s (alt)", d->clk_name);
+				for (i = 0; i < d->num_freqs; i++) {
+					unsigned int f = d->alt_freqs[i]/100000;
+					seq_printf(s, " %4u.%u", f/10, f%10);
+				}
+			}
+
+			seq_puts(s, "\n");
+		}
+	}
+
+	mutex_unlock(&dvfs_lock);
+
+	return 0;
+}
+
+static int dvfs_table_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, dvfs_table_show, inode->i_private);
+}
+
+static const struct file_operations dvfs_table_fops = {
+	.open		= dvfs_table_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+static int rail_stats_save_to_buf(char *buf, int len)
+{
+	int i;
+	struct dvfs_rail *rail;
+	char *str = buf;
+	char *end = buf + len;
+
+	str += scnprintf(str, end - str, "%-12s %-10s\n", "millivolts", "time");
+
+	mutex_lock(&dvfs_lock);
+
+	list_for_each_entry(rail, &dvfs_rail_list, node) {
+		str += scnprintf(str, end - str, "%s (bin: %d.%dmV)\n",
+			   rail->reg_id,
+			   rail->stats.bin_uv / 1000,
+			   (rail->stats.bin_uv / 10) % 100);
+
+		dvfs_rail_stats_update(rail, -1, ktime_get());
+
+		str += scnprintf(str, end - str, "%-12d %-10llu\n", 0,
+			jiffies_64_to_clock_t(msecs_to_jiffies(
+				ktime_to_ms(rail->stats.time_at_mv[0]))));
+
+		for (i = 1; i <= DVFS_RAIL_STATS_TOP_BIN; i++) {
+			ktime_t ktime_zero = ktime_set(0, 0);
+			if (ktime_compare(rail->stats.time_at_mv[i], ktime_zero) == 0)
+				continue;
+			str += scnprintf(str, end - str, "%-12d %-10llu\n",
+				rail->min_millivolts +
+				(i - 1) * rail->stats.bin_uv / 1000,
+				jiffies_64_to_clock_t(msecs_to_jiffies(
+					ktime_to_ms(rail->stats.time_at_mv[i])))
+			);
+		}
+	}
+	mutex_unlock(&dvfs_lock);
+	return str - buf;
+}
+
+static int rail_stats_show(struct seq_file *s, void *data)
+{
+	char *buf = kzalloc(PAGE_SIZE, GFP_KERNEL);
+	int size = 0;
+
+	if (!buf)
+		return -ENOMEM;
+
+	size = rail_stats_save_to_buf(buf, PAGE_SIZE);
+	seq_write(s, buf, size);
+	kfree(buf);
+	return 0;
+}
+
+static int rail_stats_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, rail_stats_show, inode->i_private);
+}
+
+static const struct file_operations rail_stats_fops = {
+	.open		= rail_stats_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+static int gpu_dvfs_t_show(struct seq_file *s, void *data)
+{
+	int i, j;
+	int num_ranges = 1;
+	int *trips = NULL;
+	struct dvfs *d;
+	struct dvfs_rail *rail = tegra_gpu_rail;
+	int max_mv[MAX_DVFS_FREQS] = {};
+
+	if (!tegra_gpu_rail) {
+		seq_printf(s, "Only supported for T124 or higher\n");
+		return -ENOSYS;
+	}
+
+	mutex_lock(&dvfs_lock);
+
+	d = list_first_entry(&rail->dvfs, struct dvfs, reg_node);
+	if (rail->vts_cdev && d->therm_dvfs) {
+		num_ranges = rail->vts_number_of_trips + 1;
+		trips = rail->vts_trips_table;
+	}
+
+	seq_printf(s, "%-11s", "T(C)\\F(kHz)");
+	for (i = 0; i < d->num_freqs; i++) {
+		unsigned int f = d->freqs[i]/1000;
+		seq_printf(s, " %7u", f);
+	}
+	seq_printf(s, "\n");
+
+	for (j = 0; j < num_ranges; j++) {
+		seq_printf(s, "%s", j == rail->therm_scale_idx ? ">" : " ");
+
+		if (!trips || (num_ranges == 1))
+			seq_printf(s, "%4s..%-4s", "", "");
+		else if (j == 0)
+			seq_printf(s, "%4s..%-4d", "", trips[j]);
+		else if (j == num_ranges - 1)
+			seq_printf(s, "%4d..%-4s", trips[j], "");
+		else
+			seq_printf(s, "%4d..%-4d", trips[j-1], trips[j]);
+
+		for (i = 0; i < d->num_freqs; i++) {
+			int mv = *(d->millivolts + j * MAX_DVFS_FREQS + i);
+			seq_printf(s, " %7d", mv);
+			max_mv[i] = max(max_mv[i], mv);
+		}
+		seq_printf(s, " mV\n");
+	}
+
+	seq_printf(s, "%3s%-8s\n", "", "------");
+	seq_printf(s, "%3s%-8s", "", "max(T)");
+	for (i = 0; i < d->num_freqs; i++)
+		seq_printf(s, " %7d", max_mv[i]);
+	seq_printf(s, " mV\n");
+
+	mutex_unlock(&dvfs_lock);
+
+	return 0;
+}
+
+static int gpu_dvfs_t_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, gpu_dvfs_t_show, NULL);
+}
+
+static const struct file_operations gpu_dvfs_t_fops = {
+	.open           = gpu_dvfs_t_open,
+	.read           = seq_read,
+	.llseek         = seq_lseek,
+	.release        = single_release,
+};
+
+static int dvfs_offset_get(void *data, u64 *val)
+{
+	struct dvfs_rail *rail = data;
+
+	*val = rail->dbg_mv_offs;
+
+	return 0;
+}
+
+static int dvfs_offset_set(void *data, u64 val)
+{
+	struct dvfs_rail *rail = data;
+
+	mutex_lock(&dvfs_lock);
+
+	rail->dbg_mv_offs = val;
+	dvfs_rail_update(rail);
+
+	mutex_unlock(&dvfs_lock);
+
+	return 0;
+}
+DEFINE_SIMPLE_ATTRIBUTE(dvfs_offset_fops, dvfs_offset_get, dvfs_offset_set, "%lld\n");
+
+static int dvfs_override_get(void *data, u64 *val)
+{
+	struct dvfs_rail *rail = data;
+
+	*val = rail->override_millivolts;
+
+	return 0;
+}
+
+static int dvfs_override_set(void *data, u64 val)
+{
+	struct dvfs_rail *rail = data;
+
+	mutex_lock(&dvfs_lock);
+
+	rail->override_millivolts = val;
+	dvfs_rail_update(rail);
+
+	mutex_unlock(&dvfs_lock);
+
+	return 0;
+}
+DEFINE_SIMPLE_ATTRIBUTE(dvfs_override_fops,
+			dvfs_override_get, dvfs_override_set, "%lld\n");
+
+static int dvfs_debugfs_init(void)
+{
+	struct dentry *d_root, *d;
+
+	d_root = debugfs_create_dir("tegra_dvfs", NULL);
+	if (!d_root)
+		return -ENOMEM;
+
+	d = debugfs_create_file("dvfs", S_IRUGO, d_root, NULL,
+		&dvfs_tree_fops);
+	if (!d)
+		return -ENOMEM;
+
+	d = debugfs_create_file("dvfs_table", S_IRUGO, d_root, NULL,
+		&dvfs_table_fops);
+	if (!d)
+		return -ENOMEM;
+
+	d = debugfs_create_file("rails", S_IRUGO, d_root, NULL,
+		&rail_stats_fops);
+	if (!d)
+		return -ENOMEM;
+
+	d = debugfs_create_file("gpu_dvfs_t", S_IRUGO, d_root, NULL,
+				&gpu_dvfs_t_fops);
+	if (!d)
+		return -ENOMEM;
+
+	d = debugfs_create_file("vdd_core_offs", S_IRUGO | S_IWUSR, d_root,
+				tegra_core_rail,  &dvfs_offset_fops);
+	if (!d)
+		return -ENOMEM;
+
+	d = debugfs_create_file("vdd_gpu_offs", S_IRUGO | S_IWUSR, d_root,
+				tegra_gpu_rail,  &dvfs_offset_fops);
+	if (!d)
+		return -ENOMEM;
+
+	d = debugfs_create_file("vdd_core_override", S_IRUGO | S_IWUSR, d_root,
+				tegra_core_rail,  &dvfs_override_fops);
+	if (!d)
+		return -ENOMEM;
+
+	d = debugfs_create_file("vdd_gpu_override", S_IRUGO | S_IWUSR, d_root,
+				tegra_gpu_rail,  &dvfs_override_fops);
+	if (!d)
+		return -ENOMEM;
+
+	return 0;
+}
+
+#endif
+
+typedef int (*dvfs_init_cb_t)(struct device *);
+
+static const struct of_device_id tegra_dvfs_of_match[] = {
+	{ .compatible = "nvidia,tegra124-dvfs", .data = tegra124_init_dvfs },
+	{ .compatible = "nvidia,tegra210-dvfs", .data = tegra210_init_dvfs },
+	{ .compatible = "nvidia,tegra210b01-dvfs",
+	  .data = tegra210b01_init_dvfs },
+	{},
+};
+
+static int tegra_dvfs_probe(struct platform_device *pdev)
+{
+	const struct of_device_id *match;
+	dvfs_init_cb_t dvfs_init_cb;
+	struct dvfs_rail *rail;
+	int ret = -EINVAL;
+
+	match = of_match_node(tegra_dvfs_of_match, pdev->dev.of_node);
+	if (!match)
+		goto out;
+
+	dvfs_init_cb = (dvfs_init_cb_t)match->data;
+	ret = dvfs_init_cb(&pdev->dev);
+	if (ret)
+		goto out;
+
+	ret = tegra_dvfs_regulator_init(&pdev->dev);
+	if (ret)
+		goto out;
+
+	list_for_each_entry(rail, &dvfs_rail_list, node) {
+		rail->is_ready = true;
+		if (rail->vts_of_node) {
+			char *name;
+
+			name = kasprintf(GFP_KERNEL, "%s-vts", rail->reg_id);
+			rail->vts_cdev = thermal_of_cooling_device_register(
+				rail->vts_of_node, name, rail,
+				&tegra_vts_cooling_ops);
+			pr_info("tegra_dvfs: %s: %sregistered\n", name,
+				IS_ERR_OR_NULL(rail->vts_cdev) ? "not " : "");
+			kfree(name);
+		}
+	}
+
+#ifdef CONFIG_DEBUG_FS
+	dvfs_debugfs_init();
+#endif
+	return 0;
+out:
+	return ret;
+}
+
+static int tegra_dvfs_remove(struct platform_device *pdev)
+{
+	struct dvfs *d;
+
+	core_dvfs_started = false;
+
+	unregister_pm_notifier(&tegra_dvfs_reboot_nb);
+	unregister_pm_notifier(&tegra_dvfs_pm_nb);
+
+	list_for_each_entry(d, &tegra_core_rail->dvfs, reg_node) {
+		clk_notifier_unregister(d->clk, &tegra_dvfs_nb);
+	}
+
+	return 0;
+}
+
+static struct platform_driver tegra_dvfs_platdrv = {
+	.driver = {
+		.name	= "tegra-dvfs",
+		.owner	= THIS_MODULE,
+		.of_match_table = tegra_dvfs_of_match,
+	},
+	.probe		= tegra_dvfs_probe,
+	.remove		= tegra_dvfs_remove,
+};
+
+static int __init tegra_dvfs_platdrv_init(void)
+{
+	return platform_driver_register(&tegra_dvfs_platdrv);
+}
+subsys_initcall_sync(tegra_dvfs_platdrv_init);
+
+static void __exit tegra_dvfs_platdrv_exit(void)
+{
+	platform_driver_unregister(&tegra_dvfs_platdrv);
+}
+module_exit(tegra_dvfs_platdrv_exit);
diff --git a/drivers/soc/tegra/tegra210-dvfs.c b/drivers/soc/tegra/tegra210-dvfs.c
new file mode 100644
index 000000000000..1f3d664c53e0
--- /dev/null
+++ b/drivers/soc/tegra/tegra210-dvfs.c
@@ -0,0 +1,2358 @@
+/*
+ * Copyright (c) 2014-2019, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/clk.h>
+#include <linux/clk-provider.h>
+#include <linux/of_address.h>
+#include <linux/thermal.h>
+#include <linux/regulator/consumer.h>
+
+#include <soc/tegra/cvb.h>
+#include <soc/tegra/tegra-dfll.h>
+#include <soc/tegra/tegra-dvfs.h>
+#include <soc/tegra/fuse.h>
+#include <soc/tegra/tegra_emc.h>
+
+#include <dt-bindings/thermal/tegra210b01-trips.h>
+
+#define KHZ		1000
+#define MHZ		1000000
+#define VDD_SAFE_STEP	100
+
+/* Margin % applied to PLL CVB tables */
+#define CVB_PLL_MARGIN	30
+
+#define VDD_CPU_INDEX	0
+#define VDD_CORE_INDEX	1
+#define VDD_GPU_INDEX	2
+
+struct tegra_dvfs_data {
+	struct dvfs_rail **rails;
+	int rails_num;
+	struct cpu_dvfs *cpu_fv_table;
+	int cpu_fv_table_size;
+	struct cvb_dvfs *gpu_cvb_table;
+	int gpu_cvb_table_size;
+	struct dvb_dvfs *emc_dvb_table;
+	int emc_dvb_table_size;
+
+	const int *core_mv;
+	struct dvfs *core_vf_table;
+	int core_vf_table_size;
+	struct dvfs *spi_vf_table;
+	struct dvfs *spi_slave_vf_table;
+	struct dvfs *qspi_sdr_vf_table;
+	struct dvfs *qspi_ddr_vf_table;
+	struct dvfs *sor1_dp_vf_table;
+	int sor1_dp_vf_table_size;
+	int (*get_core_min_mv)(void);
+	int (*get_core_max_mv)(void);
+
+	struct dvfs_therm_limits *core_floors;
+	struct dvfs_therm_limits *core_caps;
+	struct dvfs_therm_limits *core_caps_ucm2;
+	const char *core_dvfs_ver;
+};
+
+static struct tegra_dvfs_data *dvfs_data;
+
+static bool tegra_dvfs_cpu_disabled;
+static bool tegra_dvfs_core_disabled;
+static bool tegra_dvfs_gpu_disabled;
+static int cpu_millivolts[MAX_DVFS_FREQS];
+static int cpu_dfll_millivolts[MAX_DVFS_FREQS];
+static int cpu_lp_millivolts[MAX_DVFS_FREQS];
+
+static struct dvfs_therm_limits
+tegra210_core_therm_floors[MAX_THERMAL_LIMITS] = {
+	{15, 950},
+	{0, 0},
+};
+
+static struct dvfs_therm_limits
+tegra210_core_therm_caps[MAX_THERMAL_LIMITS] = {
+	{86, 1132},
+	{0, 0},
+};
+
+static struct dvfs_therm_limits
+tegra210_core_therm_caps_ucm2[MAX_THERMAL_LIMITS] = {
+	{86, 1090},
+	{0, 0},
+};
+
+static struct dvfs_therm_limits
+tegra210b01_core_therm_floors[MAX_THERMAL_LIMITS] = {
+	{TEGRA210B01_SOC_THERMAL_FLOOR_0 / 1000, 800},
+	{0, 0},
+};
+
+static struct dvfs_therm_limits
+tegra210b01_core_therm_caps[MAX_THERMAL_LIMITS] = {
+	{TEGRA210B01_SOC_THERMAL_CAP_0 / 1000, 1010},
+	{0, 0},
+};
+
+static struct dvfs_therm_limits
+tegra210b01_core_therm_caps_ucm2[MAX_THERMAL_LIMITS] = {
+	{TEGRA210B01_SOC_THERMAL_CAP_0 / 1000, 1010},
+	{0, 0},
+};
+
+
+static struct dvfs_rail tegra210_dvfs_rail_vdd_cpu = {
+	.reg_id = "vdd-cpu",
+	.max_millivolts = 1300,
+	.min_millivolts = 800,
+	.step = VDD_SAFE_STEP,
+	.step_up = 1300,
+	.jmp_to_zero = true,
+	.dfll_mode = true,
+	.alignment = {
+		.step_uv = 6250, /* 6.25mV */
+	},
+	.stats = {
+		.bin_uv = 6250, /* 6.25mV */
+	},
+	.is_ready = false,
+};
+
+static struct dvfs_rail tegra210_dvfs_rail_vdd_core = {
+	.reg_id = "vdd-core",
+	.max_millivolts = 1300,
+	.step = VDD_SAFE_STEP,
+	.step_up = 1300,
+	.alignment = {
+		.step_uv = 12500, /* 12.5mV */
+	},
+	.stats = {
+		.bin_uv = 12500, /* 12.5mV */
+	},
+	.is_ready = false,
+};
+
+static struct dvfs_rail tegra210_dvfs_rail_vdd_gpu = {
+	.reg_id = "vdd-gpu",
+	.max_millivolts = 1300,
+	.step = VDD_SAFE_STEP,
+	.step_up = 1300,
+	.alignment = {
+		.step_uv = 10000, /* 10mV */
+	},
+	.stats = {
+		.bin_uv = 10000, /* 10mV */
+	},
+	.in_band_pm = true,
+};
+
+static struct dvfs_rail *tegra210_dvfs_rails[] = {
+	[VDD_CPU_INDEX] = &tegra210_dvfs_rail_vdd_cpu,
+	[VDD_CORE_INDEX] = &tegra210_dvfs_rail_vdd_core,
+	[VDD_GPU_INDEX] = &tegra210_dvfs_rail_vdd_gpu,
+};
+
+static struct dvfs_rail tegra210b01_dvfs_rail_vdd_core = {
+	.reg_id = "vdd-core",
+	.max_millivolts = 1300,
+	.step = VDD_SAFE_STEP,
+	.step_up = 1300,
+	.alignment = {
+		.step_uv = 12500, /* 12.5mV */
+	},
+	.stats = {
+		.bin_uv = 12500, /* 12.5mV */
+	},
+};
+
+static struct dvfs_rail tegra210b01_dvfs_rail_vdd_cpu = {
+	.reg_id = "vdd-cpu",
+	.max_millivolts = 1125,
+	.step = VDD_SAFE_STEP,
+	.step_up = 1125,
+	.jmp_to_zero = true,
+	.dfll_mode = true,
+	.alignment = {
+		.step_uv = 5000, /* 5.0mV */
+	},
+	.stats = {
+		.bin_uv = 5000, /* 5.0mV */
+	},
+};
+
+static struct dvfs_rail tegra210b01_dvfs_rail_vdd_gpu = {
+	.reg_id = "vdd-gpu",
+	.max_millivolts = 1125,
+	.step = VDD_SAFE_STEP,
+	.step_up = 1125,
+	.alignment = {
+		.step_uv = 5000, /* 5mV */
+	},
+	.stats = {
+		.bin_uv = 5000, /* 10mV */
+	},
+	.vts_floors_table =		/* applied if no vts cdev */
+		{ {TEGRA210B01_GPU_DVFS_THERMAL_MIN / 1000, 800 }, },
+	.in_band_pm = true,
+};
+
+static struct dvfs_rail *tegra210b01_dvfs_rails[] = {
+	[VDD_CPU_INDEX] = &tegra210b01_dvfs_rail_vdd_cpu,
+	[VDD_CORE_INDEX] = &tegra210b01_dvfs_rail_vdd_core,
+	[VDD_GPU_INDEX] = &tegra210b01_dvfs_rail_vdd_gpu,
+};
+
+static struct dvfs_rail vdd_cpu_rail;
+static struct dvfs_rail vdd_gpu_rail;
+static struct dvfs_rail vdd_core_rail;
+
+static struct dvfs_rail *vdd_dvfs_rails[] = {
+	[VDD_CPU_INDEX] = &vdd_cpu_rail,
+	[VDD_CORE_INDEX] = &vdd_core_rail,
+	[VDD_GPU_INDEX] = &vdd_gpu_rail,
+};
+
+static struct dvfs cpu_dvfs = {
+	.clk_name	= "cclk_g",
+	.millivolts	= cpu_millivolts,
+	.dfll_millivolts = cpu_dfll_millivolts,
+	.auto_dvfs	= true,
+	.dvfs_rail	= &vdd_cpu_rail,
+};
+
+/* CPU DVFS tables */
+#define CPU_PLL_CVB_TABLE \
+	.pll_min_millivolts = 950, \
+	.speedo_scale = 100,	\
+	.voltage_scale = 1000,	\
+	.cvb_pll_table = {		\
+		{204000000UL,	{        0,        0,        0 } }, \
+		{306000000UL,	{        0,        0,        0 } }, \
+		{408000000UL,	{        0,        0,        0 } }, \
+		{510000000UL,	{        0,        0,        0 } }, \
+		{612000000UL,	{        0,        0,        0 } }, \
+		{714000000UL,	{        0,        0,        0 } }, \
+		{816000000UL,	{        0,        0,        0 } }, \
+		{918000000UL,	{        0,        0,        0 } }, \
+		{1020000000UL,	{ -2875621,   358099,    -8585 } }, \
+		{1122000000UL,	{   -52225,   104159,    -2816 } }, \
+		{1224000000UL,	{  1076868,     8356,     -727 } }, \
+		{1326000000UL,	{  2208191,   -84659,     1240 } }, \
+		{1428000000UL,	{  2519460,  -105063,     1611 } }, \
+		{1530000000UL,	{  2639809,  -108729,     1626 } }, \
+		{1632000000UL,	{  2889664,  -122173,     1834 } }, \
+		{1734000000UL,	{  3386160,  -154021,     2393 } }, \
+		{1836000000UL,	{  5100873,  -279186,     4747 } }, \
+		{1912500000UL,	{  5100873,  -279186,     4747 } }, \
+		{2014500000UL,	{  5100873,  -279186,     4747 } }, \
+		{2218500000UL,	{  5100873,  -279186,     4747 } }, \
+		{0,           	{ } }, \
+	}
+
+#define CPU_PLL_CVB_TABLE_XA \
+	.pll_min_millivolts = 950, \
+	.speedo_scale = 100,	\
+	.voltage_scale = 1000,	\
+	.cvb_pll_table = {		\
+		{204000000UL,	{        0,        0,        0 } }, \
+		{306000000UL,	{        0,        0,        0 } }, \
+		{408000000UL,	{        0,        0,        0 } }, \
+		{510000000UL,	{        0,        0,        0 } }, \
+		{612000000UL,	{        0,        0,        0 } }, \
+		{714000000UL,	{        0,        0,        0 } }, \
+		{816000000UL,	{        0,        0,        0 } }, \
+		{918000000UL,	{        0,        0,        0 } }, \
+		{1020000000UL,	{ -2875621,   358099,    -8585 } }, \
+		{1122000000UL,	{   -52225,   104159,    -2816 } }, \
+		{1224000000UL,	{  1076868,     8356,     -727 } }, \
+		{1326000000UL,	{  2208191,   -84659,     1240 } }, \
+		{1428000000UL,	{  2519460,  -105063,     1611 } }, \
+		{1530000000UL,	{  2639809,  -108729,     1626 } }, \
+		{1606500000UL,	{  2889664,  -122173,     1834 } }, \
+		{1632000000UL,	{  3386160,  -154021,     2393 } }, \
+		{0,           	{      0,      0,   0} }, \
+	}
+
+#define CPU_PLL_CVB_TABLE_EUCM1 \
+	.pll_min_millivolts = 950, \
+	.speedo_scale = 100,	\
+	.voltage_scale = 1000,	\
+	.cvb_pll_table = {		\
+		{204000000UL,	{        0,        0,        0 } }, \
+		{306000000UL,	{        0,        0,        0 } }, \
+		{408000000UL,	{        0,        0,        0 } }, \
+		{510000000UL,	{        0,        0,        0 } }, \
+		{612000000UL,	{        0,        0,        0 } }, \
+		{714000000UL,	{        0,        0,        0 } }, \
+		{816000000UL,	{        0,        0,        0 } }, \
+		{918000000UL,	{        0,        0,        0 } }, \
+		{1020000000UL,	{ -2875621,   358099,    -8585 } }, \
+		{1122000000UL,	{   -52225,   104159,    -2816 } }, \
+		{1224000000UL,	{  1076868,     8356,     -727 } }, \
+		{1326000000UL,	{  2208191,   -84659,     1240 } }, \
+		{1428000000UL,	{  2519460,  -105063,     1611 } }, \
+		{1555500000UL,	{  2639809,  -108729,     1626 } }, \
+		{1632000000UL,	{  2889664,  -122173,     1834 } }, \
+		{1734000000UL,	{  3386160,  -154021,     2393 } }, \
+		{0,           	{ } }, \
+	}
+
+#define CPU_PLL_CVB_TABLE_EUCM2 \
+	.pll_min_millivolts = 950, \
+	.speedo_scale = 100,	\
+	.voltage_scale = 1000,	\
+	.cvb_pll_table = {		\
+		{204000000UL,	{        0,        0,        0 } }, \
+		{306000000UL,	{        0,        0,        0 } }, \
+		{408000000UL,	{        0,        0,        0 } }, \
+		{510000000UL,	{        0,        0,        0 } }, \
+		{612000000UL,	{        0,        0,        0 } }, \
+		{714000000UL,	{        0,        0,        0 } }, \
+		{816000000UL,	{        0,        0,        0 } }, \
+		{918000000UL,	{        0,        0,        0 } }, \
+		{1020000000UL,	{ -2875621,   358099,    -8585 } }, \
+		{1122000000UL,	{   -52225,   104159,    -2816 } }, \
+		{1224000000UL,	{  1076868,     8356,     -727 } }, \
+		{1326000000UL,	{  2208191,   -84659,     1240 } }, \
+		{1479000000UL,	{  2519460,  -105063,     1611 } }, \
+		{1555500000UL,	{  2639809,  -108729,     1626 } }, \
+		{1683000000UL,	{  2889664,  -122173,     1834 } }, \
+		{0,           	{ } }, \
+	}
+
+#define CPU_PLL_CVB_TABLE_EUCM2_JOINT_RAIL \
+	.pll_min_millivolts = 950, \
+	.speedo_scale = 100,	\
+	.voltage_scale = 1000,	\
+	.cvb_pll_table = {		\
+		{204000000UL,	{        0,        0,        0 } }, \
+		{306000000UL,	{        0,        0,        0 } }, \
+		{408000000UL,	{        0,        0,        0 } }, \
+		{510000000UL,	{        0,        0,        0 } }, \
+		{612000000UL,	{        0,        0,        0 } }, \
+		{714000000UL,	{        0,        0,        0 } }, \
+		{816000000UL,	{        0,        0,        0 } }, \
+		{918000000UL,	{        0,        0,        0 } }, \
+		{1020000000UL,	{ -2875621,   358099,    -8585 } }, \
+		{1122000000UL,	{   -52225,   104159,    -2816 } }, \
+		{1224000000UL,	{  1076868,     8356,     -727 } }, \
+		{1326000000UL,	{  2208191,   -84659,     1240 } }, \
+		{1479000000UL,	{  2519460,  -105063,     1611 } }, \
+		{1504500000UL,	{  2639809,  -108729,     1626 } }, \
+		{0,           	{ } }, \
+	}
+
+#define CPU_PLL_CVB_TABLE_ODN \
+	.pll_min_millivolts = 950, \
+	.speedo_scale = 100,	\
+	.voltage_scale = 1000,	\
+	.cvb_pll_table = {		\
+		{204000000UL,	{        0,        0,        0 } }, \
+		{306000000UL,	{        0,        0,        0 } }, \
+		{408000000UL,	{        0,        0,        0 } }, \
+		{510000000UL,	{        0,        0,        0 } }, \
+		{612000000UL,	{        0,        0,        0 } }, \
+		{714000000UL,	{        0,        0,        0 } }, \
+		{816000000UL,	{        0,        0,        0 } }, \
+		{918000000UL,	{        0,        0,        0 } }, \
+		{1020000000UL,	{ -2875621,   358099,    -8585 } }, \
+		{1122000000UL,	{   -52225,   104159,    -2816 } }, \
+		{1224000000UL,	{  1076868,     8356,     -727 } }, \
+		{1326000000UL,	{  2208191,   -84659,     1240 } }, \
+		{1428000000UL,	{  2519460,  -105063,     1611 } }, \
+		{1581000000UL,	{  2889664,  -122173,     1834 } }, \
+		{1683000000UL,	{  5100873,  -279186,     4747 } }, \
+		{1785000000UL,	{  5100873,  -279186,     4747 } }, \
+		{0,           	{ } }, \
+	}
+
+static struct cpu_dvfs cpu_fv_dvfs_table[] = {
+	{
+		.speedo_id = 10,
+		.process_id = 0,
+		.min_mv = 840,
+		.max_mv = 1120,
+		CPU_PLL_CVB_TABLE_EUCM2_JOINT_RAIL,
+	},
+	{
+		.speedo_id = 10,
+		.process_id = 1,
+		.min_mv = 840,
+		.max_mv = 1120,
+		CPU_PLL_CVB_TABLE_EUCM2_JOINT_RAIL,
+	},
+	{
+		.speedo_id = 9,
+		.process_id = 0,
+		.min_mv = 900,
+		.max_mv = 1162,
+		CPU_PLL_CVB_TABLE_EUCM2,
+	},
+	{
+		.speedo_id = 9,
+		.process_id = 1,
+		.min_mv = 900,
+		.max_mv = 1162,
+		CPU_PLL_CVB_TABLE_EUCM2,
+	},
+	{
+		.speedo_id = 8,
+		.process_id = 0,
+		.min_mv = 900,
+		.max_mv = 1195,
+		CPU_PLL_CVB_TABLE_EUCM2,
+	},
+	{
+		.speedo_id = 8,
+		.process_id = 1,
+		.min_mv = 900,
+		.max_mv = 1195,
+		CPU_PLL_CVB_TABLE_EUCM2,
+	},
+	{
+		.speedo_id = 7,
+		.process_id = 0,
+		.min_mv = 841,
+		.max_mv = 1227,
+		CPU_PLL_CVB_TABLE_EUCM1,
+	},
+	{
+		.speedo_id = 7,
+		.process_id = 1,
+		.min_mv = 841,
+		.max_mv = 1227,
+		CPU_PLL_CVB_TABLE_EUCM1,
+	},
+	{
+		.speedo_id = 6,
+		.process_id = 0,
+		.min_mv = 870,
+		.max_mv = 1150,
+		CPU_PLL_CVB_TABLE,
+	},
+	{
+		.speedo_id = 6,
+		.process_id = 1,
+		.min_mv = 870,
+		.max_mv = 1150,
+		CPU_PLL_CVB_TABLE,
+	},
+	{
+		.speedo_id = 5,
+		.process_id = 0,
+		.min_mv = 818,
+		.max_mv = 1227,
+		CPU_PLL_CVB_TABLE,
+	},
+	{
+		.speedo_id = 5,
+		.process_id = 1,
+		.min_mv = 818,
+		.max_mv = 1227,
+		CPU_PLL_CVB_TABLE,
+	},
+	{
+		.speedo_id = 4,
+		.process_id = -1,
+		.min_mv = 918,
+		.max_mv = 1113,
+		CPU_PLL_CVB_TABLE_XA,
+	},
+	{
+		.speedo_id = 3,
+		.process_id = 0,
+		.min_mv = 825,
+		.max_mv = 1227,
+		CPU_PLL_CVB_TABLE_ODN,
+	},
+	{
+		.speedo_id = 3,
+		.process_id = 1,
+		.min_mv = 825,
+		.max_mv = 1227,
+		CPU_PLL_CVB_TABLE_ODN,
+	},
+	{
+		.speedo_id = 2,
+		.process_id = 0,
+		.min_mv = 870,
+		.max_mv = 1227,
+		CPU_PLL_CVB_TABLE,
+	},
+	{
+		.speedo_id = 2,
+		.process_id = 1,
+		.min_mv = 870,
+		.max_mv = 1227,
+		CPU_PLL_CVB_TABLE,
+	},
+	{
+		.speedo_id = 1,
+		.process_id = 0,
+		.min_mv = 837,
+		.max_mv = 1227,
+		CPU_PLL_CVB_TABLE,
+	},
+	{
+		.speedo_id = 1,
+		.process_id = 1,
+		.min_mv = 837,
+		.max_mv = 1227,
+		CPU_PLL_CVB_TABLE,
+	},
+	{
+		.speedo_id = 0,
+		.process_id = 0,
+		.min_mv = 850,
+		.max_mv = 1170,
+		CPU_PLL_CVB_TABLE,
+	},
+	{
+		.speedo_id = 0,
+		.process_id = 1,
+		.min_mv = 850,
+		.max_mv = 1170,
+		CPU_PLL_CVB_TABLE,
+	},
+};
+
+#define CPUB01_PLL_CVB_TABLE_SLT	\
+	.speedo_scale = 100,	\
+	.voltage_scale = 1000,	\
+	.cvb_pll_table = {	\
+		/* f	                c0,       c1,       c2 */   \
+		{  204000000UL, {        0,        0,        0 } }, \
+		{  306000000UL, {        0,        0,        0 } }, \
+		{  408000000UL, {        0,        0,        0 } }, \
+		{  510000000UL, {        0,        0,        0 } }, \
+		{  612000000UL, {        0,        0,        0 } }, \
+		{  714000000UL, {        0,        0,        0 } }, \
+		{  816000000UL, {        0,        0,        0 } }, \
+		{  918000000UL, {        0,        0,        0 } }, \
+		{ 1020000000UL, {  1120000,        0,        0 } }, \
+		{ 1122000000UL, {  1120000,        0,        0 } }, \
+		{ 1224000000UL, {  1120000,        0,        0 } }, \
+		{ 1326000000UL, {  1120000,        0,        0 } }, \
+		{ 1428000000UL, {  1120000,        0,        0 } }, \
+		{ 1581000000UL, {  1120000,        0,        0 } }, \
+		{ 1683000000UL, {  1120000,        0,        0 } }, \
+		{ 1785000000UL, {  1120000,        0,        0 } }, \
+		{ 1887000000UL, {  1120000,        0,        0 } }, \
+		{ 1963500000UL, {  1120000,        0,        0 } }, \
+		{ 2091000000UL, {  1120000,        0,        0 } }, \
+		{ 0,	        { } }, \
+	}, \
+	.pll_min_millivolts = 800
+
+#define CPUB01_PLL_CVB_TABLE	\
+	.speedo_scale = 100,	\
+	.voltage_scale = 1000,	\
+	.cvb_pll_table = {	\
+		/* f	                c0,       c1,       c2 */   \
+		{  204000000UL, {        0,        0,        0 } }, \
+		{  306000000UL, {        0,        0,        0 } }, \
+		{  408000000UL, {        0,        0,        0 } }, \
+		{  510000000UL, {        0,        0,        0 } }, \
+		{  612000000UL, {        0,        0,        0 } }, \
+		{  714000000UL, {        0,        0,        0 } }, \
+		{  816000000UL, {        0,        0,        0 } }, \
+		{  918000000UL, {        0,        0,        0 } }, \
+		{ 1020000000UL, {  1120000,        0,        0 } }, \
+		{ 1122000000UL, {  1120000,        0,        0 } }, \
+		{ 1224000000UL, {  1120000,        0,        0 } }, \
+		{ 1326000000UL, {  1120000,        0,        0 } }, \
+		{ 1428000000UL, {  1120000,        0,        0 } }, \
+		{ 1581000000UL, {  1120000,        0,        0 } }, \
+		{ 1683000000UL, {  1120000,        0,        0 } }, \
+		{ 1785000000UL, {  1120000,        0,        0 } }, \
+		{ 1887000000UL, {  1120000,        0,        0 } }, \
+		{ 1963500000UL, {  1120000,        0,        0 } }, \
+		{ 2014500000UL, {  1120000,        0,        0 } }, \
+		{ 0,	        { } }, \
+	}, \
+	.pll_min_millivolts = 800
+
+static struct cpu_dvfs cpub01_fv_dvfs_table[] = {
+	{
+		.speedo_id = 2,
+		.process_id = -1,
+		.max_mv = 1120,
+		CPUB01_PLL_CVB_TABLE_SLT,
+	},
+	{
+		.speedo_id = -1,
+		.process_id = -1,
+		.max_mv = 1120,
+		CPUB01_PLL_CVB_TABLE,
+	},
+};
+
+
+/* CPU LP DVFS tables */
+static unsigned long cpu_lp_max_freq[] = {
+/* speedo_id	0	 1        2        3	    4	    5 */
+		1132800, 1132800, 1132800, 1132800, 940800, 1132800
+};
+
+#define CPU_LP_FV_TABLE		 \
+	.fv_table = {		 \
+		{51000,   850},	 \
+		{102000,  850},	 \
+		{204000,  850},	 \
+		{307200,  850},	 \
+		{403200,  850},	 \
+		{518400,  850},	 \
+		{614400,  868},	 \
+		{710400,  912},	 \
+		{825600,  962},	 \
+		{921600,  1006}, \
+		{1036800, 1062}, \
+		{1132800, 1118}, \
+		{1228800, 1168}, \
+	}
+
+static struct cpu_dvfs cpu_lp_fv_dvfs_table[] = {
+	{
+		.speedo_id = 5,
+		.process_id = -1,
+		.min_mv = 818,
+		.max_mv = 1227,
+		CPU_LP_FV_TABLE,
+	},
+	{
+		.speedo_id = 2,
+		.process_id = -1,
+		.min_mv = 804,
+		.max_mv = 1170,
+		CPU_LP_FV_TABLE,
+	},
+	{
+		.speedo_id = 1,
+		.process_id = -1,
+		.min_mv = 837,
+		.max_mv = 1227,
+		CPU_LP_FV_TABLE,
+	},
+	{
+		.speedo_id = -1,
+		.process_id = -1,
+		.min_mv = 850,
+		.max_mv = 1170,
+		CPU_LP_FV_TABLE,
+	},
+};
+
+static struct dvfs cpu_lp_dvfs = {
+	.clk_name	= "cclk_lp",
+	.millivolts	= cpu_lp_millivolts,
+	.auto_dvfs	= true,
+	.dvfs_rail	= &vdd_cpu_rail,
+	.freqs_mult	= KHZ,
+};
+
+/* GPU DVFS tables */
+#define NA_FREQ_CVB_TABLE	\
+	.freqs_mult = KHZ,	\
+	.speedo_scale = 100,	\
+	.thermal_scale = 10,	\
+	.voltage_scale = 1000,	\
+	.cvb_table = {		\
+		/* f	   dfll pll:    c0,       c1,       c2,       c3,       c4,       c5 */    \
+		{   76800, { }, {   814294,     8144,     -940,      808,   -21583,      226 }, }, \
+		{  153600, { }, {   856185,     8144,     -940,      808,   -21583,      226 }, }, \
+		{  230400, { }, {   898077,     8144,     -940,      808,   -21583,      226 }, }, \
+		{  307200, { }, {   939968,     8144,     -940,      808,   -21583,      226 }, }, \
+		{  384000, { }, {   981860,     8144,     -940,      808,   -21583,      226 }, }, \
+		{  460800, { }, {  1023751,     8144,     -940,      808,   -21583,      226 }, }, \
+		{  537600, { }, {  1065642,     8144,     -940,      808,   -21583,      226 }, }, \
+		{  614400, { }, {  1107534,     8144,     -940,      808,   -21583,      226 }, }, \
+		{  691200, { }, {  1149425,     8144,     -940,      808,   -21583,      226 }, }, \
+		{  768000, { }, {  1191317,     8144,     -940,      808,   -21583,      226 }, }, \
+		{  844800, { }, {  1233208,     8144,     -940,      808,   -21583,      226 }, }, \
+		{  921600, { }, {  1275100,     8144,     -940,      808,   -21583,      226 }, }, \
+		{  998400, { }, {  1316991,     8144,     -940,      808,   -21583,      226 }, }, \
+		{ 0,	   { }, { }, }, \
+	}
+
+#define NA_FREQ_CVB_TABLE_XA	\
+	.freqs_mult = KHZ,	\
+	.speedo_scale = 100,	\
+	.thermal_scale = 10,	\
+	.voltage_scale = 1000,	\
+	.cvb_table = {		\
+		/* f	   dfll pll:    c0,       c1,       c2,       c3,       c4,       c5 */    \
+		{   76800, { }, {  1526811,   -59106,      963,      238,   -11292,      185 }, }, \
+		{  153600, { }, {  1543573,   -57798,      910,      179,    -9918,      191 }, }, \
+		{  230400, { }, {  1567838,   -56991,      869,       60,    -8545,      203 }, }, \
+		{  307200, { }, {  1600241,   -56742,      841,        0,    -7019,      209 }, }, \
+		{  384000, { }, {  1635184,   -56501,      813,        0,    -5493,      221 }, }, \
+		{  460800, { }, {  1672308,   -56300,      787,     -119,    -3662,      226 }, }, \
+		{  537600, { }, {  1712114,   -56093,      759,     -179,    -1526,      238 }, }, \
+		{  614400, { }, {  1756009,   -56048,      737,     -298,      610,      244 }, }, \
+		{  691200, { }, {  1790251,   -54860,      687,     -358,     3204,      238 }, }, \
+		{  768000, { }, {  1783830,   -49449,      532,     -477,     6714,      197 }, }, \
+		{  844800, { }, {  1819706,   -45928,      379,     -358,     7019,       89 }, }, \
+		{ 0,	   { }, { }, }, \
+	}
+
+#define FIXED_FREQ_CVB_TABLE	\
+	.freqs_mult = KHZ,	\
+	.speedo_scale = 100,	\
+	.thermal_scale = 10,	\
+	.voltage_scale = 1000,	\
+	.cvb_table = {		\
+		/* f	   dfll pll:    c0,       c1,       c2 */    \
+		{   76800, { }, {  1786666,   -85625,     1632 }, }, \
+		{  153600, { }, {  1846729,   -87525,     1632 }, }, \
+		{  230400, { }, {  1910480,   -89425,     1632 }, }, \
+		{  307200, { }, {  1977920,   -91325,     1632 }, }, \
+		{  384000, { }, {  2049049,   -93215,     1632 }, }, \
+		{  460800, { }, {  2122872,   -95095,     1632 }, }, \
+		{  537600, { }, {  2201331,   -96985,     1632 }, }, \
+		{  614400, { }, {  2283479,   -98885,     1632 }, }, \
+		{  691200, { }, {  2369315,  -100785,     1632 }, }, \
+		{  768000, { }, {  2458841,  -102685,     1632 }, }, \
+		{  844800, { }, {  2550821,  -104555,     1632 }, }, \
+		{  921600, { }, {  2647676,  -106455,     1632 }, }, \
+		{ 0,	   { }, { }, }, \
+	}
+
+static struct dvfs gpu_dvfs = {
+	.clk_name       = "gbus",
+	.auto_dvfs      = true,
+	.dvfs_rail      = &vdd_gpu_rail,
+};
+
+static struct cvb_dvfs gpu_cvb_dvfs_table[] = {
+	{
+		.speedo_id = 4,
+		.process_id = -1,
+		.pll_min_millivolts = 918,
+		.max_mv = 1113,
+		.max_freq = 844800,
+#ifdef CONFIG_TEGRA_USE_NA_GPCPLL
+		NA_FREQ_CVB_TABLE_XA,
+#else
+		FIXED_FREQ_CVB_TABLE,
+#endif
+	},
+
+	{
+		.speedo_id = 3,
+		.process_id = -1,
+		.pll_min_millivolts = 810,
+		.max_mv = 1150,
+		.max_freq = 921600,
+#ifdef CONFIG_TEGRA_USE_NA_GPCPLL
+		NA_FREQ_CVB_TABLE,
+#else
+		FIXED_FREQ_CVB_TABLE,
+#endif
+	},
+
+	{
+		.speedo_id = 2,
+		.process_id = -1,
+		.pll_min_millivolts = 818,
+		.max_mv = 1150,
+		.max_freq = 998400,
+#ifdef CONFIG_TEGRA_USE_NA_GPCPLL
+		NA_FREQ_CVB_TABLE,
+#else
+		FIXED_FREQ_CVB_TABLE,
+#endif
+	},
+
+	{
+		.speedo_id = 1,
+		.process_id = -1,
+		.pll_min_millivolts = 840,
+		.max_mv = 1150,
+		.max_freq = 998400,
+#ifdef CONFIG_TEGRA_USE_NA_GPCPLL
+		NA_FREQ_CVB_TABLE,
+#else
+		FIXED_FREQ_CVB_TABLE,
+#endif
+	},
+
+	{
+		.speedo_id = 0,
+		.process_id = -1,
+		.pll_min_millivolts = 950,
+#ifdef CONFIG_TEGRA_GPU_DVFS
+		.max_mv = 1150,
+#else
+		.max_mv = 1000,
+#endif
+		.max_freq = 921600,
+		FIXED_FREQ_CVB_TABLE,
+	},
+};
+
+#define GPUB01_NA_CVB_TABLE_SLT	\
+	.freqs_mult = KHZ,	\
+	.speedo_scale = 100,	\
+	.thermal_scale = 10,	\
+	.voltage_scale = 1000,	\
+	.cvb_table = {		\
+		/* f	   dfll pll:    c0,       c1,       c2,       c3,       c4,       c5 */    \
+		{   76800, { }, {   590000,        0,        0,        0,        0,        0 }, }, \
+		{  153600, { }, {   590000,        0,        0,        0,        0,        0 }, }, \
+		{  230400, { }, {   590000,        0,        0,        0,        0,        0 }, }, \
+		{  307200, { }, {   590000,        0,        0,        0,        0,        0 }, }, \
+		{  384000, { }, {   590000,        0,        0,        0,        0,        0 }, }, \
+		{  460800, { }, {   795089,   -11096,     -163,      298,   -10421,      162 }, }, \
+		{  537600, { }, {   795089,   -11096,     -163,      298,   -10421,      162 }, }, \
+		{  614400, { }, {   820606,    -6285,     -452,      238,    -6182,       81 }, }, \
+		{  691200, { }, {   846289,    -4565,     -552,      119,    -3958,       -2 }, }, \
+		{  768000, { }, {   888720,    -5110,     -584,        0,    -2849,       39 }, }, \
+		{  844800, { }, {   936634,    -6089,     -602,      -60,      -99,      -93 }, }, \
+		{  921600, { }, {   982562,    -7373,     -614,     -179,     1797,      -13 }, }, \
+		{  998400, { }, {  1090179,   -14125,     -497,     -179,     3518,        9 }, }, \
+		{ 1075200, { }, {  1155798,   -13465,     -648,        0,     1077,       40 }, }, \
+		{ 1152000, { }, {  1198568,   -10904,     -830,        0,     1469,      110 }, }, \
+		{ 1228800, { }, {  1269988,   -12707,     -859,        0,     3722,      313 }, }, \
+		{ 1267200, { }, {  1308155,   -13694,     -867,        0,     3681,      559 }, }, \
+		{ 0,	   { }, { }, }, \
+	}, \
+	.cvb_vmin = {   0, { }, {   590000,        0,        0 }, }, \
+	.cvb_version = "NAPLL En - p4v2-AggressiveSLT"
+
+#define GPUB01_NA_CVB_TABLE	\
+	.freqs_mult = KHZ,	\
+	.speedo_scale = 100,	\
+	.thermal_scale = 10,	\
+	.voltage_scale = 1000,	\
+	.cvb_table = {		\
+		/* f	   dfll pll:    c0,       c1,       c2,       c3,       c4,       c5 */    \
+		{   76800, { }, {   610000,        0,        0,        0,        0,        0 }, }, \
+		{  153600, { }, {   610000,        0,        0,        0,        0,        0 }, }, \
+		{  230400, { }, {   610000,        0,        0,        0,        0,        0 }, }, \
+		{  307200, { }, {   610000,        0,        0,        0,        0,        0 }, }, \
+		{  384000, { }, {   610000,        0,        0,        0,        0,        0 }, }, \
+		{  460800, { }, {   610000,        0,        0,        0,        0,        0 }, }, \
+		{  537600, { }, {   801688,   -10900,     -163,      298,   -10599,      162 }, }, \
+		{  614400, { }, {   824214,    -5743,     -452,      238,    -6325,       81 }, }, \
+		{  691200, { }, {   848830,    -3903,     -552,      119,    -4030,       -2 }, }, \
+		{  768000, { }, {   891575,    -4409,     -584,        0,    -2849,       39 }, }, \
+		{  844800, { }, {   940071,    -5367,     -602,      -60,      -63,      -93 }, }, \
+		{  921600, { }, {   986765,    -6637,     -614,     -179,     1905,      -13 }, }, \
+		{  998400, { }, {  1098475,   -13529,     -497,     -179,     3626,        9 }, }, \
+		{ 1075200, { }, {  1163644,   -12688,     -648,        0,     1077,       40 }, }, \
+		{ 1152000, { }, {  1204812,    -9908,     -830,        0,     1469,      110 }, }, \
+		{ 1228800, { }, {  1277303,   -11675,     -859,        0,     3722,      313 }, }, \
+		{ 1267200, { }, {  1335531,   -12567,     -867,        0,     3681,      559 }, }, \
+		{ 0,	   { }, { }, }, \
+	}, \
+	.cvb_vmin = {   0, { }, {   610000,        0,        0 }, }, \
+	.cvb_version = "NAPLL En - p4v3"
+
+static struct cvb_dvfs gpub01_cvb_dvfs_table[] = {
+	{
+		.speedo_id = 2,
+		.process_id = -1,
+		.max_mv = 1050,
+		.max_freq = 1267200,
+		GPUB01_NA_CVB_TABLE_SLT,
+	},
+	{
+		.speedo_id = -1,
+		.process_id = -1,
+		.max_mv = 1050,
+		.max_freq = 1267200,
+		GPUB01_NA_CVB_TABLE,
+	},
+};
+
+static int gpu_vmin[MAX_THERMAL_RANGES];
+static int gpu_peak_millivolts[MAX_DVFS_FREQS];
+static int gpu_millivolts[MAX_THERMAL_RANGES][MAX_DVFS_FREQS];
+static struct dvfs_therm_limits vdd_gpu_therm_caps_table[MAX_THERMAL_LIMITS];
+static struct dvfs_therm_limits vdd_gpu_therm_caps_ucm2_table[MAX_THERMAL_LIMITS];
+static unsigned long gpu_cap_rates[MAX_THERMAL_LIMITS];
+static struct clk *vgpu_cap_clk;
+
+/* Core DVFS tables */
+static int core_millivolts[MAX_DVFS_FREQS];
+
+#define CORE_DVFS(_clk_name, _speedo_id, _process_id, _auto, _mult, _freqs...) \
+	{							\
+		.clk_name	= _clk_name,			\
+		.speedo_id	= _speedo_id,			\
+		.process_id	= _process_id,			\
+		.freqs		= {_freqs},			\
+		.freqs_mult	= _mult,			\
+		.millivolts	= core_millivolts,		\
+		.auto_dvfs	= _auto,			\
+		.dvfs_rail	= &vdd_core_rail,	\
+	}
+
+
+/* Include T210 core DVFS tables generated from characterization data */
+#include "tegra210-core-dvfs.c"
+
+/* Include T210b01 core DVFS tables generated from characterization data */
+#include "tegra210b01-core-dvfs.c"
+#include "tegra210b01-slt-core-dvfs.c"
+
+int tegra_dvfs_disable_core_set(const char *arg, const struct kernel_param *kp)
+{
+	int ret;
+
+	ret = param_set_bool(arg, kp);
+	if (ret)
+		return ret;
+
+	if (tegra_dvfs_core_disabled)
+		tegra_dvfs_rail_disable(&vdd_core_rail);
+	else
+		tegra_dvfs_rail_enable(&vdd_core_rail);
+
+	return 0;
+}
+
+int tegra_dvfs_disable_cpu_set(const char *arg, const struct kernel_param *kp)
+{
+	int ret;
+
+	ret = param_set_bool(arg, kp);
+	if (ret)
+		return ret;
+
+	if (tegra_dvfs_cpu_disabled)
+		tegra_dvfs_rail_disable(&vdd_cpu_rail);
+	else
+		tegra_dvfs_rail_enable(&vdd_cpu_rail);
+
+	return 0;
+}
+
+int tegra_dvfs_disable_gpu_set(const char *arg, const struct kernel_param *kp)
+{
+	int ret;
+
+	ret = param_set_bool(arg, kp);
+	if (ret)
+		return ret;
+
+	if (tegra_dvfs_gpu_disabled)
+		tegra_dvfs_rail_disable(&vdd_gpu_rail);
+	else
+		tegra_dvfs_rail_enable(&vdd_gpu_rail);
+
+	return 0;
+}
+
+int tegra_dvfs_disable_get(char *buffer, const struct kernel_param *kp)
+{
+	return param_get_bool(buffer, kp);
+}
+
+static struct kernel_param_ops tegra_dvfs_disable_core_ops = {
+	.set = tegra_dvfs_disable_core_set,
+	.get = tegra_dvfs_disable_get,
+};
+
+static struct kernel_param_ops tegra_dvfs_disable_cpu_ops = {
+	.set = tegra_dvfs_disable_cpu_set,
+	.get = tegra_dvfs_disable_get,
+};
+
+static struct kernel_param_ops tegra_dvfs_disable_gpu_ops = {
+	.set = tegra_dvfs_disable_gpu_set,
+	.get = tegra_dvfs_disable_get,
+};
+
+module_param_cb(disable_core, &tegra_dvfs_disable_core_ops,
+	&tegra_dvfs_core_disabled, 0644);
+module_param_cb(disable_cpu, &tegra_dvfs_disable_cpu_ops,
+	&tegra_dvfs_cpu_disabled, 0644);
+module_param_cb(disable_gpu, &tegra_dvfs_disable_gpu_ops,
+	&tegra_dvfs_gpu_disabled, 0644);
+
+static void init_dvfs_one(struct dvfs *d, int max_freq_index)
+{
+	int ret;
+	struct clk *c = clk_get_sys(d->clk_name, d->clk_name);
+
+	if (IS_ERR(c)) {
+		pr_info("tegra210_dvfs: no clock found for %s\n",
+			d->clk_name);
+		return;
+	}
+
+	d->max_millivolts = d->dvfs_rail->nominal_millivolts;
+	d->num_freqs = max_freq_index + 1;
+
+	ret = tegra_setup_dvfs(c, d);
+	if (ret)
+		pr_err("tegra210_dvfs: failed to enable dvfs on %s\n",
+				__clk_get_name(c));
+}
+
+static bool match_dvfs_one(const char *name, int dvfs_speedo_id,
+			   int dvfs_process_id, int speedo_id, int process_id)
+{
+	if ((dvfs_process_id != -1 && dvfs_process_id != process_id) ||
+		(dvfs_speedo_id != -1 && dvfs_speedo_id != speedo_id)) {
+		pr_debug("tegra210_dvfs: rejected %s speedo %d, process %d\n",
+			 name, dvfs_speedo_id, dvfs_process_id);
+		return false;
+	}
+	return true;
+}
+
+static int set_cpu_dvfs_data(struct cpu_dvfs *d,
+			     struct dvfs *cpu_dvfs, int *max_freq_index)
+{
+	int i, mv, dfll_mv, min_mv, min_dfll_mv, num_freqs;
+	unsigned long fmax_at_vmin = 0;
+	unsigned long fmin_use_dfll = 0;
+	unsigned long *freqs;
+	int *dfll_millivolts;
+	struct rail_alignment *align = tegra_dfll_get_alignment();
+	const char *version = tegra_dfll_get_cvb_version();
+	int speedo = tegra_sku_info.cpu_speedo_value;
+
+	if (align == ERR_PTR(-EPROBE_DEFER))
+		return -EPROBE_DEFER;
+
+	vdd_cpu_rail.nvver = version;
+
+	min_dfll_mv = d->min_mv;
+	if (min_dfll_mv < vdd_cpu_rail.min_millivolts) {
+		pr_debug("tegra210_dvfs: dfll min %dmV below rail min %dmV\n",
+			 min_dfll_mv, vdd_cpu_rail.min_millivolts);
+		min_dfll_mv = vdd_cpu_rail.min_millivolts;
+	}
+	min_dfll_mv = tegra_round_voltage(min_dfll_mv, align, true);
+	d->max_mv = tegra_round_voltage(d->max_mv, align, false);
+
+	min_mv = d->pll_min_millivolts;
+	if (min_mv < vdd_cpu_rail.min_millivolts) {
+		pr_debug("tegra210_dvfs: pll min %dmV below rail min %dmV\n",
+			 min_mv, vdd_cpu_rail.min_millivolts);
+		min_mv = vdd_cpu_rail.min_millivolts;
+	}
+	min_mv = tegra_round_voltage(min_mv, align, true);
+
+	if (tegra_get_cpu_fv_table(&num_freqs, &freqs, &dfll_millivolts))
+		return -EPROBE_DEFER;
+
+	for (i = 0; i < num_freqs; i++) {
+		if (freqs[i] != d->cvb_pll_table[i].freq) {
+			pr_err("Err: DFLL freq ladder does not match PLL's\n");
+			return -EINVAL;
+		}
+
+		/*
+		 * Check maximum frequency at minimum voltage for dfll source;
+		 * round down unless all table entries are above Vmin, then use
+		 * the 1st entry as is.
+		 */
+		dfll_mv = max(dfll_millivolts[i] / 1000, min_dfll_mv);
+		if (dfll_mv > min_dfll_mv) {
+			if (!i)
+				fmax_at_vmin = freqs[i];
+			if (!fmax_at_vmin)
+				fmax_at_vmin = freqs[i - 1];
+		}
+
+		/* Clip maximum frequency at maximum voltage for pll source */
+		mv = tegra_get_cvb_voltage(speedo, d->speedo_scale,
+					   &d->cvb_pll_table[i].coefficients);
+		mv = (100 + CVB_PLL_MARGIN) * mv / 100;
+		mv = tegra_round_cvb_voltage(mv, d->voltage_scale, align);
+		mv = max(mv, min_mv);
+		if ((mv > d->max_mv) && !i) {
+			pr_err("Err: volt of 1st entry is higher than Vmax\n");
+			return -EINVAL;
+		}
+
+		/* Minimum rate with pll source voltage above dfll Vmin */
+		if ((mv >= min_dfll_mv) && !fmin_use_dfll)
+			fmin_use_dfll = freqs[i];
+
+		/* fill in dvfs tables */
+		cpu_dvfs->freqs[i] = freqs[i];
+		cpu_millivolts[i] = mv;
+		cpu_dfll_millivolts[i] = min(dfll_mv, d->max_mv);
+	}
+
+	/*
+	 * In the dfll operating range dfll voltage at any rate should be
+	 * better (below) than pll voltage
+	 */
+	if (!fmin_use_dfll || (fmin_use_dfll > fmax_at_vmin))
+		fmin_use_dfll = fmax_at_vmin;
+
+	/* dvfs tables are successfully populated - fill in the rest */
+	cpu_dvfs->speedo_id = d->speedo_id;
+	cpu_dvfs->process_id = d->process_id;
+	cpu_dvfs->dvfs_rail->nominal_millivolts = min(d->max_mv,
+		max(cpu_millivolts[i - 1], cpu_dfll_millivolts[i - 1]));
+	*max_freq_index = i - 1;
+
+	cpu_dvfs->use_dfll_rate_min = fmin_use_dfll;
+
+	return 0;
+}
+
+/*
+ * Setup slow CPU (a.k.a LP CPU) DVFS table from FV data. Only PLL is used as
+ * a clock source for slow CPU. Its maximum frequency must be reached within
+ * nominal voltage -- FV frequency list is cut off at rate that exceeds either
+ * sku-based maximum limit or requires voltage above nominal. Error when DVFS
+ * table can not be constructed must never happen.
+ *
+ * Final CPU rail nominal voltage is set as maximum of fast and slow CPUs
+ * nominal voltages.
+ */
+static int set_cpu_lp_dvfs_data(unsigned long max_freq, struct cpu_dvfs *d,
+				struct dvfs *cpu_lp_dvfs, int *max_freq_index)
+{
+	int i, mv, min_mv;
+	struct rail_alignment *align = &vdd_cpu_rail.alignment;
+
+	min_mv = d->min_mv;
+	if (min_mv < vdd_cpu_rail.min_millivolts) {
+		pr_debug("tegra210_dvfs: scpu min %dmV below rail min %dmV\n",
+			 min_mv, vdd_cpu_rail.min_millivolts);
+		min_mv = vdd_cpu_rail.min_millivolts;
+	}
+	min_mv = tegra_round_voltage(min_mv, align, true);
+
+	d->max_mv = tegra_round_voltage(d->max_mv, align, false);
+	BUG_ON(d->max_mv > vdd_cpu_rail.max_millivolts);
+	cpu_lp_dvfs->dvfs_rail->nominal_millivolts =
+		max(cpu_lp_dvfs->dvfs_rail->nominal_millivolts, d->max_mv);
+
+	for (i = 0; i < MAX_DVFS_FREQS; i++) {
+		struct cpu_pll_fv_table *t = &d->fv_table[i];
+		if (!t->freq || t->freq > max_freq)
+			break;
+
+		mv = t->volt;
+		mv = max(mv, min_mv);
+		if (mv > d->max_mv) {
+			pr_warn("tegra210_dvfs: %dmV for %s rate %d above limit %dmV\n",
+			     mv, cpu_lp_dvfs->clk_name, t->freq, d->max_mv);
+			break;
+		}
+
+		/* fill in dvfs tables */
+		cpu_lp_dvfs->freqs[i] = t->freq;
+		cpu_lp_millivolts[i] = mv;
+	}
+
+	/* Table must not be empty */
+	if (!i) {
+		pr_err("tegra210_dvfs: invalid cpu lp dvfs table\n");
+		return -ENOENT;
+	}
+
+	/* dvfs tables are successfully populated - fill in the rest */
+	cpu_lp_dvfs->speedo_id = d->speedo_id;
+	cpu_lp_dvfs->process_id = d->process_id;
+	*max_freq_index = i - 1;
+
+	return 0;
+}
+
+int of_tegra_dvfs_init(const struct of_device_id *matches)
+{
+	int ret;
+	struct device_node *np;
+
+	for_each_matching_node(np, matches) {
+		const struct of_device_id *match = of_match_node(matches, np);
+		int (*dvfs_init_cb)(struct device_node *) = match->data;
+
+		ret = dvfs_init_cb(np);
+		if (ret) {
+			pr_err("dt: Failed to read %s data from DT\n",
+			       match->compatible);
+			return ret;
+		}
+	}
+	return 0;
+}
+
+/*
+ * QSPI DVFS tables are different in SDR and DDR modes. Use SDR tables by
+ * default. Check if DDR mode is specified for enabled QSPI device in DT,
+ * and overwrite DVFS table, respectively.
+ */
+
+static struct dvfs *qspi_dvfs;
+
+static int of_update_qspi_dvfs(struct device_node *dn)
+{
+	if (of_device_is_available(dn)) {
+		if (of_get_property(dn, "nvidia,x4-is-ddr", NULL))
+			qspi_dvfs = &dvfs_data->qspi_ddr_vf_table[0];
+	}
+	return 0;
+}
+
+static struct of_device_id tegra210_dvfs_qspi_of_match[] = {
+	{ .compatible = "nvidia,tegra210-qspi", .data = of_update_qspi_dvfs, },
+	{ },
+};
+
+static void init_qspi_dvfs(int soc_speedo_id, int core_process_id,
+				  int core_nominal_mv_index)
+{
+	qspi_dvfs = &dvfs_data->qspi_sdr_vf_table[0];
+
+	of_tegra_dvfs_init(tegra210_dvfs_qspi_of_match);
+
+	if (match_dvfs_one(qspi_dvfs->clk_name, qspi_dvfs->speedo_id,
+		qspi_dvfs->process_id, soc_speedo_id, core_process_id))
+		init_dvfs_one(qspi_dvfs, core_nominal_mv_index);
+}
+
+static const struct of_device_id emcb01_dvb_dvfs_match[] = {
+	{ .compatible = "nvidia,t210b01-emc-dvb-dvfs"},
+	{}
+};
+
+static int update_emc_override_dvb_dvfs(struct tegra_dvfs_data *dvfs_data)
+{
+	struct device_node *node;
+	struct dvb_dvfs *dvbd;
+	const u32 freq_cnt = 6, arr_sz = freq_cnt*4;
+	u32 val[arr_sz];
+	int i, j, k, err;
+
+	node = of_find_matching_node(NULL, emcb01_dvb_dvfs_match);
+	dvbd = NULL;
+
+	if (!node || !dvfs_data)
+		return 0;
+
+	err = of_property_read_u32_array(node,
+			"nvidia,freq-x-bin-min-volt", val,
+			arr_sz);
+
+	if (err || !dvfs_data->emc_dvb_table)
+		return 0;
+
+	for (i = 0; i < dvfs_data->emc_dvb_table_size; i++) {
+		if (dvfs_data->emc_dvb_table[i].speedo_id == -1) {
+			dvbd = &dvfs_data->emc_dvb_table[i];
+			break;
+		}
+	}
+
+	if (!dvbd)
+		return 0;
+
+	for (i = 0; i < MAX_DVFS_FREQS; i++) {
+		for (k = 0; k < freq_cnt; ++k)
+			if (dvbd->dvb_table[i].freq == val[k*4])
+				for (j = 1; j < 4; ++j)
+					dvbd->dvb_table[i].mvolts[j-1] =
+						val[k*4+j];
+	}
+
+	return 0;
+}
+
+
+/*
+ * SPI DVFS tables are different in master and in slave mode. Use master tables
+ * by default. Check if slave mode is specified for enabled SPI devices in DT,
+ * and overwrite master table for the respective SPI controller.
+ */
+
+static struct {
+	u64 address;
+	struct dvfs *d;
+} spi_map[] = {
+	{ 0x7000d400, },
+	{ 0x7000d600, },
+	{ 0x7000d800, },
+	{ 0x7000da00, },
+};
+
+static int of_update_spi_slave_dvfs(struct device_node *dn)
+{
+	int i;
+	u64 addr = 0;
+	const __be32 *reg;
+
+	if (!of_device_is_available(dn))
+		return 0;
+
+	reg = of_get_property(dn, "reg", NULL);
+	if (reg)
+		addr = of_translate_address(dn, reg);
+
+	for (i = 0; i < ARRAY_SIZE(spi_map); i++) {
+		if (spi_map[i].address == addr) {
+			spi_map[i].d = &dvfs_data->spi_slave_vf_table[i];
+			break;
+		}
+	}
+	return 0;
+}
+
+static struct of_device_id tegra21_dvfs_spi_slave_of_match[] = {
+	{ .compatible = "nvidia,tegra210-spi-slave",
+	  .data = of_update_spi_slave_dvfs, },
+	{ },
+};
+
+static void init_spi_dvfs(int soc_speedo_id, int core_process_id,
+			  int core_nominal_mv_index)
+{
+	int i;
+
+	for (i = 0; i <  ARRAY_SIZE(spi_map); i++)
+		spi_map[i].d = &dvfs_data->spi_vf_table[i];
+
+	of_tegra_dvfs_init(tegra21_dvfs_spi_slave_of_match);
+
+	for (i = 0; i <  ARRAY_SIZE(spi_map); i++) {
+		struct dvfs *d = spi_map[i].d;
+		if (!match_dvfs_one(d->clk_name, d->speedo_id,
+			d->process_id, soc_speedo_id, core_process_id))
+			continue;
+		init_dvfs_one(d, core_nominal_mv_index);
+	}
+}
+
+static void init_sor1_dvfs(int soc_speedo_id, int core_process_id,
+			   int core_nominal_mv_index)
+{
+	struct dvfs *sor1_dp_dvfs = &dvfs_data->sor1_dp_vf_table[0];
+	struct clk *c;
+	int i, n = dvfs_data->sor1_dp_vf_table_size;
+
+	c = clk_get_sys(sor1_dp_dvfs->clk_name, sor1_dp_dvfs->clk_name);
+	if (IS_ERR(c)) {
+		pr_debug("init_sor1_dvfs: no clock found for %s\n",
+			sor1_dp_dvfs->clk_name);
+		return;
+	}
+
+	for (i = 0; i < n; i++, sor1_dp_dvfs++) {
+		if (match_dvfs_one(sor1_dp_dvfs->clk_name,
+			sor1_dp_dvfs->speedo_id, sor1_dp_dvfs->process_id,
+			soc_speedo_id, core_process_id)) {
+			tegra_dvfs_add_alt_freqs(c, sor1_dp_dvfs);
+			break;
+		}
+	}
+
+	return;
+
+}
+
+static int get_core_sku_max_mv(void)
+{
+	int speedo_rev = tegra_sku_info.speedo_rev;
+
+	switch (tegra_sku_info.soc_process_id) {
+	case 0:
+		if (tegra_sku_info.soc_speedo_id == 1)
+			return 1100;
+		if (speedo_rev <= 1)
+			return 1000;
+
+		return 1125;
+	case 1:
+		return 1075;
+	case 2:
+		return 1000;
+	default:
+		pr_err("Un-supported Tegra210 speedo %d\n",
+				tegra_sku_info.soc_speedo_id);
+		return -EINVAL;
+	}
+}
+
+static int get_core_sku_min_mv(void)
+{
+	int rev = tegra_sku_info.revision;
+	bool a02 = (rev == TEGRA_REVISION_A02) || (rev == TEGRA_REVISION_A02p);
+
+	if (tegra_sku_info.soc_speedo_id == 1)
+		return 1100;
+
+	switch (tegra_sku_info.sku_id) {
+	case 0x0:
+	case 0x1:
+	case 0x7:
+	case 0x17:
+	case 0x13:
+		if (!a02)
+			return 825;
+		return 800;
+	case 0x87:
+		return 825;
+	case 0x83:
+	case 0x8f:
+		return 800;
+	default:
+		return 950;
+	}
+}
+
+static int get_coreb01_sku_max_mv(void)
+{
+	switch (tegra_sku_info.soc_process_id) {
+	case 0:
+		return 1050;
+	case 1:
+		return 1025;
+	case 2:
+		return 1000;
+	default:
+		pr_err("Un-supported Tegra210b01 process id %d\n",
+				tegra_sku_info.soc_process_id);
+		return -EINVAL;
+	}
+}
+
+static int get_coreb01slt_sku_max_mv(void)
+{
+	return get_coreb01_sku_max_mv();
+}
+
+static int get_coreb01_sku_min_mv(void)
+{
+	return 637;
+}
+
+static int get_coreb01slt_sku_min_mv(void)
+{
+	return 600;
+}
+
+static int get_core_nominal_mv_index(int speedo_id)
+{
+	int i;
+	int mv = dvfs_data->get_core_max_mv();
+
+	if (mv < 0)
+		return mv;
+
+	/* Round nominal level down to the nearest core scaling step */
+	for (i = 0; i < MAX_DVFS_FREQS; i++) {
+		if ((core_millivolts[i] == 0) || (mv < core_millivolts[i]))
+			break;
+	}
+
+	if (i == 0) {
+		pr_err("tegra210-dvfs: failed to get nominal idx at volt %d\n",
+		       mv);
+		return -ENOSYS;
+	}
+
+	return i - 1;
+}
+
+static int get_core_min_mv_index(void)
+{
+	int i;
+	int mv = dvfs_data->get_core_min_mv();
+
+	if (mv < 0)
+		return mv;
+
+	/* Round minimum level up to the nearest core scaling step */
+	for (i = 0; i < MAX_DVFS_FREQS - 1; i++) {
+		if ((core_millivolts[i+1] == 0) || (mv <= core_millivolts[i]))
+			break;
+	}
+
+	return i;
+}
+
+static int init_cpu_dvfs_table(struct cpu_dvfs *fv_dvfs_table,
+			       int table_size, int *cpu_max_freq_index)
+{
+	int i, ret;
+	int cpu_speedo_id = tegra_sku_info.cpu_speedo_id;
+	int cpu_process_id = tegra_sku_info.cpu_process_id;
+
+	for (ret = 0, i = 0; i < table_size; i++) {
+		struct cpu_dvfs *d = &fv_dvfs_table[i];
+
+		if (match_dvfs_one("cpu dvfs", d->speedo_id, d->process_id,
+				   cpu_speedo_id, cpu_process_id)) {
+			ret = set_cpu_dvfs_data(
+				d, &cpu_dvfs, cpu_max_freq_index);
+			if (ret)
+				return ret;
+			break;
+		}
+	}
+	BUG_ON(i == table_size);
+
+	return ret;
+}
+
+static int init_cpu_lp_dvfs_table(int *cpu_lp_max_freq_index)
+{
+	int i, ret;
+	unsigned int cpu_lp_speedo_id = tegra_sku_info.cpu_speedo_id;
+	int cpu_lp_process_id = tegra_sku_info.cpu_process_id;
+	unsigned long max_freq;
+
+	if (cpu_lp_speedo_id >= ARRAY_SIZE(cpu_lp_max_freq))
+		max_freq = cpu_lp_max_freq[0];
+	else
+		max_freq = cpu_lp_max_freq[cpu_lp_speedo_id];
+
+	for (ret = 0, i = 0; i <  ARRAY_SIZE(cpu_lp_fv_dvfs_table); i++) {
+		struct cpu_dvfs *d = &cpu_lp_fv_dvfs_table[i];
+		if (match_dvfs_one("cpu lp dvfs", d->speedo_id, d->process_id,
+				   cpu_lp_speedo_id, cpu_lp_process_id)) {
+			ret = set_cpu_lp_dvfs_data(max_freq,
+				d, &cpu_lp_dvfs, cpu_lp_max_freq_index);
+			break;
+		}
+	}
+
+	return ret;
+}
+
+static void adjust_emc_dvfs_from_timing_table(struct dvfs *d)
+{
+	unsigned long rate;
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(core_millivolts); i++) {
+		if (core_millivolts[i] == 0)
+			return;
+
+		rate = tegra210_predict_emc_rate(core_millivolts[i]);
+		if (IS_ERR_VALUE(rate))
+			return;
+
+		if (rate)
+			d->freqs[i] = rate;
+	}
+}
+
+static unsigned long dvb_predict_rate(
+	int process_id, struct dvb_dvfs *dvbd, int mv)
+{
+	int i;
+	unsigned long rate = 0;
+
+	for (i = 0; i < MAX_DVFS_FREQS; i++) {
+		if (!dvbd->dvb_table[i].freq)
+			break;
+		if (dvbd->dvb_table[i].mvolts[process_id] > mv)
+			break;
+		rate = dvbd->dvb_table[i].freq * dvbd->freqs_mult;
+	}
+
+	return rate;
+}
+
+static void adjust_emc_dvfs_from_dvb_table(
+	int process_id, struct dvb_dvfs *dvbd, struct dvfs *d)
+{
+	unsigned long rate;
+	int i;
+
+	if (process_id > MAX_PROCESS_ID) {
+		WARN(1, "Process id %d above emc dvb table max\n", process_id);
+		return;
+	}
+
+	for (i = 0; i < ARRAY_SIZE(core_millivolts); i++) {
+		if (core_millivolts[i] == 0)
+			return;
+
+		rate = dvb_predict_rate(process_id, dvbd, core_millivolts[i]);
+		if (rate)
+			d->freqs[i] = rate;
+	}
+}
+
+static struct dvb_dvfs *get_emc_dvb_dvfs(int speedo_id)
+{
+	int i;
+
+	if (dvfs_data->emc_dvb_table) {
+		for (i = 0; i < dvfs_data->emc_dvb_table_size; i++) {
+			struct dvb_dvfs *dvbd = &dvfs_data->emc_dvb_table[i];
+
+			if (dvbd->speedo_id == -1 ||
+			    dvbd->speedo_id == speedo_id)
+				return dvbd;
+		}
+	}
+	return NULL;
+}
+
+/*
+ * Find maximum GPU frequency that can be reached at minimum voltage across all
+ * temperature ranges.
+ */
+static unsigned long find_gpu_fmax_at_vmin(
+	struct dvfs *gpu_dvfs, int thermal_ranges, int freqs_num)
+{
+	int i, j;
+	unsigned long fmax = ULONG_MAX;
+
+	/*
+	 * For voltage scaling row in each temperature range, as well as peak
+	 * voltage row find maximum frequency at lowest voltage, and return
+	 * minimax. On Tegra21 all GPU DVFS thermal dependencies are integrated
+	 * into thermal DVFS table (i.e., there is no separate thermal floors
+	 * applied in the rail level). Hence, returned frequency specifies max
+	 * frequency safe at minimum voltage across all temperature ranges.
+	 */
+	for (j = 0; j < thermal_ranges; j++) {
+		for (i = 1; i < freqs_num; i++) {
+			if (gpu_millivolts[j][i] > gpu_millivolts[j][0])
+				break;
+		}
+		fmax = min(fmax, gpu_dvfs->freqs[i - 1]);
+	}
+
+	for (i = 1; i < freqs_num; i++) {
+		if (gpu_peak_millivolts[i] > gpu_peak_millivolts[0])
+			break;
+	}
+	fmax = min(fmax, gpu_dvfs->freqs[i - 1]);
+
+	return fmax;
+}
+
+/*
+ * Determine minimum voltage safe at maximum frequency across all temperature
+ * ranges.
+ */
+static int find_gpu_vmin_at_fmax(
+	struct dvfs *gpu_dvfs, int thermal_ranges, int freqs_num)
+{
+	int j, vmin;
+
+	/*
+	 * For voltage scaling row in each temperature range find minimum
+	 * voltage at maximum frequency and return max Vmin across ranges.
+	 */
+	for (vmin = 0, j = 0; j < thermal_ranges; j++)
+		vmin = max(vmin, gpu_millivolts[j][freqs_num-1]);
+
+	return vmin;
+}
+
+static int of_parse_dvfs_rail_cdev_trips(struct device_node *node,
+		int *therm_trips_table,
+		struct dvfs_therm_limits *therm_limits_table,
+		struct dvfs_therm_limits *therm_limits_ucm2_table,
+		struct rail_alignment *align, bool up)
+{
+	struct of_phandle_iter iter;
+	int cells_num, i = 0, t;
+
+	/* 1 cell per trip-point, if constraint is specified */
+	cells_num = of_property_read_bool(node, "nvidia,constraint-ucm2") ? 2 :
+		of_property_read_bool(node, "nvidia,constraint") ? 1 : 0;
+
+	if (of_phandle_iterator_init(&iter, node, "nvidia,trips",
+				     NULL, cells_num))
+		return -ENOENT;
+
+	while (!of_phandle_iterator_next(&iter)) {
+		struct device_node *trip_dn = iter.node;
+
+		if (i >= MAX_THERMAL_LIMITS) {
+			pr_err("tegra_dvfs: list of scaling cdev trips exceeds max limit\n");
+			return -EINVAL;
+		}
+
+		if (of_property_read_s32(trip_dn, "temperature", &t)) {
+			pr_err("tegra_dvfs: failed to read scalings cdev trip %d\n", i);
+			return -ENODATA;
+		}
+
+		if (therm_trips_table)
+			therm_trips_table[i] = t / 1000; /* convert mC to C */
+
+		if (cells_num && therm_limits_table) {
+			int mv = 0;
+			struct of_phandle_args dvfs_args;
+			dvfs_args.args_count = of_phandle_iterator_args(&iter,
+						dvfs_args.args, MAX_PHANDLE_ARGS);
+
+			mv = dvfs_args.args[0];
+			mv = tegra_round_voltage(mv, align, up);
+			therm_limits_table[i].temperature = t / 1000;
+			therm_limits_table[i].mv = mv;
+			if (cells_num == 2 && therm_limits_ucm2_table) {
+				mv = dvfs_args.args[1];
+				mv = tegra_round_voltage(mv, align, up);
+				therm_limits_ucm2_table[i].temperature = t/1000;
+				therm_limits_ucm2_table[i].mv = mv;
+			}
+		}
+		i++;
+	}
+
+	return i;
+}
+
+static int init_gpu_rail_thermal_scaling(struct device_node *node,
+					 struct dvfs_rail *rail,
+					 struct cvb_dvfs *d)
+{
+	int thermal_ranges;
+	struct device_node *cdev_node;
+
+	cdev_node = of_find_compatible_node(NULL, NULL,
+				"nvidia,tegra210-rail-scaling-cdev");
+	if (!cdev_node || !of_device_is_available(cdev_node))
+		return 1;
+
+	rail->vts_of_node = cdev_node;
+
+	thermal_ranges = of_parse_dvfs_rail_cdev_trips(cdev_node,
+		&rail->vts_trips_table[0], &rail->vts_floors_table[0],
+		NULL, &rail->alignment, true);
+
+	if (thermal_ranges <= 0)
+		return 1;
+
+	rail->vts_number_of_trips = thermal_ranges - 1;
+
+	return thermal_ranges;
+}
+
+/* cooling device to limit GPU frequenct based on the vmax thermal profile */
+#define GPU_MAX_RATE 1300000000UL
+static int gpu_dvfs_rail_get_vmax_cdev_max_state(
+	struct thermal_cooling_device *cdev, unsigned long *max_state)
+{
+	struct dvfs_rail *rail = cdev->devdata;
+
+	*max_state = rail->therm_caps_size;
+
+	return 0;
+}
+
+static int gpu_dvfs_rail_get_vmax_cdev_cur_state(
+	struct thermal_cooling_device *cdev, unsigned long *cur_state)
+{
+	struct dvfs_rail *rail = cdev->devdata;
+
+	*cur_state = rail->therm_cap_idx;
+
+	return 0;
+}
+
+static int gpu_dvfs_rail_set_vmax_cdev_cur_state(
+	struct thermal_cooling_device *cdev, unsigned long cur_state)
+{
+	struct dvfs_rail *rail = cdev->devdata;
+	int level = 0, err = -EINVAL;
+	unsigned long cap_rate = GPU_MAX_RATE;
+
+	if (cur_state)
+		level = rail->therm_caps[cur_state - 1].mv;
+
+	if (level) {
+		if (rail->vts_cdev && gpu_dvfs.therm_dvfs)
+			cap_rate = gpu_cap_rates[cur_state - 1];
+		else
+			cap_rate = tegra_dvfs_predict_hz_at_mv_max_tfloor(
+				gpu_dvfs.clk, level);
+	}
+
+	if (!IS_ERR_VALUE(cap_rate)) {
+		err = clk_set_rate(vgpu_cap_clk, cap_rate);
+		if (err)
+			pr_err("tegra_dvfs: Failed to set GPU cap rate %lu\n",
+			       cap_rate);
+	} else {
+		pr_err("tegra_dvfs: Failed to find GPU cap rate for %dmV\n",
+		       level);
+	}
+
+	rail->therm_cap_idx = cur_state;
+
+	return err;
+}
+
+static struct thermal_cooling_device_ops gpu_dvfs_rail_vmax_cooling_ops = {
+	.get_max_state = gpu_dvfs_rail_get_vmax_cdev_max_state,
+	.get_cur_state = gpu_dvfs_rail_get_vmax_cdev_cur_state,
+	.set_cur_state = gpu_dvfs_rail_set_vmax_cdev_cur_state,
+};
+
+#define CAP_TRIP_ON_SCALING_MARGIN 5
+
+static int init_gpu_rail_thermal_caps(struct device_node *node,
+		struct dvfs *dvfs, struct dvfs_rail *rail, int thermal_ranges,
+		int freqs_num)
+{
+	struct device_node *cdev_node;
+	int num_trips, i, j, k;
+	bool ucm2 = tegra_sku_info.ucm == TEGRA_UCM2;
+
+	if (thermal_ranges <= 1 )
+		return 0;
+
+	cdev_node = of_find_compatible_node(NULL, NULL,
+				"nvidia,tegra210-rail-vmax-cdev");
+	if (!cdev_node || !of_device_is_available(cdev_node))
+		return 0;
+
+	rail->vmax_of_node = cdev_node;
+
+	vgpu_cap_clk = of_clk_get_by_name(cdev_node, "cap-clk");
+	if (IS_ERR(vgpu_cap_clk))
+		return 0;
+
+	num_trips = of_parse_dvfs_rail_cdev_trips(cdev_node,
+		NULL, vdd_gpu_therm_caps_table, vdd_gpu_therm_caps_ucm2_table,
+		&rail->alignment, false);
+	if (num_trips <= 0)
+		return 0;
+
+	rail->therm_caps =
+		ucm2 ? vdd_gpu_therm_caps_ucm2_table : vdd_gpu_therm_caps_table;
+	if (!rail->therm_caps[0].mv) {
+		pr_err("tegra_dvfs: invalid gpu cap table\n");
+		rail->therm_caps = NULL;
+		return 0;
+	}
+	rail->therm_caps_size = num_trips;
+	rail->therm_cap_idx = num_trips;
+
+	for (k = 0; k < num_trips; k++) {
+		int cap_tempr = rail->therm_caps[k].temperature;
+		int cap_level = rail->therm_caps[k].mv;
+		unsigned long cap_freq = GPU_MAX_RATE;
+
+		for (j = 0; j < thermal_ranges; j++) {
+			if ((j < thermal_ranges - 1) &&	/* vts trips=ranges-1 */
+			    (rail->vts_trips_table[j] +
+			    CAP_TRIP_ON_SCALING_MARGIN < cap_tempr))
+				continue;
+
+			for (i = 1; i < freqs_num; i++) {
+				if (gpu_millivolts[j][i] > cap_level)
+					break;
+			}
+			cap_freq = min(cap_freq, dvfs->freqs[i - 1]);
+		}
+		gpu_cap_rates[k] = cap_freq * dvfs->freqs_mult;
+	}
+
+
+	clk_set_rate(vgpu_cap_clk, gpu_cap_rates[num_trips - 1]);
+	rail->vmax_cdev = thermal_of_cooling_device_register(rail->vmax_of_node,
+		"GPU-cap", rail, &gpu_dvfs_rail_vmax_cooling_ops);
+	pr_info("tegra_dvfs: GPU-cap: %sregistered\n",
+		IS_ERR_OR_NULL(rail->vmax_cdev) ? "not " : "");
+
+	return num_trips;
+}
+
+/*
+ * Setup gpu dvfs tables from cvb data, determine nominal voltage for gpu rail,
+ * and gpu maximum frequency. Error when gpu dvfs table can not be constructed
+ * must never happen.
+ */
+static int set_gpu_dvfs_data(struct device_node *node, unsigned long max_freq,
+	struct cvb_dvfs *d, struct dvfs *gpu_dvfs, int *max_freq_index)
+{
+	int i, j, thermal_ranges, mv, min_mv, err;
+	struct cvb_dvfs_table *table = NULL;
+	int speedo = tegra_sku_info.gpu_speedo_value;
+	struct dvfs_rail *rail = &vdd_gpu_rail;
+	struct rail_alignment *align = &rail->alignment;
+
+	rail->nvver = d->cvb_version;
+
+	d->max_mv = tegra_round_voltage(d->max_mv, align, false);
+	min_mv = d->pll_min_millivolts;
+	mv = tegra_get_cvb_voltage(
+		speedo, d->speedo_scale, &d->cvb_vmin.cvb_pll_param);
+	mv = tegra_round_cvb_voltage(mv, d->voltage_scale, align);
+	min_mv = max(min_mv, mv);
+	if (min_mv < rail->min_millivolts) {
+		pr_debug("tegra21_dvfs: gpu min %dmV below rail min %dmV\n",
+			 min_mv, rail->min_millivolts);
+		min_mv = rail->min_millivolts;
+	}
+
+	/*
+	 * Get scaling thermal ranges; 1 range implies no thermal dependency.
+	 * Invalidate scaling cooling device in the latter case.
+	 */
+	thermal_ranges = init_gpu_rail_thermal_scaling(node, rail, d);
+	if (thermal_ranges == 1)
+		rail->vts_cdev = NULL;
+
+	/*
+	 * Apply fixed thermal floor for each temperature range
+	 */
+	for (j = 0; j < thermal_ranges; j++) {
+		mv = max(min_mv, (int)rail->vts_floors_table[j].mv);
+		gpu_vmin[j] = tegra_round_voltage(mv, align, true);
+	}
+
+	/*
+	 * Use CVB table to fill in gpu dvfs frequencies and voltages. Each
+	 * CVB entry specifies gpu frequency and CVB coefficients to calculate
+	 * the respective voltage.
+	 */
+	for (i = 0; i < MAX_DVFS_FREQS; i++) {
+		table = &d->cvb_table[i];
+		if (!table->freq || (table->freq > max_freq))
+			break;
+
+		mv = tegra_get_cvb_voltage(
+			speedo, d->speedo_scale, &table->cvb_pll_param);
+
+		for (j = 0; j < thermal_ranges; j++) {
+			int mvj = mv;
+			int t = 0;
+
+			if (thermal_ranges > 1)
+				t = rail->vts_trips_table[j];
+
+			/* get thermal offset for this trip-point */
+			mvj += tegra_get_cvb_t_voltage(speedo, d->speedo_scale,
+				t, d->thermal_scale, &table->cvb_pll_param);
+			mvj = tegra_round_cvb_voltage(mvj, d->voltage_scale, align);
+
+			/* clip to minimum, abort if above maximum */
+			mvj = max(mvj, gpu_vmin[j]);
+			if (mvj > d->max_mv)
+				break;
+
+			/*
+			 * Update voltage for adjacent ranges bounded by this
+			 * trip-point (cvb & dvfs are transpose matrices, and
+			 * cvb freq row index is column index for dvfs matrix)
+			 */
+			gpu_millivolts[j][i] = mvj;
+			if (j && (gpu_millivolts[j-1][i] < mvj))
+				gpu_millivolts[j-1][i] = mvj;
+		}
+		/* Make sure all voltages for this frequency are below max */
+		if (j < thermal_ranges)
+			break;
+
+		/* fill in gpu dvfs tables */
+		gpu_dvfs->freqs[i] = table->freq;
+	}
+
+	gpu_dvfs->millivolts = &gpu_millivolts[0][0];
+
+	/*
+	 * Table must not be empty, must have at least one entry in range, and
+	 * must specify monotonically increasing voltage on frequency dependency
+	 * in each temperature range.
+	 */
+	err = tegra_dvfs_init_thermal_dvfs_voltages(&gpu_millivolts[0][0],
+			gpu_peak_millivolts, i, thermal_ranges, gpu_dvfs);
+
+	if (err || !i) {
+		pr_err("tegra21_dvfs: invalid gpu dvfs table\n");
+		return -ENOENT;
+	}
+
+	/* Shift out the 1st trip-point */
+	for (j = 1; j < thermal_ranges; j++)
+		rail->vts_trips_table[j - 1] = rail->vts_trips_table[j];
+
+	/* dvfs tables are successfully populated - fill in the gpu dvfs */
+	gpu_dvfs->speedo_id = d->speedo_id;
+	gpu_dvfs->process_id = d->process_id;
+	gpu_dvfs->freqs_mult = d->freqs_mult;
+
+	*max_freq_index = i - 1;
+
+	gpu_dvfs->dvfs_rail->nominal_millivolts = min(d->max_mv,
+		find_gpu_vmin_at_fmax(gpu_dvfs, thermal_ranges, i));
+
+	gpu_dvfs->fmax_at_vmin_safe_t = d->freqs_mult *
+		find_gpu_fmax_at_vmin(gpu_dvfs, thermal_ranges, i);
+
+	/* Initialize thermal capping */
+	init_gpu_rail_thermal_caps(node, gpu_dvfs, rail, thermal_ranges, i);
+
+#ifdef CONFIG_TEGRA_USE_NA_GPCPLL
+	/*
+	 * Set NA DVFS flag, if GPCPLL NA mode is enabled. This is necessary to
+	 * make sure that GPCPLL configuration is updated by tegra core DVFS
+	 * when thermal DVFS cooling device state is changed. Since tegra core
+	 * DVFS does not support NA operations for Vmin cooling device, GPU Vmin
+	 * thermal floors have been integrated with thermal DVFS, and no Vmin
+	 * cooling device is installed.
+	 */
+	if (tegra_sku_info.gpu_speedo_id)
+		gpu_dvfs->na_dvfs = 1;
+#else
+	if (of_device_is_compatible(node, "nvidia,tegra210b01-dvfs"))
+		WARN(1, "TEGRA_USE_NA_GPCPLL must be set on T210b01\n");
+#endif
+	return 0;
+}
+
+static void init_gpu_dvfs_table(struct device_node *node,
+	struct cvb_dvfs *cvb_dvfs_table, int table_size,
+	int *gpu_max_freq_index)
+{
+	int i, ret;
+	int gpu_speedo_id = tegra_sku_info.gpu_speedo_id;
+	int gpu_process_id = tegra_sku_info.gpu_process_id;
+
+	for (ret = 0, i = 0; i < table_size; i++) {
+		struct cvb_dvfs *d = &cvb_dvfs_table[i];
+		unsigned long max_freq = d->max_freq;
+		u32 f;
+
+		if (!of_property_read_u32(node, "nvidia,gpu-max-freq-khz", &f))
+			max_freq = min(max_freq, (unsigned long)f);
+
+		if (match_dvfs_one("gpu cvb", d->speedo_id, d->process_id,
+				   gpu_speedo_id, gpu_process_id)) {
+			ret = set_gpu_dvfs_data(node, max_freq,
+				d, &gpu_dvfs, gpu_max_freq_index);
+			break;
+		}
+	}
+	BUG_ON((i == table_size) || ret);
+}
+
+static void init_core_dvfs_table(int soc_speedo_id, int core_process_id)
+{
+	int core_nominal_mv_index;
+	int core_min_mv_index;
+	int i;
+	static bool initialized;
+
+	if (initialized)
+		return;
+
+	initialized = true;
+
+	/*
+	 * Find nominal voltages for core (1st) and cpu rails before rail
+	 * init. Nominal voltage index in core scaling ladder can also be
+	 * used to determine max dvfs frequencies for all core clocks. In
+	 * case of error disable core scaling and set index to 0, so that
+	 * core clocks would not exceed rates allowed at minimum voltage.
+	 */
+	core_nominal_mv_index = get_core_nominal_mv_index(soc_speedo_id);
+	if (core_nominal_mv_index < 0) {
+		vdd_core_rail.disabled = true;
+		tegra_dvfs_core_disabled = true;
+		core_nominal_mv_index = 0;
+	}
+
+	core_min_mv_index = get_core_min_mv_index();
+	if (core_min_mv_index < 0) {
+		vdd_core_rail.disabled = true;
+		tegra_dvfs_core_disabled = true;
+		core_min_mv_index = 0;
+	}
+
+	vdd_core_rail.nominal_millivolts =
+		core_millivolts[core_nominal_mv_index];
+	vdd_core_rail.min_millivolts =
+		core_millivolts[core_min_mv_index];
+	vdd_core_rail.nvver = dvfs_data->core_dvfs_ver;
+
+	/*
+	 * Search core dvfs table for speedo/process matching entries and
+	 * initialize dvfs-ed clocks
+	 */
+	for (i = 0; i < dvfs_data->core_vf_table_size; i++) {
+		struct dvfs *d = &dvfs_data->core_vf_table[i];
+		if (!match_dvfs_one(d->clk_name, d->speedo_id,
+			d->process_id, soc_speedo_id, core_process_id))
+			continue;
+		init_dvfs_one(d, core_nominal_mv_index);
+
+		/*
+		 * If EMC DVB table is installed, use it to set EMC VF opps.
+		 * Otherwise, EMC dvfs is board dependent, EMC frequencies are
+		 * determined by the Tegra BCT and the board specific EMC DFS
+		 * table owned by EMC driver.
+		 */
+		if (!strcmp(d->clk_name, "emc") && tegra210_emc_is_ready()) {
+			struct dvb_dvfs *dvbd = get_emc_dvb_dvfs(soc_speedo_id);
+
+			if (dvbd)
+				adjust_emc_dvfs_from_dvb_table(
+					core_process_id, dvbd, d);
+			else
+				adjust_emc_dvfs_from_timing_table(d);
+		}
+	}
+
+	init_qspi_dvfs(soc_speedo_id, core_process_id, core_nominal_mv_index);
+	init_sor1_dvfs(soc_speedo_id, core_process_id, core_nominal_mv_index);
+	init_spi_dvfs(soc_speedo_id, core_process_id, core_nominal_mv_index);
+}
+
+static void init_dvfs_data(struct tegra_dvfs_data *data)
+{
+	int i;
+	static bool initialized;
+
+	if (initialized)
+		return;
+
+	initialized = true;
+
+	dvfs_data = data;
+
+	BUG_ON(dvfs_data->rails_num != ARRAY_SIZE(vdd_dvfs_rails));
+	vdd_cpu_rail = *dvfs_data->rails[VDD_CPU_INDEX];
+	vdd_core_rail = *dvfs_data->rails[VDD_CORE_INDEX];
+	vdd_gpu_rail = *dvfs_data->rails[VDD_GPU_INDEX];
+
+	for (i = 0; i < MAX_DVFS_FREQS; i++)
+		core_millivolts[i] = dvfs_data->core_mv[i];
+}
+
+static struct tegra_dvfs_data tegra210_dvfs_data = {
+	.rails = tegra210_dvfs_rails,
+	.rails_num = ARRAY_SIZE(tegra210_dvfs_rails),
+	.cpu_fv_table = cpu_fv_dvfs_table,
+	.cpu_fv_table_size = ARRAY_SIZE(cpu_fv_dvfs_table),
+	.gpu_cvb_table = gpu_cvb_dvfs_table,
+	.gpu_cvb_table_size = ARRAY_SIZE(gpu_cvb_dvfs_table),
+
+	.core_mv = core_voltages_mv,
+	.core_vf_table = core_dvfs_table,
+	.core_vf_table_size = ARRAY_SIZE(core_dvfs_table),
+	.spi_vf_table = spi_dvfs_table,
+	.spi_slave_vf_table = spi_slave_dvfs_table,
+	.qspi_sdr_vf_table = qspi_sdr_dvfs_table,
+	.qspi_ddr_vf_table = qspi_ddr_dvfs_table,
+	.sor1_dp_vf_table = sor1_dp_dvfs_table,
+	.sor1_dp_vf_table_size = ARRAY_SIZE(sor1_dp_dvfs_table),
+	.get_core_min_mv = get_core_sku_min_mv,
+	.get_core_max_mv = get_core_sku_max_mv,
+
+	.core_floors = tegra210_core_therm_floors,
+	.core_caps = tegra210_core_therm_caps,
+	.core_caps_ucm2 = tegra210_core_therm_caps_ucm2,
+};
+
+static struct tegra_dvfs_data tegra210b01_dvfs_data = {
+	.rails = tegra210b01_dvfs_rails,
+	.rails_num = ARRAY_SIZE(tegra210b01_dvfs_rails),
+	.cpu_fv_table = cpub01_fv_dvfs_table,
+	.cpu_fv_table_size = ARRAY_SIZE(cpub01_fv_dvfs_table),
+	.gpu_cvb_table = gpub01_cvb_dvfs_table,
+	.gpu_cvb_table_size = ARRAY_SIZE(gpub01_cvb_dvfs_table),
+
+	.emc_dvb_table = emcb01_dvb_dvfs_table,
+	.emc_dvb_table_size = ARRAY_SIZE(emcb01_dvb_dvfs_table),
+
+	.core_mv = coreb01_voltages_mv,
+	.core_vf_table = coreb01_dvfs_table,
+	.core_vf_table_size = ARRAY_SIZE(coreb01_dvfs_table),
+	.spi_vf_table = spib01_dvfs_table,
+	.spi_slave_vf_table = spi_slaveb01_dvfs_table,
+	.qspi_sdr_vf_table = qspi_sdrb01_dvfs_table,
+	.qspi_ddr_vf_table = qspi_ddrb01_dvfs_table,
+	.sor1_dp_vf_table = sor1_dpb01_dvfs_table,
+	.sor1_dp_vf_table_size = ARRAY_SIZE(sor1_dpb01_dvfs_table),
+	.get_core_min_mv = get_coreb01_sku_min_mv,
+	.get_core_max_mv = get_coreb01_sku_max_mv,
+
+	.core_floors = tegra210b01_core_therm_floors,
+	.core_caps = tegra210b01_core_therm_caps,
+	.core_caps_ucm2 = tegra210b01_core_therm_caps_ucm2,
+
+	.core_dvfs_ver = coreb01_dvfs_table_ver,
+};
+
+static struct tegra_dvfs_data tegra210b01slt_dvfs_data = {
+	.rails = tegra210b01_dvfs_rails,
+	.rails_num = ARRAY_SIZE(tegra210b01_dvfs_rails),
+	.cpu_fv_table = cpub01_fv_dvfs_table,
+	.cpu_fv_table_size = ARRAY_SIZE(cpub01_fv_dvfs_table),
+	.gpu_cvb_table = gpub01_cvb_dvfs_table,
+	.gpu_cvb_table_size = ARRAY_SIZE(gpub01_cvb_dvfs_table),
+
+	.emc_dvb_table = emcb01slt_dvb_dvfs_table,
+	.emc_dvb_table_size = ARRAY_SIZE(emcb01slt_dvb_dvfs_table),
+
+	.core_mv = coreb01slt_voltages_mv,
+	.core_vf_table = coreb01slt_dvfs_table,
+	.core_vf_table_size = ARRAY_SIZE(coreb01slt_dvfs_table),
+	.spi_vf_table = spib01slt_dvfs_table,
+	.spi_slave_vf_table = spi_slaveb01slt_dvfs_table,
+	.qspi_sdr_vf_table = qspi_sdrb01slt_dvfs_table,
+	.qspi_ddr_vf_table = qspi_ddrb01slt_dvfs_table,
+	.sor1_dp_vf_table = sor1_dpb01slt_dvfs_table,
+	.sor1_dp_vf_table_size = ARRAY_SIZE(sor1_dpb01slt_dvfs_table),
+	.get_core_min_mv = get_coreb01slt_sku_min_mv,
+	.get_core_max_mv = get_coreb01slt_sku_max_mv,
+
+	.core_floors = tegra210b01_core_therm_floors,
+	.core_caps = tegra210b01_core_therm_caps,
+	.core_caps_ucm2 = tegra210b01_core_therm_caps_ucm2,
+
+	.core_dvfs_ver = coreb01slt_dvfs_table_ver,
+};
+
+static void disable_rail_scaling(struct device_node *np)
+{
+	/* With DFLL as clock source CPU rail scaling cannot be disabled */
+
+	if (tegra_dvfs_core_disabled ||
+	    of_property_read_bool(np, "nvidia,core-rail-scaling-disabled")) {
+		vdd_dvfs_rails[VDD_CORE_INDEX]->disabled = true;
+	}
+	if (tegra_dvfs_gpu_disabled ||
+	    of_property_read_bool(np, "nvidia,gpu-rail-scaling-disabled")) {
+		vdd_dvfs_rails[VDD_GPU_INDEX]->disabled = true;
+	}
+}
+
+static int tegra210x_init_dvfs(struct device *dev, bool cpu_lp_init)
+{
+	int soc_speedo_id = tegra_sku_info.soc_speedo_id;
+	int core_process_id = tegra_sku_info.soc_process_id;
+	bool ucm2 = tegra_sku_info.ucm == TEGRA_UCM2;
+	int i, ret;
+	int cpu_max_freq_index = 0;
+	int cpu_lp_max_freq_index = 0;
+	int gpu_max_freq_index = 0;
+	struct device_node *node = dev->of_node;
+
+	tegra_dvfs_init_rails_lists(vdd_dvfs_rails, dvfs_data->rails_num);
+	init_core_dvfs_table(soc_speedo_id, core_process_id);
+
+	/* Get rails alignment, defer probe if regulators are not ready */
+	for (i = 0; i <  dvfs_data->rails_num; i++) {
+		struct regulator *reg;
+		unsigned int step_uv;
+		int min_uV, max_uV, ret;
+
+		reg = regulator_get(dev, vdd_dvfs_rails[i]->reg_id);
+		if (IS_ERR(reg)) {
+			pr_info("tegra_dvfs: Unable to get %s rail for step info, defering probe\n",
+					vdd_dvfs_rails[i]->reg_id);
+			return -EPROBE_DEFER;
+		}
+
+		ret = regulator_get_constraint_voltages(reg, &min_uV, &max_uV);
+		if (!ret)
+			vdd_dvfs_rails[i]->alignment.offset_uv = min_uV;
+
+		step_uv = regulator_get_linear_step(reg); /* 1st try get step */
+		if (!step_uv && !ret) {    /* if no step, try to calculate it */
+			int n_voltages = regulator_count_voltages(reg);
+
+			if (n_voltages > 1)
+				step_uv = (max_uV - min_uV) / (n_voltages - 1);
+		}
+		if (step_uv) {
+			vdd_dvfs_rails[i]->alignment.step_uv = step_uv;
+			vdd_dvfs_rails[i]->stats.bin_uv = step_uv;
+		}
+		regulator_put(reg);
+	}
+
+	/*
+	 * Construct fast and slow CPU DVFS tables from FV data; find maximum
+	 * frequency, minimum  and nominal voltage for each CPU cluster, and
+	 * combined rail limits (fast CPU should be initialized 1st).
+	 */
+	ret = init_cpu_dvfs_table(dvfs_data->cpu_fv_table,
+		dvfs_data->cpu_fv_table_size, &cpu_max_freq_index);
+	if (ret)
+		goto out;
+
+	if (cpu_lp_init) {
+		ret = init_cpu_lp_dvfs_table(&cpu_lp_max_freq_index);
+		if (ret)
+			goto out;
+	}
+
+	/*
+	 * Construct GPU DVFS table from CVB data; find GPU maximum frequency,
+	 * and nominal voltage.
+	 */
+	init_gpu_dvfs_table(node, dvfs_data->gpu_cvb_table,
+		dvfs_data->gpu_cvb_table_size, &gpu_max_freq_index);
+
+	/* Init core thermal floors abd caps */
+	vdd_core_rail.therm_floors = dvfs_data->core_floors;
+	vdd_core_rail.therm_caps =
+		ucm2 ? dvfs_data->core_caps_ucm2 : dvfs_data->core_caps;
+	tegra_dvfs_core_init_therm_limits(&vdd_core_rail);
+
+	/* Init rail structures and dependencies */
+	tegra_dvfs_init_rails(vdd_dvfs_rails, dvfs_data->rails_num);
+
+	if (of_property_read_bool(of_chosen, "nvidia,tegra-joint_xpu_rail"))
+		vdd_dvfs_rails[VDD_GPU_INDEX]->joint_rail_with_dfll = true;
+
+	/*
+	 * Initialize matching cpu dvfs entry already found when nominal
+	 * voltage was determined
+	 */
+	init_dvfs_one(&cpu_dvfs, cpu_max_freq_index);
+	if (cpu_lp_init)
+		init_dvfs_one(&cpu_lp_dvfs, cpu_lp_max_freq_index);
+	init_dvfs_one(&gpu_dvfs, gpu_max_freq_index);
+
+	disable_rail_scaling(node);
+
+	for (i = 0; i < dvfs_data->rails_num; i++) {
+		struct dvfs_rail *rail = vdd_dvfs_rails[i];
+		pr_info("tegra dvfs: %s: nominal %dmV, offset %duV, step %duV, scaling %s\n",
+			rail->reg_id, rail->nominal_millivolts,
+			rail->alignment.offset_uv, rail->alignment.step_uv,
+			rail->disabled ? "disabled" : "enabled");
+	}
+	return 0;
+out:
+	return ret;
+}
+
+int tegra210_init_dvfs(struct device *dev)
+{
+	init_dvfs_data(&tegra210_dvfs_data);
+	return tegra210x_init_dvfs(dev, true);
+}
+
+int tegra210b01_init_dvfs(struct device *dev)
+{
+	update_emc_override_dvb_dvfs(&tegra210b01_dvfs_data);
+
+	if (tegra_sku_info.soc_speedo_id == 2)
+		init_dvfs_data(&tegra210b01slt_dvfs_data);
+	else
+		init_dvfs_data(&tegra210b01_dvfs_data);
+
+	return tegra210x_init_dvfs(dev, false);
+}
diff --git a/drivers/thermal/tegra/soctherm.c b/drivers/thermal/tegra/soctherm.c
index ea66cba09e56..9c0212540ceb 100644
--- a/drivers/thermal/tegra/soctherm.c
+++ b/drivers/thermal/tegra/soctherm.c
@@ -1969,6 +1969,34 @@ static void tegra_soctherm_throttle(struct device *dev)
 	writel(v, ts->regs + THERMCTL_STATS_CTL);
 }
 
+struct tsensor_hw_pllx_offset {
+	bool cpu_rail_low_voltage;
+	bool gpu_rail_low_voltage;
+	void __iomem *sensor_valid_reg;
+};
+
+static struct tsensor_hw_pllx_offset hw_pllx;
+static DEFINE_SPINLOCK(soctherm_lock);
+
+void tegra_soctherm_gpu_tsens_invalidate(bool low_voltage_range)
+{
+	u32 r;
+	unsigned long flags;
+
+	if (!hw_pllx.sensor_valid_reg)
+		return;
+
+	spin_lock_irqsave(&soctherm_lock, flags);
+	hw_pllx.gpu_rail_low_voltage = low_voltage_range;
+	r = readl(hw_pllx.sensor_valid_reg);
+	r = (hw_pllx.gpu_rail_low_voltage) ?
+		(r | SENSOR_GPU_VALID_MASK) :
+		(r & ~SENSOR_GPU_VALID_MASK);
+	writel(r, hw_pllx.sensor_valid_reg);
+	spin_unlock_irqrestore(&soctherm_lock, flags);
+}
+EXPORT_SYMBOL_GPL(tegra_soctherm_gpu_tsens_invalidate);
+
 static int soctherm_interrupts_init(struct platform_device *pdev,
 				    struct tegra_soctherm *tegra)
 {
diff --git a/drivers/thermal/tegra/soctherm.h b/drivers/thermal/tegra/soctherm.h
index 70501e73d586..1c227b4ba337 100644
--- a/drivers/thermal/tegra/soctherm.h
+++ b/drivers/thermal/tegra/soctherm.h
@@ -56,6 +56,11 @@
 #define SENSOR_TEMP2_MEM_TEMP_MASK		(0xffff << 16)
 #define SENSOR_TEMP2_PLLX_TEMP_MASK		0xffff
 
+#define SENSOR_VALID			0x1e0
+#define SENSOR_GPU_VALID_MASK		BIT(9)
+#define SENSOR_CPU_VALID_MASK		0xf
+#define SENSOR_MEM_VALID_MASK		(0x3 << 10)
+
 /**
  * struct tegra_tsensor_group - SOC_THERM sensor group data
  * @name: short name of the temperature sensor group
diff --git a/drivers/video/Kconfig b/drivers/video/Kconfig
index 66e55f4ef9e3..fac8c7849a6c 100644
--- a/drivers/video/Kconfig
+++ b/drivers/video/Kconfig
@@ -74,6 +74,21 @@ endif
 
 if ARCH_TEGRA
 	source "drivers/video/tegra/nvmap/Kconfig"
+
+config TEGRA_BWMGR
+	bool "Use BWMGR"
+	default y
+	help
+	  say Y here to use bwmgr
+
+config TEGRA_DVFS
+	bool "Use DVFS"
+	default y
+	help
+	  say Y here to use DVFS
+
 endif
 
+
+
 endmenu
diff --git a/drivers/video/tegra/nvmap/nvmap_ioctl.c b/drivers/video/tegra/nvmap/nvmap_ioctl.c
index f2de833f2376..7fbef88456f1 100644
--- a/drivers/video/tegra/nvmap/nvmap_ioctl.c
+++ b/drivers/video/tegra/nvmap/nvmap_ioctl.c
@@ -19,6 +19,7 @@
 
 #include <linux/dma-mapping.h>
 #include <linux/export.h>
+#include <linux/fdtable.h>
 #include <linux/fs.h>
 #include <linux/io.h>
 #include <linux/kernel.h>
@@ -459,7 +460,7 @@ int nvmap_ioctl_free(struct file *filp, unsigned long arg)
 		return 0;
 
 	nvmap_free_handle_fd(client, arg);
-	return sys_close(arg);
+	return close_fd(arg);
 }
 
 static ssize_t rw_handle(struct nvmap_client *client, struct nvmap_handle *h,
diff --git a/include/linux/clk-provider.h b/include/linux/clk-provider.h
index 1293c38ddb7f..476bb3243d6f 100644
--- a/include/linux/clk-provider.h
+++ b/include/linux/clk-provider.h
@@ -359,6 +359,7 @@ struct clk_fixed_rate {
 #define CLK_FIXED_RATE_PARENT_ACCURACY	BIT(0)
 
 extern const struct clk_ops clk_fixed_rate_ops;
+bool __clk_is_prepared(struct clk *clk);
 struct clk_hw *__clk_hw_register_fixed_rate(struct device *dev,
 		struct device_node *np, const char *name,
 		const char *parent_name, const struct clk_hw *parent_hw,
@@ -1297,6 +1298,21 @@ void clk_hw_unregister(struct clk_hw *hw);
 const char *__clk_get_name(const struct clk *clk);
 const char *clk_hw_get_name(const struct clk_hw *hw);
 #ifdef CONFIG_COMMON_CLK
+
+#ifdef CONFIG_DEBUG_FS
+struct dentry *__clk_debugfs_add_file(struct clk *clk, char *name,
+		umode_t mode, void *data, const struct file_operations *fops);
+struct dentry *clk_debugfs_add_file(struct clk_hw *hw, char *name, umode_t mode,
+				void *data, const struct file_operations *fops);
+#else
+static inline struct dentry *__clk_debugfs_add_file(struct clk *clk, char *name,
+		umode_t mode, void *data, const struct file_operations *fops)
+{ return NULL; }
+static inline struct dentry *clk_debugfs_add_file(struct clk_hw *hw, char *name,
+		umode_t mode, void *data, const struct file_operations *fops)
+{ return NULL; }
+#endif
+
 struct clk_hw *__clk_get_hw(struct clk *clk);
 #else
 static inline struct clk_hw *__clk_get_hw(struct clk *clk)
diff --git a/include/linux/clk.h b/include/linux/clk.h
index 06f1b292f8a0..38ea99fba492 100644
--- a/include/linux/clk.h
+++ b/include/linux/clk.h
@@ -837,6 +837,16 @@ int clk_set_min_rate(struct clk *clk, unsigned long rate);
  */
 int clk_set_max_rate(struct clk *clk, unsigned long rate);
 
+/**
+ * clk_set_rate_refresh - re-set a clocks rate, including triggering any
+ *                        notifiers.
+ *
+ * @clk: clock source
+ *
+ * Returns success (0) or negative errno.
+ */
+int clk_set_rate_refresh(struct clk *clk);
+
 /**
  * clk_set_parent - set the parent clock source for this clock
  * @clk: clock source
diff --git a/include/linux/platform/tegra/emc_bwmgr.h b/include/linux/platform/tegra/emc_bwmgr.h
new file mode 100644
index 000000000000..fe44b7dadf9c
--- /dev/null
+++ b/include/linux/platform/tegra/emc_bwmgr.h
@@ -0,0 +1,396 @@
+/**
+ * Copyright (c) 2015-2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __EMC_BWMGR_H
+#define __EMC_BWMGR_H
+
+#include <linux/clk.h>
+#include <linux/device.h>
+#include <linux/of_address.h>
+#include <linux/platform/tegra/iso_client.h>
+
+/* keep in sync with tegra_bwmgr_client_names */
+enum tegra_bwmgr_client_id {
+	TEGRA_BWMGR_CLIENT_CPU_CLUSTER_0,
+	TEGRA_BWMGR_CLIENT_CPU_CLUSTER_1,
+	TEGRA_BWMGR_CLIENT_CPU_CLUSTER_2,
+	TEGRA_BWMGR_CLIENT_CPU_CLUSTER_3,
+	TEGRA_BWMGR_CLIENT_DISP0,
+	TEGRA_BWMGR_CLIENT_DISP1,
+	TEGRA_BWMGR_CLIENT_DISP2,
+	TEGRA_BWMGR_CLIENT_DISP1_LA_EMC,
+	TEGRA_BWMGR_CLIENT_DISP2_LA_EMC,
+	TEGRA_BWMGR_CLIENT_USBD,
+	TEGRA_BWMGR_CLIENT_XHCI,
+	TEGRA_BWMGR_CLIENT_SDMMC1,
+	TEGRA_BWMGR_CLIENT_SDMMC2,
+	TEGRA_BWMGR_CLIENT_SDMMC3,
+	TEGRA_BWMGR_CLIENT_SDMMC4,
+	TEGRA_BWMGR_CLIENT_MON,
+	TEGRA_BWMGR_CLIENT_GPU,
+	TEGRA_BWMGR_CLIENT_MSENC,
+	TEGRA_BWMGR_CLIENT_NVENC1,
+	TEGRA_BWMGR_CLIENT_NVJPG,
+	TEGRA_BWMGR_CLIENT_NVDEC,
+	TEGRA_BWMGR_CLIENT_NVDEC1,
+	TEGRA_BWMGR_CLIENT_TSEC,
+	TEGRA_BWMGR_CLIENT_TSECB,
+	TEGRA_BWMGR_CLIENT_VI,
+	TEGRA_BWMGR_CLIENT_ISPA,
+	TEGRA_BWMGR_CLIENT_ISPB,
+	TEGRA_BWMGR_CLIENT_CAMERA,
+	TEGRA_BWMGR_CLIENT_CAMERA_NON_ISO,
+	TEGRA_BWMGR_CLIENT_CAMRTC,
+	TEGRA_BWMGR_CLIENT_ISOMGR,
+	TEGRA_BWMGR_CLIENT_THERMAL_CAP,
+	TEGRA_BWMGR_CLIENT_VIC,
+	TEGRA_BWMGR_CLIENT_APE_ADSP,
+	TEGRA_BWMGR_CLIENT_APE_ADMA,
+	TEGRA_BWMGR_CLIENT_PCIE,
+	TEGRA_BWMGR_CLIENT_PCIE_1,
+	TEGRA_BWMGR_CLIENT_PCIE_2,
+	TEGRA_BWMGR_CLIENT_PCIE_3,
+	TEGRA_BWMGR_CLIENT_PCIE_4,
+	TEGRA_BWMGR_CLIENT_PCIE_5,
+	TEGRA_BWMGR_CLIENT_BBC_0,
+	TEGRA_BWMGR_CLIENT_EQOS,
+	TEGRA_BWMGR_CLIENT_SE0,
+	TEGRA_BWMGR_CLIENT_SE1,
+	TEGRA_BWMGR_CLIENT_SE2,
+	TEGRA_BWMGR_CLIENT_SE3,
+	TEGRA_BWMGR_CLIENT_SE4,
+	TEGRA_BWMGR_CLIENT_PMQOS,
+	TEGRA_BWMGR_CLIENT_NVPMODEL,
+	TEGRA_BWMGR_CLIENT_DEBUG,
+	TEGRA_BWMGR_CLIENT_DLA0,
+	TEGRA_BWMGR_CLIENT_DLA1,
+	TEGRA_BWMGR_CLIENT_COUNT /* Should always be last */
+};
+
+enum tegra_bwmgr_request_type {
+	TEGRA_BWMGR_SET_EMC_FLOOR, /* lower bound */
+	TEGRA_BWMGR_SET_EMC_CAP, /* upper bound */
+	TEGRA_BWMGR_SET_EMC_ISO_CAP, /* upper bound that affects ISO Bw */
+	TEGRA_BWMGR_SET_EMC_SHARED_BW, /* shared bw request */
+	TEGRA_BWMGR_SET_EMC_SHARED_BW_ISO, /* for use by ISO Mgr only */
+	TEGRA_BWMGR_SET_EMC_REQ_COUNT /* Should always be last */
+};
+
+enum bwmgr_dram_types {
+	DRAM_TYPE_NONE,
+	DRAM_TYPE_LPDDR4_16CH_ECC,
+	DRAM_TYPE_LPDDR4_8CH_ECC,
+	DRAM_TYPE_LPDDR4_4CH_ECC,
+	DRAM_TYPE_LPDDR4_2CH_ECC,
+	DRAM_TYPE_LPDDR4_16CH,
+	DRAM_TYPE_LPDDR4_8CH,
+	DRAM_TYPE_LPDDR4_4CH,
+	DRAM_TYPE_LPDDR4_2CH,
+	DRAM_TYPE_LPDDR3_2CH,
+	DRAM_TYPE_DDR3_2CH
+};
+
+extern u8 bwmgr_dram_efficiency;
+extern u8 bwmgr_dram_num_channels;
+/* flag to determine supported memory and channel configuration */
+extern u8 bwmgr_dram_config_supported;
+extern u32 *bwmgr_dram_iso_eff_table;
+extern u32 *bwmgr_dram_noniso_eff_table;
+extern u32 *bwmgr_max_nvdis_bw_reqd;
+extern u32 *bwmgr_max_vi_bw_reqd;
+extern int *bwmgr_slope;
+extern u32 *bwmgr_vi_bw_reqd_offset;
+extern int bwmgr_iso_bw_percentage;
+extern enum bwmgr_dram_types bwmgr_dram_type;
+extern int emc_to_dram_freq_factor;
+
+struct tegra_bwmgr_client;
+
+struct bwmgr_ops {
+	unsigned long (*freq_to_bw)(unsigned long freq);
+	unsigned long (*bw_to_freq)(unsigned long bw);
+	u32 (*dvfs_latency)(u32 ufreq);
+	unsigned long (*bwmgr_apply_efficiency)(
+		unsigned long total_bw, unsigned long iso_bw,
+		unsigned long max_rate, u64 usage_flags,
+		unsigned long *iso_bw_min, unsigned long iso_bw_nvdis,
+		unsigned long iso_bw_vi);
+	unsigned long (*get_best_iso_freq)(long iso_bw,
+		long iso_bw_nvdis, long iso_bw_vi);
+	void (*update_efficiency)(unsigned long dram_refresh_rate);
+	u32 (*get_max_iso_bw)(enum tegra_iso_client client);
+};
+
+struct bwmgr_ops *bwmgr_eff_init_t21x(void);
+struct bwmgr_ops *bwmgr_eff_init_t18x(void);
+struct bwmgr_ops *bwmgr_eff_init_t19x(void);
+
+#if defined(CONFIG_TEGRA_BWMGR)
+/**
+ * tegra_bwmgr_register - register an EMC Bandwidth Manager client.
+ *			  Also see tegra_bwmgr_unregister().
+ * @client      client id from tegra_bwmgr_client_id
+ *
+ * Returns a valid handle on successful registration, NULL on error.
+ */
+struct tegra_bwmgr_client *tegra_bwmgr_register(
+		enum tegra_bwmgr_client_id client);
+
+/**
+ * tegra_bwmgr_unregister - unregister an EMC Bandwidth Manager client.
+ *			    Callers should match register/unregister calls.
+ *			    Persistence of old requests across
+ *			    register/unregister calls is undefined.
+ *			    Also see tegra_bwmgr_set_emc()
+ *
+ * @handle      handle acquired during tegra_bwmgr_register
+ */
+void tegra_bwmgr_unregister(struct tegra_bwmgr_client *handle);
+
+/**
+ * tegra_bwmgr_get_dram_num_channels - get the number of DRAM channels
+ *
+ * Returns the number of DRAM channels that are configured on the underlying
+ * platform.
+ */
+u8 tegra_bwmgr_get_dram_num_channels(void);
+
+/**
+ * tegra_bwmgr_get_emc_rate - get the current EMC rate.
+ *
+ * Returns current memory clock rate in Hz.
+ */
+unsigned long tegra_bwmgr_get_emc_rate(void);
+
+/**
+ * tegra_bwmgr_get_max_emc_rate - get the max EMC rate.
+ *
+ * Returns the max memory clock rate in Hz.
+ */
+unsigned long tegra_bwmgr_get_max_emc_rate(void);
+
+/**
+ * tegra_bwmgr_get_core_emc_rate - get the actual emc frequency calculated
+ *			using the dram frequency and emc_to_dram
+ *			conversion factor.
+ *
+ * Returns the core emc rate in Hz.
+ */
+unsigned long tegra_bwmgr_get_core_emc_rate(void);
+
+/**
+ * tegra_bwmgr_round_rate - round up to next EMC rate which can be provided
+ *
+ * @bw		Input rate
+ *
+ * Returns the next higher rate from the Input rate that EMC can run at.
+ */
+unsigned long tegra_bwmgr_round_rate(unsigned long bw);
+
+/**
+ * tegra_bwmgr_set_emc - request to bwmgr to set an EMC rate parameter.
+ *			 Actual clock rate depends on aggregation of
+ *			 requests by all clients. If needed, use
+ *			 tegra_bwmgr_get_emc_rate() to get the rate after
+ *			 a tegra_bwmgr_set_emc() call.
+ *
+ *			 Call tegra_bwmgr_set_emc() with same request type and
+ *			 val = 0 to clear request.
+ *
+ * @handle      handle acquired during tegra_bwmgr_register
+ * @val         value to be set in Hz, 0 to clear old request of the same type
+ * @req         chosen type from tegra_bwmgr_request_type
+ *
+ * Returns success (0) or negative errno.
+ */
+int tegra_bwmgr_set_emc(struct tegra_bwmgr_client *handle, unsigned long val,
+		enum tegra_bwmgr_request_type req);
+
+/**
+ * tegra_bwmgr_get_client_info - outputs the value previously set with
+ *                       tegra_bwmgr_set_emc or 0 if no value has been set.
+ *
+ * @handle      handle acquired during tegra_bwmgr_register
+ * @out_val     bandwidth value in Hz
+ * @req         chosen type from tegra_bwmgr_request_type
+ *
+ * Returns success (0) or negative errno.
+ */
+int tegra_bwmgr_get_client_info(struct tegra_bwmgr_client *handle,
+		unsigned long *out_val,
+		enum tegra_bwmgr_request_type req);
+
+/**
+ * tegra_bwmgr_notifier_register - register a notifier callback when
+ *		emc rate changes. Must be called from non-atomic
+ *		context. The callback must not call any bwmgr API.
+ * @nb		linux notifier block
+ *
+ * Returns success (0) or negative errno.
+ */
+int tegra_bwmgr_notifier_register(struct notifier_block *nb);
+
+/**
+ * tegra_bwmgr_notifier_unregister - unregister a notifier callback.
+ * @nb		linux notifier block
+ *
+ * Returns success (0) or negative errno.
+ */
+int tegra_bwmgr_notifier_unregister(struct notifier_block *nb);
+
+/*
+ * Initialize bwmgr.
+ * This api would be called by .init_machine during boot.
+ * bwmgr clients, don't call this api.
+ */
+int __init bwmgr_init(void);
+
+void __exit bwmgr_exit(void);
+
+/*
+ * Initialize pmqos bwmgr code which registers pmqos as bwmgr client and
+ * registers a notifier which gets called on update to PMQOS_EMC_FREQ_MIN.
+ */
+int __init pmqos_bwmgr_init(void);
+
+#else /* CONFIG_TEGRA_BWMGR */
+
+static inline struct tegra_bwmgr_client *tegra_bwmgr_register(
+		enum tegra_bwmgr_client_id client_id)
+{
+	static int i;
+	/* return a dummy handle to allow client to function
+	 * as if bwmgr were enabled.
+	 */
+	return (struct tegra_bwmgr_client *) &i;
+}
+
+static inline void tegra_bwmgr_unregister(struct tegra_bwmgr_client *handle) {}
+
+static inline int bwmgr_init(void)
+{
+	return 0;
+}
+
+static inline void bwmgr_exit(void) {}
+
+static inline u8 tegra_bwmgr_get_dram_num_channels(void)
+{
+	return 0;
+}
+
+static inline unsigned long tegra_bwmgr_get_emc_rate(void)
+{
+	static struct clk *bwmgr_emc_clk;
+	struct device_node *dn;
+
+	if (!bwmgr_emc_clk) {
+		dn = of_find_compatible_node(NULL, NULL, "nvidia,bwmgr");
+		if (dn == NULL) {
+			pr_err("bwmgr: dt node not found.\n");
+			return 0;
+		}
+
+		bwmgr_emc_clk = of_clk_get(dn, 0);
+		if (IS_ERR_OR_NULL(bwmgr_emc_clk)) {
+			pr_err("bwmgr: couldn't find emc clock.\n");
+			bwmgr_emc_clk = NULL;
+			WARN_ON(true);
+			return 0;
+		}
+	}
+
+	return clk_get_rate(bwmgr_emc_clk);
+}
+
+static inline unsigned long tegra_bwmgr_get_max_emc_rate(void)
+{
+	static struct clk *bwmgr_emc_clk;
+	struct device_node *dn;
+
+	if (!bwmgr_emc_clk) {
+		dn = of_find_compatible_node(NULL, NULL, "nvidia,bwmgr");
+		if (dn == NULL) {
+			pr_err("bwmgr: dt node not found.\n");
+			return 0;
+		}
+
+		bwmgr_emc_clk = of_clk_get(dn, 0);
+		if (IS_ERR_OR_NULL(bwmgr_emc_clk)) {
+			pr_err("bwmgr: couldn't find emc clock.\n");
+			bwmgr_emc_clk = NULL;
+			WARN_ON(true);
+			return 0;
+		}
+	}
+
+	/* Use LONG_MAX as clk_round_rate treats rate argument as signed */
+	return clk_round_rate(bwmgr_emc_clk, LONG_MAX);
+}
+
+static inline unsigned long tegra_bwmgr_get_core_emc_rate(void)
+{
+	return 0;
+}
+static inline unsigned long tegra_bwmgr_round_rate(unsigned long bw)
+{
+	static struct clk *bwmgr_emc_clk;
+	struct device_node *dn;
+
+	if (!bwmgr_emc_clk) {
+		dn = of_find_compatible_node(NULL, NULL, "nvidia,bwmgr");
+		if (dn == NULL) {
+			pr_err("bwmgr: dt node not found.\n");
+			return 0;
+		}
+
+		bwmgr_emc_clk = of_clk_get(dn, 0);
+		if (IS_ERR_OR_NULL(bwmgr_emc_clk)) {
+			pr_err("bwmgr: couldn't find emc clock.\n");
+			bwmgr_emc_clk = NULL;
+			WARN_ON(true);
+			return 0;
+		}
+	}
+
+	return clk_round_rate(bwmgr_emc_clk, bw);
+}
+
+static inline int tegra_bwmgr_set_emc(struct tegra_bwmgr_client *handle,
+		unsigned long val, enum tegra_bwmgr_request_type req)
+{
+	return 0;
+}
+
+static inline int tegra_bwmgr_get_client_info(struct tegra_bwmgr_client *handle,
+		unsigned long *out_val,
+		enum tegra_bwmgr_request_type req)
+{
+	if (!out_val)
+		return -EINVAL;
+	*out_val = 0;
+	return 0;
+}
+
+static inline int tegra_bwmgr_notifier_register(struct notifier_block *nb)
+{
+	return 0;
+}
+
+static inline int tegra_bwmgr_notifier_unregister(struct notifier_block *nb)
+{
+	return 0;
+}
+
+#endif /* CONFIG_TEGRA_BWMGR */
+#endif /* __EMC_BWMGR_H */
diff --git a/include/linux/platform/tegra/isomgr.h b/include/linux/platform/tegra/isomgr.h
new file mode 100644
index 000000000000..624dbb5eec43
--- /dev/null
+++ b/include/linux/platform/tegra/isomgr.h
@@ -0,0 +1,215 @@
+/*
+ * include/mach/isomgr.h
+ *
+ * Copyright (c) 2012-2018, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+ */
+
+#ifndef _INCLUDE_MACH_ISOMGR_H
+#define _INCLUDE_MACH_ISOMGR_H
+
+#include <linux/platform/tegra/emc_bwmgr.h>
+#include <linux/platform/tegra/iso_client.h>
+
+#define ISOMGR_MAGIC  0x150A1C
+
+/* handle to identify registered client */
+#define tegra_isomgr_handle void *
+
+/* callback to client to renegotiate ISO BW allocation */
+typedef void (*tegra_isomgr_renegotiate)(void *priv,
+					 u32 avail_bw); /* KB/sec */
+
+struct isoclient_info {
+	enum tegra_iso_client client;
+	char *name;
+	char *dev_name;
+	char *emc_clk_name;
+	enum tegra_bwmgr_client_id bwmgr_id;
+};
+
+struct isomgr_client {
+	u32 magic;              /* magic to identify handle */
+	struct kref kref;       /* ref counting */
+	s32 dedi_bw;            /* BW dedicated to this client  (KB/sec) */
+	s32 rsvd_bw;            /* BW reserved for this client  (KB/sec) */
+	s32 real_bw;            /* BW realized for this client  (KB/sec) */
+	s32 lti;                /* Client spec'd Latency Tolerance (usec) */
+	s32 lto;                /* MC calculated Latency Tolerance (usec) */
+	s32 rsvd_mf;            /* reserved minimum freq in support of LT */
+	s32 real_mf;            /* realized minimum freq in support of LT */
+	s32 real_mf_rq;         /* real_mf requested */
+	tegra_isomgr_renegotiate renegotiate;   /* ask client to renegotiate */
+	bool realize;           /* bw realization in progress */
+	s32 sleep_bw;           /* sleeping for realize */
+	s32 margin_bw;          /* BW set aside for this client (KB/sec) */
+	u8 limit_bw_percentage; /* Insufficient HW buffers cause BW to be
+				 * limited to this percentage of DRAM BW
+				 */
+	void *priv;             /* client driver's private data */
+	struct completion cmpl; /* so we can sleep waiting for delta BW */
+
+#ifdef CONFIG_COMMON_CLK
+	struct tegra_bwmgr_client *bwmgr_handle;
+#else
+	struct clk *emc_clk;    /* client emc clk for bw */
+#endif
+
+#ifdef CONFIG_TEGRA_ISOMGR_SYSFS
+	struct kobject *client_kobj;
+	struct isomgr_client_attrs {
+		struct kobj_attribute dedi_bw;
+		struct kobj_attribute rsvd_bw;
+		struct kobj_attribute real_bw;
+		struct kobj_attribute lti;
+		struct kobj_attribute lto;
+		struct kobj_attribute rsvd_mf;
+		struct kobj_attribute real_mf;
+		struct kobj_attribute sleep_bw;
+		struct kobj_attribute margin_bw;
+	} client_attrs;
+#endif /* CONFIG_TEGRA_ISOMGR_SYSFS */
+};
+
+struct isomgr {
+	struct mutex lock;              /* to lock ALL isomgr state */
+	struct task_struct *task;       /* check reentrant/mismatched locks */
+
+#ifdef CONFIG_COMMON_CLK
+	struct tegra_bwmgr_client *bwmgr_handle;
+#else
+	struct clk *emc_clk;            /* isomgr emc clock for floor freq */
+#endif
+
+	s32 lt_mf;                      /* min freq to support worst LT */
+	s32 lt_mf_rq;                   /* requested lt_mf */
+	s32 avail_bw;                   /* globally available MC BW */
+	s32 dedi_bw;                    /* total BW 'dedicated' to clients */
+	s32 sleep_bw;                   /* pending bw requirement */
+	u32 max_iso_bw;                 /* max ISO BW MC can accommodate */
+	struct kobject *kobj;           /* for sysfs linkage */
+	struct isomgr_ops *ops;         /* ops structure for isomgr*/
+};
+
+extern struct isoclient_info *isoclient_info;
+/*platform specific flag for requesting max emc floor req for camera client*/
+extern u8 isomgr_camera_max_floor_req;
+extern int isoclients;
+extern bool client_valid[TEGRA_ISO_CLIENT_COUNT];
+extern struct isomgr_client isomgr_clients[TEGRA_ISO_CLIENT_COUNT];
+extern struct isomgr isomgr;
+extern char *cname[];
+
+struct isomgr_ops {
+	void (*isomgr_plat_init)(void);
+	bool (*isomgr_plat_register)(u32 dedi_bw,
+			enum tegra_iso_client client);
+	void (*isomgr_plat_unregister)(struct isomgr_client *cp);
+	bool (*isomgr_plat_reserve)(struct isomgr_client *cp,
+			u32 bw, enum tegra_iso_client client);
+	bool (*isomgr_plat_realize)(struct isomgr_client *cp);
+	u32 (*isomgr_max_iso_bw)(enum tegra_iso_client client);
+};
+
+struct isomgr_ops *pre_t19x_isomgr_init(void);
+struct isomgr_ops *t19x_isomgr_init(void);
+
+#if defined(CONFIG_TEGRA_ISOMGR)
+/* Register an ISO BW client */
+tegra_isomgr_handle tegra_isomgr_register(enum tegra_iso_client client,
+					  u32 dedicated_bw,	/* KB/sec */
+					  tegra_isomgr_renegotiate renegotiate,
+					  void *priv);
+
+/* Unregister an ISO BW client */
+void tegra_isomgr_unregister(tegra_isomgr_handle handle);
+
+/* Reserve ISO BW on behalf of client - don't apply, rval is dvfs thresh usec */
+u32 tegra_isomgr_reserve(tegra_isomgr_handle handle,
+			 u32 bw,	/* KB/sec */
+			 u32 lt);	/* usec */
+
+/* Realize client reservation - apply settings, rval is dvfs thresh usec */
+u32 tegra_isomgr_realize(tegra_isomgr_handle handle);
+
+/* This sets bw aside for the client specified. */
+int tegra_isomgr_set_margin(enum tegra_iso_client client, u32 bw, bool wait);
+
+int tegra_isomgr_get_imp_time(enum tegra_iso_client, u32 bw);
+
+/* returns available in iso bw in KB/sec */
+u32 tegra_isomgr_get_available_iso_bw(void);
+
+/* returns total iso bw in KB/sec */
+u32 tegra_isomgr_get_total_iso_bw(enum tegra_iso_client client);
+
+/* Initialize isomgr.
+ * This api would be called by .init_machine during boot.
+ * isomgr clients, don't call this api.
+ */
+int __init isomgr_init(void);
+#else
+static inline tegra_isomgr_handle tegra_isomgr_register(
+					  enum tegra_iso_client client,
+					  u32 dedicated_bw,
+					  tegra_isomgr_renegotiate renegotiate,
+					  void *priv)
+{
+	/* return a dummy handle to allow client function
+	 * as if isomgr were enabled.
+	 */
+	return (tegra_isomgr_handle)1;
+}
+
+static inline void tegra_isomgr_unregister(tegra_isomgr_handle handle) {}
+
+static inline u32 tegra_isomgr_reserve(tegra_isomgr_handle handle,
+			 u32 bw, u32 lt)
+{
+	return 1;
+}
+
+static inline u32 tegra_isomgr_realize(tegra_isomgr_handle handle)
+{
+	return 1;
+}
+
+static inline int tegra_isomgr_set_margin(enum tegra_iso_client client, u32 bw)
+{
+	return 0;
+}
+
+static inline int tegra_isomgr_get_imp_time(enum tegra_iso_client client,
+	u32 bw)
+{
+	return 0;
+}
+
+static inline u32 tegra_isomgr_get_available_iso_bw(void)
+{
+	return UINT_MAX;
+}
+
+static inline u32 tegra_isomgr_get_total_iso_bw(enum tegra_iso_client client)
+{
+	return UINT_MAX;
+}
+
+static inline int isomgr_init(void)
+{
+	return 0;
+}
+#endif
+#endif /* _INCLUDE_MACH_ISOMGR_H */
diff --git a/include/linux/pm_qos.h b/include/linux/pm_qos.h
index 4a69d4af3ff8..5c3da7f0aff6 100644
--- a/include/linux/pm_qos.h
+++ b/include/linux/pm_qos.h
@@ -16,6 +16,22 @@
 #include <linux/notifier.h>
 #include <linux/device.h>
 
+
+/**
+ * enum pm_qos_bounded_classes - Class ID's for bounded constraints
+ *
+ * Each class wraps around a corresponding min and max pm qos node
+ * and binds the two constraints in one.
+ */
+enum pm_qos_bounded_classes {
+	PM_QOS_RESERVED_BOUNDS = 0,
+	PM_QOS_CPU_FREQ_BOUNDS,	/* requests should be in KHz to not exceed s32*/
+	PM_QOS_GPU_FREQ_BOUNDS,	/* requests should be in KHz to not exceed s32*/
+	PM_QOS_ONLINE_CPUS_BOUNDS,
+	/* insert new bounded class ids here */
+	PM_QOS_NUM_BOUNDED_CLASSES,
+};
+
 enum pm_qos_flags_status {
 	PM_QOS_FLAGS_UNDEFINED = -1,
 	PM_QOS_FLAGS_NONE,
@@ -27,6 +43,23 @@ enum pm_qos_flags_status {
 #define PM_QOS_LATENCY_ANY	S32_MAX
 #define PM_QOS_LATENCY_ANY_NS	((s64)PM_QOS_LATENCY_ANY * NSEC_PER_USEC)
 
+#define PM_QOS_CPU_DMA_LAT_DEFAULT_VALUE	(2000 * USEC_PER_SEC)
+#define PM_QOS_NETWORK_LAT_DEFAULT_VALUE	(2000 * USEC_PER_SEC)
+#define PM_QOS_NETWORK_THROUGHPUT_DEFAULT_VALUE	0
+#define PM_QOS_MEMORY_BANDWIDTH_DEFAULT_VALUE	0
+#define PM_QOS_LATENCY_TOLERANCE_DEFAULT_VALUE	0
+#define PM_QOS_LATENCY_TOLERANCE_NO_CONSTRAINT	(-1)
+#define PM_QOS_GPU_FREQ_MIN_DEFAULT_VALUE	0
+#define PM_QOS_GPU_FREQ_MAX_DEFAULT_VALUE	INT_MAX
+#define PM_QOS_MAX_ONLINE_CPUS_DEFAULT_VALUE	INT_MAX
+#define PM_QOS_MIN_ONLINE_CPUS_DEFAULT_VALUE	0
+#define PM_QOS_CPU_FREQ_MIN_DEFAULT_VALUE	0
+#define PM_QOS_CPU_FREQ_MAX_DEFAULT_VALUE	INT_MAX
+#define PM_QOS_EMC_FREQ_MIN_DEFAULT_VALUE	0
+#define PM_QOS_EMC_FREQ_MAX_DEFAULT_VALUE	INT_MAX
+#define PM_QOS_CPU_POWER_MAX_DEFAULT_VALUE	INT_MAX
+#define PM_QOS_GPU_POWER_MAX_DEFAULT_VALUE	INT_MAX
+
 #define PM_QOS_CPU_LATENCY_DEFAULT_VALUE	(2000 * USEC_PER_SEC)
 #define PM_QOS_RESUME_LATENCY_DEFAULT_VALUE	PM_QOS_LATENCY_ANY
 #define PM_QOS_RESUME_LATENCY_NO_CONSTRAINT	PM_QOS_LATENCY_ANY
@@ -38,10 +71,51 @@ enum pm_qos_flags_status {
 
 #define PM_QOS_FLAG_NO_POWER_OFF	(1 << 0)
 
+enum {
+	PM_QOS_RESERVED = 0,
+	PM_QOS_CPU_DMA_LATENCY,
+	PM_QOS_NETWORK_LATENCY,
+	PM_QOS_NETWORK_THROUGHPUT,
+	PM_QOS_MEMORY_BANDWIDTH,
+	PM_QOS_GPU_FREQ_MIN,
+	PM_QOS_GPU_FREQ_MAX,
+	PM_QOS_MIN_ONLINE_CPUS,
+	PM_QOS_MAX_ONLINE_CPUS,
+	PM_QOS_CPU_FREQ_MIN,
+	PM_QOS_CPU_FREQ_MAX,
+	PM_QOS_EMC_FREQ_MIN,
+	PM_QOS_MAX_CPU_POWER,
+	PM_QOS_MAX_GPU_POWER,
+
+	/* insert new class ID */
+	PM_QOS_NUM_CLASSES,
+};
+
 enum pm_qos_type {
 	PM_QOS_UNITIALIZED,
 	PM_QOS_MAX,		/* return the largest value */
 	PM_QOS_MIN,		/* return the smallest value */
+	PM_QOS_SUM,		/* return the sum */
+};
+
+/**
+ * struct pm_qos_bounded_constraint - binds two pm_qos_constraints
+ * @prio_list: list of priorities
+ * @max_class: Class id of the upper bound
+ * @min_class: Class id of the lower bound
+ * @min_wins: Pick min if min > max
+ *
+ * Requests are added at their corresponding priority level. Target
+ * values for a bounded constraint will be the intersection of all the
+ * (min, max) ranges specified by each priority level. If the intersection
+ * is null at any priority level, the higher priority level's requests are
+ * honoured.
+ */
+struct pm_qos_bounded_constraint {
+	struct plist_head prio_list;
+	int max_class;
+	int min_class;
+	bool min_wins;
 };
 
 /*
@@ -56,6 +130,7 @@ struct pm_qos_constraints {
 	s32 no_constraint_value;
 	enum pm_qos_type type;
 	struct blocking_notifier_head *notifiers;
+	int parent_class;
 };
 
 struct pm_qos_request {
@@ -163,6 +238,9 @@ static inline void cpu_latency_qos_remove_request(struct pm_qos_request *req) {}
 #endif
 
 #ifdef CONFIG_PM
+int pm_qos_request_func(int pm_qos_class);
+int pm_qos_add_notifier(int pm_qos_class, struct notifier_block *notifier);
+int pm_qos_remove_notifier(int pm_qos_class, struct notifier_block *notifier);
 enum pm_qos_flags_status __dev_pm_qos_flags(struct device *dev, s32 mask);
 enum pm_qos_flags_status dev_pm_qos_flags(struct device *dev, s32 mask);
 s32 __dev_pm_qos_resume_latency(struct device *dev);
diff --git a/include/linux/regulator/consumer.h b/include/linux/regulator/consumer.h
index 2c526c8d10cc..e000dcc9b646 100644
--- a/include/linux/regulator/consumer.h
+++ b/include/linux/regulator/consumer.h
@@ -275,6 +275,8 @@ int regulator_set_voltage(struct regulator *regulator, int min_uV, int max_uV);
 int regulator_set_voltage_time(struct regulator *regulator,
 			       int old_uV, int new_uV);
 int regulator_get_voltage(struct regulator *regulator);
+int regulator_get_constraint_voltages(struct regulator *regulator,
+	int *min_uV, int *max_uV);
 int regulator_sync_voltage(struct regulator *regulator);
 int regulator_set_current_limit(struct regulator *regulator,
 			       int min_uA, int max_uA);
diff --git a/include/soc/tegra/cvb.h b/include/soc/tegra/cvb.h
new file mode 100644
index 000000000000..2d969c40310a
--- /dev/null
+++ b/include/soc/tegra/cvb.h
@@ -0,0 +1,127 @@
+/*
+ * Utility functions for parsing Tegra CVB voltage tables
+ *
+ * Copyright (C) 2012-2017 NVIDIA Corporation.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ */
+
+#ifndef __DRIVERS_CLK_TEGRA_CVB_H
+#define __DRIVERS_CLK_TEGRA_CVB_H
+
+#include <linux/types.h>
+
+struct device;
+
+#define MAX_DVFS_FREQS	40
+
+struct rail_alignment {
+	int offset_uv;
+	int step_uv;
+};
+
+struct cvb_coefficients {
+	int c0;
+	int c1;
+	int c2;
+	int c3;
+	int c4;
+	int c5;
+};
+
+struct cvb_table_freq_entry {
+	unsigned long freq;
+	struct cvb_coefficients coefficients;
+};
+
+struct cvb_cpu_dfll_data {
+	u32 tune0_low;
+	u32 tune0_high;
+	u32 tune1_low;
+	u32 tune1_high;
+	unsigned int tune_high_min_millivolts;
+	unsigned int tune_high_margin_millivolts;
+	unsigned long dvco_calibration_max;
+};
+
+struct thermal_coefficients {
+	struct cvb_coefficients cvb_coef;
+	int c3;
+	int c4;
+	int c5;
+};
+
+/* Thermal trips and voltages */
+struct thermal_tv {
+	int temp;
+	unsigned int millivolts;
+};
+
+struct cvb_table {
+	int speedo_id;
+	int process_id;
+
+	int min_millivolts;
+	int max_millivolts;
+	struct rail_alignment alignment;
+
+	int speedo_scale;
+	int voltage_scale;
+	struct cvb_table_freq_entry entries[MAX_DVFS_FREQS];
+	struct cvb_cpu_dfll_data cpu_dfll_data;
+	struct cvb_coefficients vmin_coefficients;
+	const char *cvb_version;
+};
+
+const struct cvb_table *
+tegra_cvb_add_opp_table(struct device *dev, const struct cvb_table *cvb_tables,
+			size_t count, struct rail_alignment *align,
+			int process_id, int speedo_id, int speedo_value,
+			unsigned long max_freq, int *vmin);
+void tegra_cvb_remove_opp_table(struct device *dev,
+				const struct cvb_table *table,
+				unsigned long max_freq);
+
+struct thermal_table {
+	struct thermal_tv *thermal_floor_table;
+	unsigned int thermal_floor_table_size;
+	struct thermal_coefficients coefficients;
+	unsigned int speedo_scale;
+	unsigned int voltage_scale;
+	unsigned int temp_scale;
+
+	const struct thermal_tv *thermal_cap_table;
+	unsigned int thermal_cap_table_size;
+	const struct thermal_tv *thermal_cap_ucm2_table;
+	unsigned int thermal_cap_ucm2_table_size;
+};
+
+const struct cvb_table *tegra_cvb_build_opp_table(
+		const struct cvb_table *cvb_tables,
+		size_t sz,
+		const struct rail_alignment *align,
+		int process_id,
+		int speedo_id,
+		int speedo_value,
+		unsigned long max_rate,
+		struct device *opp_dev);
+
+int tegra_get_cvb_voltage(int speedo, int s_scale,
+			  const struct cvb_coefficients *cvb);
+int tegra_round_cvb_voltage(int mv, int v_scale,
+			    const struct rail_alignment *align);
+int tegra_round_voltage(int mv, const struct rail_alignment *align, int up);
+int tegra_get_cvb_t_voltage(int speedo, int s_scale, int t, int t_scale,
+			    struct cvb_coefficients *cvb);
+int tegra_cvb_build_thermal_table(const struct thermal_table *table,
+		int speedo_value, unsigned int soc_min_mv);
+
+#endif
diff --git a/include/soc/tegra/pmc.h b/include/soc/tegra/pmc.h
index aadb845d281d..0ed832e86c8a 100644
--- a/include/soc/tegra/pmc.h
+++ b/include/soc/tegra/pmc.h
@@ -21,6 +21,9 @@ bool tegra_pmc_cpu_is_powered(unsigned int cpuid);
 int tegra_pmc_cpu_power_on(unsigned int cpuid);
 int tegra_pmc_cpu_remove_clamping(unsigned int cpuid);
 
+u32 tegra_pmc_readl(unsigned long offset);
+void tegra_pmc_writel_relaxed(u32 value, unsigned long offset);
+
 /*
  * powergate and I/O rail APIs
  */
diff --git a/include/soc/tegra/tegra-dfll.h b/include/soc/tegra/tegra-dfll.h
new file mode 100644
index 000000000000..904dce59013e
--- /dev/null
+++ b/include/soc/tegra/tegra-dfll.h
@@ -0,0 +1,46 @@
+/*
+ * Copyright (c) 2014-2018, NVIDIA Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#ifndef _TEGRA_DFLL_H_
+#define _TEGRA_DFLL_H_
+
+#include <linux/kernel.h>
+#include <linux/platform_device.h>
+
+enum tegra_dfll_thermal_type {
+	TEGRA_DFLL_THERMAL_FLOOR = 0,
+	TEGRA_DFLL_THERMAL_CAP,
+};
+
+struct tegra_dfll;
+
+extern struct tegra_dfll *tegra_dfll_get_by_phandle(struct device_node *np,
+						    const char *prop);
+extern int tegra_dfll_update_thermal_index(struct tegra_dfll *td,
+			enum tegra_dfll_thermal_type type,
+			unsigned long new_index);
+extern int tegra_dfll_get_thermal_index(struct tegra_dfll *td,
+			enum tegra_dfll_thermal_type type);
+extern int tegra_dfll_count_thermal_states(struct tegra_dfll *td,
+			enum tegra_dfll_thermal_type type);
+int tegra_dfll_set_external_floor_mv(int external_floor_mv);
+u32 tegra_dfll_get_thermal_floor_mv(void);
+u32 tegra_dfll_get_peak_thermal_floor_mv(void);
+u32 tegra_dfll_get_thermal_cap_mv(void);
+u32 tegra_dfll_get_min_millivolts(void);
+struct rail_alignment *tegra_dfll_get_alignment(void);
+const char *tegra_dfll_get_cvb_version(void);
+#endif
diff --git a/include/soc/tegra/tegra-dvfs.h b/include/soc/tegra/tegra-dvfs.h
new file mode 100644
index 000000000000..ff48e9858f69
--- /dev/null
+++ b/include/soc/tegra/tegra-dvfs.h
@@ -0,0 +1,401 @@
+/*
+ * Copyright (c) 2014-2018, NVIDIA Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#ifndef _TEGRA_DVFS_H_
+#define _TEGRA_DVFS_H_
+
+#include <linux/platform_device.h>
+#include <soc/tegra/cvb.h>
+
+#define MAX_DVFS_FREQS		40
+#define DVFS_RAIL_STATS_TOP_BIN	200
+#define DVFS_RAIL_STATS_BIN	10000
+#define MAX_THERMAL_LIMITS	8
+#define MAX_THERMAL_RANGES	(MAX_THERMAL_LIMITS + 1)
+#define MAX_PROCESS_ID		7
+
+enum tegra_dvfs_core_thermal_type {
+	TEGRA_DVFS_CORE_THERMAL_FLOOR = 0,
+	TEGRA_DVFS_CORE_THERMAL_CAP,
+};
+
+/*
+ * dvfs_relationship between to rails, "from" and "to"
+ * when the rail changes, it will call dvfs_rail_update on the rails
+ * in the relationship_to list.
+ * when determining the voltage to set a rail to, it will consider each
+ * rail in the relationship_from list.
+ */
+struct dvfs_relationship {
+	struct dvfs_rail *to;
+	struct dvfs_rail *from;
+	int (*solve)(struct dvfs_rail *, struct dvfs_rail *);
+
+	struct list_head to_node; /* node in relationship_to list */
+	struct list_head from_node; /* node in relationship_from list */
+	bool solved_at_nominal;
+};
+
+struct cpu_pll_fv_table {
+	int freq;
+	int volt;
+};
+
+struct cpu_dvfs {
+	int speedo_id;
+	int process_id;
+
+	int max_mv;
+	int min_mv;
+	struct cpu_pll_fv_table fv_table[MAX_DVFS_FREQS];
+
+	int pll_min_millivolts; /* when PLL source is selected */
+	int speedo_scale;
+	int voltage_scale;
+	struct cvb_table_freq_entry cvb_pll_table[MAX_DVFS_FREQS];
+};
+
+struct rail_stats {
+	ktime_t time_at_mv[DVFS_RAIL_STATS_TOP_BIN + 1];
+	ktime_t last_update;
+	int last_index;
+	bool off;
+	int bin_uv;
+};
+
+struct dvfs_therm_limits {
+	u32 temperature;
+	u32 mv;
+};
+
+struct dvfs_rail {
+	const char *reg_id;
+	int min_millivolts;
+	int max_millivolts;
+	int nominal_millivolts;
+	int override_millivolts;
+	int dbg_mv_offs;
+
+	int step;
+	int step_up;
+	bool jmp_to_zero;
+	bool disabled;
+	bool resolving_to;
+
+	struct list_head node;  /* node in dvfs_rail_list */
+	struct list_head dvfs;  /* list head of attached dvfs clocks */
+	struct list_head relationships_to;
+	struct list_head relationships_from;
+	struct regulator *reg;
+	int millivolts;
+	int new_millivolts;
+	int disable_millivolts;
+	int suspend_millivolts;
+
+	bool suspended;
+	bool dfll_mode;
+	bool joint_rail_with_dfll;
+
+	struct rail_alignment alignment;
+	struct rail_stats stats;
+
+	struct dvfs_therm_limits *therm_floors;
+	int therm_floors_size;
+	int therm_floor_idx;
+
+	bool is_ready;
+	bool in_band_pm;
+
+	struct thermal_cooling_device *vts_cdev;
+	struct device_node *vts_of_node;
+	struct dvfs_therm_limits vts_floors_table[MAX_THERMAL_LIMITS];
+	int vts_trips_table[MAX_THERMAL_LIMITS];
+	int vts_number_of_trips;
+	unsigned long therm_scale_idx;
+
+	struct thermal_cooling_device *vmax_cdev;
+	struct device_node *vmax_of_node;
+	struct dvfs_therm_limits *therm_caps;
+	int therm_caps_size;
+	int therm_cap_idx;
+	bool therm_cap_warned;
+
+	const char *nvver;
+};
+
+enum dfll_range {
+	DFLL_RANGE_NONE = 0,
+	DFLL_RANGE_ALL_RATES,
+	DFLL_RANGE_HIGH_RATES,
+};
+
+struct dvfs {
+	const char *clk_name;
+	struct clk *clk;
+	int speedo_id;
+	int process_id;
+
+	int freqs_mult;
+	unsigned long freqs[MAX_DVFS_FREQS];
+	unsigned long *alt_freqs;
+	const int *millivolts;
+	const int *peak_millivolts;
+	const int *dfll_millivolts;
+	struct dvfs_rail *dvfs_rail;
+	bool auto_dvfs;
+
+	int max_millivolts;
+	int num_freqs;
+
+	enum dfll_range	range;
+	unsigned long use_dfll_rate_min;
+
+	int cur_millivolts;
+	unsigned long cur_rate;
+	struct list_head node;
+	struct list_head reg_node;
+	bool use_alt_freqs;
+	bool therm_dvfs;
+	bool na_dvfs;
+
+	/* Maximum rate safe at minimum voltage across all thermal ranges */
+	unsigned long fmax_at_vmin_safe_t;
+	long dbg_hz_offs;
+};
+
+struct cvb_dvfs_table {
+	unsigned long freq;
+
+	/* Coeffs for voltage calculation, when dfll clock source is selected */
+	struct cvb_coefficients cvb_dfll_param;
+
+	/* Coeffs for voltage calculation, when pll clock source is selected */
+	struct cvb_coefficients cvb_pll_param;
+};
+
+struct cvb_dvfs {
+	int speedo_id;
+	int process_id;
+
+	/* tuning parameters for pll clock */
+	int pll_min_millivolts;
+
+	/* dvfs Max voltage */
+	int max_mv;
+
+	/* dvfs Max frequency */
+	unsigned long max_freq;
+	int freqs_mult;
+
+	/* scaling values for voltage calculation */
+	int speedo_scale;
+	int voltage_scale;
+	int thermal_scale;
+
+	struct cvb_dvfs_table cvb_vmin;
+
+	/* CVB table for various frequencies */
+	struct cvb_dvfs_table cvb_table[MAX_DVFS_FREQS];
+
+	const char *cvb_version;
+};
+
+struct dvb_dvfs_table {
+	unsigned long freq;
+	int mvolts[MAX_PROCESS_ID + 1];
+};
+
+struct dvb_dvfs {
+	int speedo_id;
+	int freqs_mult;
+	struct dvb_dvfs_table dvb_table[MAX_DVFS_FREQS];
+};
+
+static inline bool tegra_dvfs_rail_is_dfll_mode(struct dvfs_rail *rail)
+{
+	return rail ? rail->dfll_mode : false;
+}
+
+static inline bool tegra_dvfs_is_dfll_range_entry(struct dvfs *d,
+						  unsigned long rate)
+{
+	return  d->cur_rate && d->dvfs_rail && (!d->dvfs_rail->dfll_mode) &&
+		(d->range == DFLL_RANGE_HIGH_RATES) &&
+		(rate >= d->use_dfll_rate_min) &&
+		(d->cur_rate < d->use_dfll_rate_min);
+}
+
+static inline bool tegra_dvfs_is_dfll_scale(struct dvfs *d, unsigned long rate)
+{
+	return tegra_dvfs_rail_is_dfll_mode(d->dvfs_rail) ||
+		tegra_dvfs_is_dfll_range_entry(d, rate);
+}
+
+static inline bool dvfs_is_dfll_range(struct dvfs *d, unsigned long rate)
+{
+	return (d->range == DFLL_RANGE_ALL_RATES) ||
+		((d->range == DFLL_RANGE_HIGH_RATES) &&
+		(rate >= d->use_dfll_rate_min));
+}
+
+#ifdef CONFIG_TEGRA_DVFS
+int tegra_dvfs_dfll_mode_set(struct clk *c, unsigned long rate);
+int tegra_dvfs_dfll_mode_clear(struct clk *c, unsigned long rate);
+int tegra_dvfs_get_dfll_threshold(struct clk *c, unsigned long *rate);
+int tegra_dvfs_set_rate(struct clk *c, unsigned long rate);
+unsigned long tegra_dvfs_get_rate(struct clk *c);
+int tegra_dvfs_get_freqs(struct clk *c, unsigned long **freqs, int *num_freqs);
+int tegra_setup_dvfs(struct clk *c, struct dvfs *d);
+int tegra_dvfs_init_rails(struct dvfs_rail *dvfs_rails[], int n);
+void tegra_dvfs_init_rails_lists(struct dvfs_rail *rails[], int n);
+void tegra_dvfs_add_relationships(struct dvfs_relationship *rels, int n);
+void tegra_dvfs_rail_enable(struct dvfs_rail *rail);
+void tegra_dvfs_rail_disable(struct dvfs_rail *rail);
+int tegra_dvfs_predict_millivolts(struct clk *c, unsigned long rate);
+bool tegra_dvfs_is_dfll_range(struct clk *c, unsigned long rate);
+int tegra_dvfs_set_dfll_range(struct clk *c, int range);
+int tegra_get_cpu_fv_table(int *num_freqs, unsigned long **freqs, int **mvs);
+void tegra_dvfs_core_init_therm_limits(struct dvfs_rail *rail);
+int tegra_dvfs_core_get_thermal_index(enum tegra_dvfs_core_thermal_type type);
+int
+tegra_dvfs_core_count_thermal_states(enum tegra_dvfs_core_thermal_type type);
+int tegra_dvfs_core_update_thermal_index(enum tegra_dvfs_core_thermal_type type,
+					 unsigned long new_idx);
+int tegra_dvfs_core_set_thermal_cap(struct clk *cap_clk,
+				    unsigned long thermal_index);
+unsigned long tegra_dvfs_get_maxrate(struct clk *c);
+struct dvfs_rail *tegra_dvfs_get_rail_by_name(char *name);
+bool tegra_dvfs_is_rail_up(struct dvfs_rail *rail);
+int tegra_dvfs_rail_power_down(struct dvfs_rail *rail);
+int tegra_dvfs_rail_power_up(struct dvfs_rail *rail);
+unsigned long tegra_dvfs_round_rate(struct clk *c, unsigned long rate);
+int tegra_dvfs_add_alt_freqs(struct clk *c, struct dvfs *d);
+int tegra_dvfs_use_alt_freqs_on_clk(struct clk *c, bool use_alt_freq);
+int tegra_dvfs_predict_mv_at_hz_cur_tfloor(struct clk *c, unsigned long rate);
+int tegra_dvfs_init_thermal_dvfs_voltages(int *therm_voltages,
+	int *peak_voltages, int freqs_num, int ranges_num, struct dvfs *d);
+long tegra_dvfs_predict_hz_at_mv_max_tfloor(struct clk *c, int mv);
+int tegra_dvfs_predict_mv_at_hz_max_tfloor(struct clk *c, unsigned long rate);
+unsigned long tegra_dvfs_get_fmax_at_vmin_safe_t(struct clk *c);
+bool tegra_dvfs_is_rail_ready(struct dvfs_rail *rail);
+#else
+static inline int tegra_dvfs_dfll_mode_set(struct clk *c, unsigned long rate)
+{ return -EINVAL; }
+static inline int tegra_dvfs_dfll_mode_clear(struct clk *c, unsigned long rate)
+{ return -EINVAL; }
+static inline int tegra_dvfs_get_dfll_threshold(
+		struct clk *c, unsigned long *rate)
+{ return -EINVAL; }
+static inline int tegra_dvfs_set_rate(struct clk *c, unsigned long rate)
+{ return 0; }
+static inline unsigned long tegra_dvfs_get_rate(struct clk *c)
+{ return 0; }
+static inline int tegra_dvfs_get_freqs(
+		struct clk *c, unsigned long **freqs, int *num_freqs)
+{ return -EINVAL; }
+static inline int tegra_setup_dvfs(struct clk *c, struct dvfs *d)
+{ return -EINVAL; }
+static inline int tegra_dvfs_init_rails(struct dvfs_rail *dvfs_rails[], int n)
+{ return -EINVAL; }
+static inline void tegra_dvfs_init_rails_lists(struct dvfs_rail *rails[], int n)
+{ return; }
+static inline void tegra_dvfs_add_relationships(
+		struct dvfs_relationship *rels, int n)
+{ return; }
+static inline void tegra_dvfs_rail_enable(struct dvfs_rail *rail)
+{ return; }
+static inline void tegra_dvfs_rail_disable(struct dvfs_rail *rail)
+{ return; }
+static inline int tegra_dvfs_predict_millivolts(
+		struct clk *c, unsigned long rate)
+{ return -EINVAL; }
+static inline bool tegra_dvfs_is_dfll_range(struct clk *c, unsigned long rate)
+{ return false; }
+static inline int tegra_dvfs_set_dfll_range(struct clk *c, int range)
+{ return -EINVAL; }
+static inline int tegra_get_cpu_fv_table(
+		int *num_freqs, unsigned long **freqs, int **mvs)
+{ return -EINVAL; }
+static inline void tegra_dvfs_core_init_therm_limits(struct dvfs_rail *rail)
+{ return; }
+static inline int tegra_dvfs_core_get_thermal_index(
+					enum tegra_dvfs_core_thermal_type type)
+{ return -EINVAL; }
+static inline int tegra_dvfs_core_count_thermal_states(
+					enum tegra_dvfs_core_thermal_type type)
+{ return -EINVAL; }
+static inline int tegra_dvfs_core_update_thermal_index(
+					enum tegra_dvfs_core_thermal_type type,
+					unsigned long new_idx)
+{ return -EINVAL; }
+static inline int tegra_dvfs_core_set_thermal_cap(
+	struct clk *cap_clk, unsigned long thermal_index)
+{ return -EINVAL; }
+static inline unsigned long tegra_dvfs_get_maxrate(struct clk *c)
+{ return 0; }
+static inline struct dvfs_rail *tegra_dvfs_get_rail_by_name(char *name)
+{ return NULL; }
+static inline bool tegra_dvfs_is_rail_up(struct dvfs_rail *rail)
+{ return false; }
+static inline int tegra_dvfs_rail_power_down(struct dvfs_rail *rail)
+{ return -EINVAL; }
+static inline int tegra_dvfs_rail_power_up(struct dvfs_rail *rail)
+{ return -EINVAL; }
+static inline unsigned long tegra_dvfs_round_rate(struct clk *c,
+						  unsigned long rate)
+{ return rate; }
+static inline int tegra_dvfs_add_alt_freqs(struct clk *c, struct dvfs *d)
+{ return -EINVAL; }
+static inline int tegra_dvfs_use_alt_freqs_on_clk(struct clk *c,
+						  bool use_alt_freq)
+{ return -EINVAL; }
+static inline int tegra_dvfs_predict_mv_at_hz_cur_tfloor(struct clk *c,
+							  unsigned long rate)
+{ return -EINVAL; }
+static inline int tegra_dvfs_init_thermal_dvfs_voltages(int *therm_voltages,
+	int *peak_voltages, int freqs_num, int ranges_num, struct dvfs *d)
+{ return -EINVAL; }
+static inline long tegra_dvfs_predict_hz_at_mv_max_tfloor(struct clk *c,
+							  int mv)
+{ return -EINVAL; }
+static inline int tegra_dvfs_predict_mv_at_hz_max_tfloor(struct clk *c,
+							 unsigned long rate)
+{ return -EINVAL; }
+static inline unsigned long tegra_dvfs_get_fmax_at_vmin_safe_t(struct clk *c)
+{ return 0; }
+
+static inline bool tegra_dvfs_is_rail_ready(struct dvfs_rail *rail)
+{ return false; }
+#endif
+
+#ifdef CONFIG_TEGRA_124_DVFS
+int tegra124_init_dvfs(struct device *node);
+#else
+static inline int tegra124_init_dvfs(struct device *node)
+{ return -EINVAL; }
+#endif
+
+#ifdef CONFIG_TEGRA_210_DVFS
+int tegra210_init_dvfs(struct device *node);
+int tegra210b01_init_dvfs(struct device *node);
+#else
+static inline int tegra210_init_dvfs(struct device *node)
+{ return -EINVAL; }
+static inline int tegra210b01_init_dvfs(struct device *node)
+{ return -EINVAL; }
+#endif
+
+#endif
diff --git a/include/trace/events/bwmgr.h b/include/trace/events/bwmgr.h
new file mode 100644
index 000000000000..37a08e41b3b8
--- /dev/null
+++ b/include/trace/events/bwmgr.h
@@ -0,0 +1,80 @@
+/*
+ * Bwmgr event logging to ftrace.
+ *
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM bwmgr
+
+#if !defined(_TRACE_BWMGR_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _TRACE_BWMGR_H
+
+#include <linux/ktime.h>
+#include <linux/tracepoint.h>
+
+TRACE_EVENT(tegra_bwmgr_set_emc,
+	TP_PROTO(
+		const char *handle,
+		unsigned long val,
+		const char *req
+	),
+
+	TP_ARGS(handle, val, req),
+
+	TP_STRUCT__entry(
+		__field(const char *, handle)
+		__field(unsigned long, val)
+		__field(const char *, req)
+	),
+
+	TP_fast_assign(
+		__entry->handle = handle;
+		__entry->val = val;
+		__entry->req = req;
+	),
+
+	TP_printk("handle=%s, val=%lu, req=%s",
+		__entry->handle,
+		__entry->val,
+		__entry->req
+	)
+);
+
+TRACE_EVENT(tegra_bwmgr_update_efficiency,
+	TP_PROTO(
+		unsigned long cur_state,
+		unsigned long prev_state
+	),
+
+	TP_ARGS(cur_state, prev_state),
+
+	TP_STRUCT__entry(
+		__field(unsigned long, cur_state)
+		__field(unsigned long, prev_state)
+	),
+
+	TP_fast_assign(
+		__entry->cur_state = cur_state;
+		__entry->prev_state = prev_state;
+	),
+
+	TP_printk("bwmgr cooling state changed from (%lu) to (%lu)",
+		__entry->prev_state,
+		__entry->cur_state
+	)
+);
+
+#endif /* _TRACE_BWMGR_H */
+
+/* This part must be outside protection */
+#include <trace/define_trace.h>
diff --git a/kernel/power/qos.c b/kernel/power/qos.c
index 4244b069442e..3d16ca262d5f 100644
--- a/kernel/power/qos.c
+++ b/kernel/power/qos.c
@@ -39,6 +39,254 @@
 #include <linux/export.h>
 #include <trace/events/power.h>
 
+/*
+ * locking rule: all changes to constraints or notifiers lists
+ * or pm_qos_object list and pm_qos_objects need to happen with pm_qos_lock
+ * held, taken with _irqsave.  One lock to rule them all
+ */
+struct pm_qos_object {
+	struct pm_qos_constraints *constraints;
+	struct miscdevice pm_qos_power_miscdev;
+	char *name;
+};
+
+struct pm_qos_bounded_object {
+	struct pm_qos_bounded_constraint *bounds;
+	struct miscdevice miscdev;
+	char *name;
+};
+
+static struct pm_qos_object null_pm_qos;
+static struct pm_qos_bounded_object null_pm_qos_bounded;
+
+static BLOCKING_NOTIFIER_HEAD(cpu_dma_lat_notifier);
+static struct pm_qos_constraints cpu_dma_constraints = {
+	.list = PLIST_HEAD_INIT(cpu_dma_constraints.list),
+	.target_value = PM_QOS_CPU_DMA_LAT_DEFAULT_VALUE,
+	.default_value = PM_QOS_CPU_DMA_LAT_DEFAULT_VALUE,
+	.no_constraint_value = PM_QOS_CPU_DMA_LAT_DEFAULT_VALUE,
+	.type = PM_QOS_MIN,
+	.notifiers = &cpu_dma_lat_notifier,
+};
+static struct pm_qos_object cpu_dma_pm_qos = {
+	.constraints = &cpu_dma_constraints,
+	.name = "cpu_dma_latency",
+};
+
+static BLOCKING_NOTIFIER_HEAD(network_lat_notifier);
+static struct pm_qos_constraints network_lat_constraints = {
+	.list = PLIST_HEAD_INIT(network_lat_constraints.list),
+	.target_value = PM_QOS_NETWORK_LAT_DEFAULT_VALUE,
+	.default_value = PM_QOS_NETWORK_LAT_DEFAULT_VALUE,
+	.no_constraint_value = PM_QOS_NETWORK_LAT_DEFAULT_VALUE,
+	.type = PM_QOS_MIN,
+	.notifiers = &network_lat_notifier,
+};
+static struct pm_qos_object network_lat_pm_qos = {
+	.constraints = &network_lat_constraints,
+	.name = "network_latency",
+};
+
+static BLOCKING_NOTIFIER_HEAD(network_throughput_notifier);
+static struct pm_qos_constraints network_tput_constraints = {
+	.list = PLIST_HEAD_INIT(network_tput_constraints.list),
+	.target_value = PM_QOS_NETWORK_THROUGHPUT_DEFAULT_VALUE,
+	.default_value = PM_QOS_NETWORK_THROUGHPUT_DEFAULT_VALUE,
+	.no_constraint_value = PM_QOS_NETWORK_THROUGHPUT_DEFAULT_VALUE,
+	.type = PM_QOS_MAX,
+	.notifiers = &network_throughput_notifier,
+};
+static struct pm_qos_object network_throughput_pm_qos = {
+	.constraints = &network_tput_constraints,
+	.name = "network_throughput",
+};
+
+static BLOCKING_NOTIFIER_HEAD(memory_bandwidth_notifier);
+static struct pm_qos_constraints memory_bw_constraints = {
+	.list = PLIST_HEAD_INIT(memory_bw_constraints.list),
+	.target_value = PM_QOS_MEMORY_BANDWIDTH_DEFAULT_VALUE,
+	.default_value = PM_QOS_MEMORY_BANDWIDTH_DEFAULT_VALUE,
+	.no_constraint_value = PM_QOS_MEMORY_BANDWIDTH_DEFAULT_VALUE,
+	.type = PM_QOS_SUM,
+	.notifiers = &memory_bandwidth_notifier,
+};
+static struct pm_qos_object memory_bandwidth_pm_qos = {
+	.constraints = &memory_bw_constraints,
+	.name = "memory_bandwidth",
+};
+
+static BLOCKING_NOTIFIER_HEAD(gpu_freq_min_notifier);
+static struct pm_qos_constraints gpu_freq_min_constraints = {
+	.list = PLIST_HEAD_INIT(gpu_freq_min_constraints.list),
+	.target_value = PM_QOS_GPU_FREQ_MIN_DEFAULT_VALUE,
+	.default_value = PM_QOS_GPU_FREQ_MIN_DEFAULT_VALUE,
+	.type = PM_QOS_MAX,
+	.notifiers = &gpu_freq_min_notifier,
+	.parent_class = PM_QOS_GPU_FREQ_BOUNDS,
+};
+static struct pm_qos_object gpu_freq_min_pm_qos = {
+	.constraints = &gpu_freq_min_constraints,
+	.name = "gpu_freq_min",
+};
+
+static BLOCKING_NOTIFIER_HEAD(gpu_freq_max_notifier);
+static struct pm_qos_constraints gpu_freq_max_constraints = {
+	.list = PLIST_HEAD_INIT(gpu_freq_max_constraints.list),
+	.target_value = PM_QOS_GPU_FREQ_MAX_DEFAULT_VALUE,
+	.default_value = PM_QOS_GPU_FREQ_MAX_DEFAULT_VALUE,
+	.type = PM_QOS_MIN,
+	.notifiers = &gpu_freq_max_notifier,
+	.parent_class = PM_QOS_GPU_FREQ_BOUNDS,
+};
+static struct pm_qos_object gpu_freq_max_pm_qos = {
+	.constraints = &gpu_freq_max_constraints,
+	.name = "gpu_freq_max",
+};
+
+static struct pm_qos_bounded_constraint online_cpus_constraint = {
+	.prio_list = PLIST_HEAD_INIT(online_cpus_constraint.prio_list),
+	.max_class = PM_QOS_MAX_ONLINE_CPUS,
+	.min_class = PM_QOS_MIN_ONLINE_CPUS,
+	.min_wins = true,
+};
+static struct pm_qos_bounded_object online_cpus_pm_qos = {
+	.bounds = &online_cpus_constraint,
+	.name = "constraint_online_cpus",
+};
+static BLOCKING_NOTIFIER_HEAD(min_online_cpus_notifier);
+static struct pm_qos_constraints min_online_cpus_constraints = {
+	.list = PLIST_HEAD_INIT(min_online_cpus_constraints.list),
+	.target_value = PM_QOS_MIN_ONLINE_CPUS_DEFAULT_VALUE,
+	.default_value = PM_QOS_MIN_ONLINE_CPUS_DEFAULT_VALUE,
+	.type = PM_QOS_MAX,
+	.notifiers = &min_online_cpus_notifier,
+	.parent_class = PM_QOS_ONLINE_CPUS_BOUNDS,
+};
+static struct pm_qos_object min_online_cpus_pm_qos = {
+	.constraints = &min_online_cpus_constraints,
+	.name = "min_online_cpus",
+};
+
+static BLOCKING_NOTIFIER_HEAD(max_online_cpus_notifier);
+static struct pm_qos_constraints max_online_cpus_constraints = {
+	.list = PLIST_HEAD_INIT(max_online_cpus_constraints.list),
+	.target_value = PM_QOS_MAX_ONLINE_CPUS_DEFAULT_VALUE,
+	.default_value = PM_QOS_MAX_ONLINE_CPUS_DEFAULT_VALUE,
+	.type = PM_QOS_MIN,
+	.notifiers = &max_online_cpus_notifier,
+	.parent_class = PM_QOS_ONLINE_CPUS_BOUNDS,
+};
+static struct pm_qos_object max_online_cpus_pm_qos = {
+	.constraints = &max_online_cpus_constraints,
+	.name = "max_online_cpus",
+};
+
+static struct pm_qos_bounded_constraint cpu_freq_constraint = {
+	.prio_list = PLIST_HEAD_INIT(cpu_freq_constraint.prio_list),
+	.max_class = PM_QOS_CPU_FREQ_MAX,
+	.min_class = PM_QOS_CPU_FREQ_MIN,
+	.min_wins = false,
+};
+static struct pm_qos_bounded_object cpu_freq_pm_qos = {
+	.bounds = &cpu_freq_constraint,
+	.name = "constraint_cpu_freq",
+};
+static BLOCKING_NOTIFIER_HEAD(cpu_freq_min_notifier);
+static struct pm_qos_constraints cpu_freq_min_constraints = {
+	.list = PLIST_HEAD_INIT(cpu_freq_min_constraints.list),
+	.target_value = PM_QOS_CPU_FREQ_MIN_DEFAULT_VALUE,
+	.default_value = PM_QOS_CPU_FREQ_MIN_DEFAULT_VALUE,
+	.type = PM_QOS_MAX,
+	.notifiers = &cpu_freq_min_notifier,
+	.parent_class = PM_QOS_CPU_FREQ_BOUNDS,
+};
+static struct pm_qos_object cpu_freq_min_pm_qos = {
+	.constraints = &cpu_freq_min_constraints,
+	.name = "cpu_freq_min",
+};
+
+static BLOCKING_NOTIFIER_HEAD(cpu_freq_max_notifier);
+static struct pm_qos_constraints cpu_freq_max_constraints = {
+	.list = PLIST_HEAD_INIT(cpu_freq_max_constraints.list),
+	.target_value = PM_QOS_CPU_FREQ_MAX_DEFAULT_VALUE,
+	.default_value = PM_QOS_CPU_FREQ_MAX_DEFAULT_VALUE,
+	.type = PM_QOS_MIN,
+	.notifiers = &cpu_freq_max_notifier,
+	.parent_class = PM_QOS_CPU_FREQ_BOUNDS,
+};
+static struct pm_qos_object cpu_freq_max_pm_qos = {
+	.constraints = &cpu_freq_max_constraints,
+	.name = "cpu_freq_max",
+};
+
+static BLOCKING_NOTIFIER_HEAD(emc_freq_min_notifier);
+static struct pm_qos_constraints emc_freq_min_constraints = {
+	.list = PLIST_HEAD_INIT(emc_freq_min_constraints.list),
+	.target_value = PM_QOS_EMC_FREQ_MIN_DEFAULT_VALUE,
+	.default_value = PM_QOS_EMC_FREQ_MIN_DEFAULT_VALUE,
+	.type = PM_QOS_MAX,
+	.notifiers = &emc_freq_min_notifier,
+};
+static struct pm_qos_object emc_freq_min_pm_qos = {
+	.constraints = &emc_freq_min_constraints,
+	.name = "emc_freq_min",
+};
+
+static BLOCKING_NOTIFIER_HEAD(max_cpu_pwr_notifier);
+static struct pm_qos_constraints max_cpu_pwr_constraints = {
+	.list = PLIST_HEAD_INIT(max_cpu_pwr_constraints.list),
+	.target_value = PM_QOS_CPU_POWER_MAX_DEFAULT_VALUE,
+	.default_value = PM_QOS_CPU_POWER_MAX_DEFAULT_VALUE,
+	.type = PM_QOS_MIN,
+	.notifiers = &max_cpu_pwr_notifier,
+};
+static struct pm_qos_object max_cpu_pwr_qos = {
+	.constraints = &max_cpu_pwr_constraints,
+	.name = "max_cpu_power",
+};
+
+static BLOCKING_NOTIFIER_HEAD(max_gpu_pwr_notifier);
+static struct pm_qos_constraints max_gpu_pwr_constraints = {
+	.list = PLIST_HEAD_INIT(max_gpu_pwr_constraints.list),
+	.target_value = PM_QOS_GPU_POWER_MAX_DEFAULT_VALUE,
+	.default_value = PM_QOS_GPU_POWER_MAX_DEFAULT_VALUE,
+	.type = PM_QOS_MIN,
+	.notifiers = &max_gpu_pwr_notifier,
+};
+static struct pm_qos_object max_gpu_pwr_qos = {
+	.constraints = &max_gpu_pwr_constraints,
+	.name = "max_gpu_power",
+};
+
+static struct pm_qos_object *pm_qos_array[] = {
+	&null_pm_qos,
+	&cpu_dma_pm_qos,
+	&network_lat_pm_qos,
+	&network_throughput_pm_qos,
+	&memory_bandwidth_pm_qos,
+	&gpu_freq_min_pm_qos,
+	&gpu_freq_max_pm_qos,
+	&min_online_cpus_pm_qos,
+	&max_online_cpus_pm_qos,
+	&cpu_freq_min_pm_qos,
+	&cpu_freq_max_pm_qos,
+	&emc_freq_min_pm_qos,
+	&max_cpu_pwr_qos,
+	&max_gpu_pwr_qos,
+};
+
+/**
+ * pm_qos_request - returns current system wide qos expectation
+ * @pm_qos_class: identification of which qos value is requested
+ *
+ * This function returns the current target value.
+ */
+int pm_qos_request_func(int pm_qos_class)
+{
+	return pm_qos_read_value(pm_qos_array[pm_qos_class]->constraints);
+}
+EXPORT_SYMBOL_GPL(pm_qos_request_func);
+
 /*
  * locking rule: all changes to constraints or notifiers lists
  * or pm_qos_object list and pm_qos_objects need to happen with pm_qos_lock
@@ -649,6 +897,26 @@ int freq_qos_add_notifier(struct freq_constraints *qos,
 }
 EXPORT_SYMBOL_GPL(freq_qos_add_notifier);
 
+/**
+ * pm_qos_add_notifier - sets notification entry for changes to target value
+ * @pm_qos_class: identifies which qos target changes should be notified.
+ * @notifier: notifier block managed by caller.
+ *
+ * will register the notifier into a notification chain that gets called
+ * upon changes to the pm_qos_class target value.
+ */
+int pm_qos_add_notifier(int pm_qos_class, struct notifier_block *notifier)
+{
+	int retval;
+
+	retval = blocking_notifier_chain_register(
+			pm_qos_array[pm_qos_class]->constraints->notifiers,
+			notifier);
+
+	return retval;
+}
+EXPORT_SYMBOL_GPL(pm_qos_add_notifier);
+
 /**
  * freq_qos_remove_notifier - Remove frequency QoS change notifier.
  * @qos: List of requests to remove the notifier from.
-- 
2.34.1

